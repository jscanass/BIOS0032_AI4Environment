{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/06_DL_for_Remote_Sensing/06_DL_for_Remote_Sensing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Deep Learning for Remote Sensing\n",
    "\n",
    "This week, we will learn about remote sensing! W will explore the information content we can obtain\n",
    "from optical satellite imagery, and how to train a deep learning model to make pixel-wise\n",
    "predictions (semantic segmentation) using such data.\n",
    "\n",
    "More specifically, we will train one of the most popular semantic segmentation models, U-net, to\n",
    "predict forest coverage in the Brazilian rainforest using Sentinel-2 imagery. The output will not\n",
    "only be a spatial forest map, but a prediction of **change** in time due to deforestation.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup](#1-setup)\n",
    "2. [Optical Remote Sensing Data](#2-optical-remote-sensing-data)\n",
    "3. [Forest Mapping](#3-forest-mapping)\n",
    "4. [Change Detection & Deforestation Monitoring](#4-change-detection--deforestation-monitoring)\n",
    "5. [Summary and Outlook](#5-summary-and-outlook)\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- If a line starts with the fountain pen symbol (ðŸ–Œï¸), it asks you to implement a code part or\n",
    "answer a question.\n",
    "- Lines starting with the light bulb symbol (ðŸ’¡) provide important information or tips and tricks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.1 Enable GPU Runtime\n",
    "\n",
    "Go to `Runtime` -> `Change runtime type` and select `GPU` as the hardware accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "try:\n",
    "  matplotlib.use('widget')\n",
    "except:\n",
    "  %pip install -q ipympl\n",
    "  os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "If you run the above code block for the first time, you will get a message that Google Colab has\n",
    "crashed. This is deliberate, because we need to restart the environment after installation of one of\n",
    "the packages.\n",
    "\n",
    "Just ignore this message and continue with the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('widget')\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import rasterio\n",
    "import ipywidgets as widgets\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a shortcut in you drive to this [shared folder](https://drive.google.com/drive/folders/1k2PyRm9AhYYyS_3vGJEqw3a9hAsn737X?usp=sharing).\n",
    "\n",
    "This will allow you to access the data we will use in this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!unzip /content/drive/MyDrive/BIOS0032/2025/data/lab6_data.zip -d /content/week6_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optical Remote Sensing Data\n",
    "\n",
    "As you have seen in the lecture, we have different types of remote sensing data at our disposal:\n",
    "* Optical\n",
    "* Synthetic Aperture Radar (SAR)\n",
    "* Airborne Laser Scanning (ALS), respectively Light Detection and Ranging (LiDAR)\n",
    "\n",
    "Each of these uses a different portion of the electromagnetic spectrum, ranging from thermal\n",
    "infrared (longest wavelength) via short-wave and near-infrared to the visible range (red, green,\n",
    "blue), rarely to Ultraviolet (shortest wavelength). Furthermore, SAR and LiDAR (and sonar by the\n",
    "way, which is used for underwater sensing) are **active** sensors, that is, they emit their own\n",
    "radiation and measure properties of what bounces back from the Earth. Optical sensors in turn\n",
    "measure the reflected _irradiance_ from the sun.\n",
    "\n",
    "As you can imagine, optical sensors can be categorised by their _spatial_ resolution (just like your\n",
    "camera â€“ how many megapixels, _etc._). However, they can also differ in their **spectral**\n",
    "resolution, which includes the number of spectral _bands_ and the bandwidth:\n",
    "* RGB sensors just measure one band each in the red, green, and blue wavelengths.\n",
    "* **Multispectral** sensors include at least one band beyond the visible range.\n",
    "* **Hyperspectral** sensors have many narrow, evenly spaced bands across large ranges of the\n",
    "  electromagnetic spectrum.\n",
    "\n",
    "\n",
    "###Â 2.1 Sentinel-2\n",
    "\n",
    "Below, we will take a look at data captured by the\n",
    "[Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) satellite\n",
    "missions. Sentinel-2 consists of two satellites, recording multispectral (12 bands) data at 10m\n",
    "resolution.\n",
    "\n",
    "Sentinel-2 (and other related missions, such as the SAR-based Sentinel-1) are part of the Copernicus\n",
    "program and free of charge to use.\n",
    "\n",
    "ðŸ’¡ You can download Sentinel-1 and -2 imagery free of charge from the [Copernicus\n",
    "browser](https://browser.dataspace.copernicus.eu/) (registration required).\n",
    "\n",
    "Let us now first take a look at a satellite dataset downloaded from Copernicus over the Brazilian\n",
    "Rainforest:\n",
    "\n",
    "1. Open the \"Files\" tab in Google Colab (click the folder icon to the left).\n",
    "2. Navigate to folder\n",
    "   `week6_data/deforestation/Sentinel-2_stripes/2017-06-28-00:00_2017-06-28-23:59_Sentinel-2_L2A`\n",
    "\n",
    "As you can see, this folder contains twelve `*.tiff` files. TIFF (Tagged Image File Format) is an\n",
    "image format that allows storing pixel data without any loss due to compression, as well as\n",
    "information like geospatial position (**GeoTIFF**). Unlike JPEG, it also can store more than three\n",
    "channels per image, which is important for multi- and hyperspectral remote sensing data.\n",
    "\n",
    "ðŸ’¡ You will notice suffix \"_L2A\". This designates the processing **level** of the satellite dataset.\n",
    "Loads of levels are available designating various procedures the data has undergone (error\n",
    "correction, radiometric correction, geocoding, _etc._). Unless you are interested in very specific\n",
    "parts of the pipeline, or else atmospheric data (_e.g._, on clouds), you usually want to use the\n",
    "Level 2A (L2A) products. More information on Sentinel-2 processing levels can be found\n",
    "[here](https://sentiwiki.copernicus.eu/web/s2-processing).\n",
    "\n",
    "Let's take a look at one of those files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = '/content/week6_data/deforestation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(BASE_FOLDER, 'Sentinel-2_stripes', '2017-06-28-00:00_2017-06-28-23:59_Sentinel-2_L2A/2017-06-28-00:00_2017-06-28-23:59_Sentinel-2_L2A_B01_(Raw).tiff')\n",
    "\n",
    "\n",
    "with rasterio.open(file_path, 'r') as f_band:\n",
    "    band = f_band.read()\n",
    "\n",
    "print(f'Data type: {type(band)}')\n",
    "print(f'Data shape: {band.shape}')\n",
    "print(f'Data number type: {band.dtype}')\n",
    "print(f'Data min/max: {band.min()}/{band.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have noticed that we didn't use standard image libraries like PIL anymore. This is\n",
    "because those libraries cannot deal with GeoTIFFs properly: they cannot read multiband images, nor\n",
    "any geospatial metadata. Hence, we are using [Rasterio](https://rasterio.readthedocs.io/).\n",
    "\n",
    "ðŸ’¡ In the background, Rasterio uses the [Geospatial Data Abstraction Library\n",
    "(GDAL)](https://gdal.org/), the largest and most versatile open-source framework for geospatial\n",
    "data.\n",
    "\n",
    "ðŸ’¡ [QGIS](https://qgis.org/) is the best known open-source Geographic Information System (GIS) and\n",
    "uses GDAL for anything related to spatial processing.\n",
    "\n",
    "ðŸ’¡ The most comparable equivalent to Rasterio for R is [terra](https://rspatial.github.io/terra/),\n",
    "also building on GDAL.\n",
    "\n",
    "\n",
    "Rasterio returns a NumPy array of size `BxHxW` (bands x height x width). We can visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "@widgets.interact(brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0))\n",
    "def vis_band(brightness=1.0):\n",
    "    plt.gca().clear()\n",
    "    plt.imshow(brightness * band.squeeze().astype(float) / 10000.0,\n",
    "               cmap='gray',\n",
    "               vmin=0,\n",
    "               vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice a couple of things from the above figure:\n",
    "* The image is in greyscale. As you might have guessed already, this TIFF file just contains a\n",
    "  single band.\n",
    "* It is very dark, unless you turn up the brightness. Standard cameras like in your smartphone\n",
    "  record images in 8-bit unsigned integer, which can store values from 0 to 255. That would be very\n",
    "  problematic for Earth observation, since we may want to record many more values than just 256. The\n",
    "  above Sentinel-2 band is encoded in 16-bit unsigned integer, which can store 65,536 different\n",
    "  intensity values.\n",
    "* The image contains some strange, diagonal stripes. These are artefacts from the data processing.\n",
    "  The diagonality comes from the fact that the satellite orbits the Earth while it is rotating\n",
    "  around its own axis.\n",
    "\n",
    "It may be difficult to see what we are looking at in the image above. However, we have almost all\n",
    "the other bands recorded by the satellite, so perhaps we can visualise a colour composite instead?\n",
    "\n",
    "ðŸ–Œï¸ Go to [this Web page](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/bands/)\n",
    "and take a look at all the bands Sentinel-2 measures. Let's write them down in a list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 band wavelengths and names in order; see here:\n",
    "# https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/bands/\n",
    "S2_LAMBDAS = [  # wavelengths [nm] (wavelength is often denoted as lambda)\n",
    "    443,\n",
    "    490,\n",
    "    560,\n",
    "    665,\n",
    "    705,\n",
    "    740,\n",
    "    783,\n",
    "    842,\n",
    "    865,\n",
    "    945,\n",
    "    # 1375,\n",
    "    1610,\n",
    "    2190\n",
    "]\n",
    "\n",
    "S2_BANDS = [\n",
    "    '443 nm (aerosol)',\n",
    "    '490 nm (Blue)',\n",
    "    '560 nm (Green)',\n",
    "    '665 nm (Red)',\n",
    "    '705 nm (Red Edge)',\n",
    "    '740 nm',\n",
    "    '783 nm',\n",
    "    '842 nm (NIR)',\n",
    "    '865 nm',\n",
    "    '945 nm',\n",
    "    # '1375 nm',            # band 10 is for cirrus cloud detection; we don't have it for L2A\n",
    "    '1610 nm (SWIR 1)',\n",
    "    '2190 nm (SWIR 2)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load all the bands in correct order for a given satellite scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_composite(folder):\n",
    "    # find all files\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    # load all bands in correct order\n",
    "    bands = ('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12')\n",
    "\n",
    "    # create layer stack\n",
    "    layer_stack = []\n",
    "    for band in bands:\n",
    "        # find correct file name\n",
    "        file_name = [file for file in files if band in file][0]\n",
    "        with rasterio.open(os.path.join(folder, file_name), 'r') as f_layer:\n",
    "            band = f_layer.read().astype(float) / 10000.0\n",
    "            layer_stack.append(band)\n",
    "    return np.concatenate(layer_stack, 0)\n",
    "\n",
    "\n",
    "# load all bands in order and stack them together into a layer stack\n",
    "FOLDER_2017 = os.path.join(BASE_FOLDER, 'Sentinel-2_stripes', '2017-06-28-00:00_2017-06-28-23:59_Sentinel-2_L2A/')\n",
    "\n",
    "data_2017 = load_composite(FOLDER_2017)\n",
    "\n",
    "print(f'Layer stack shape: {data_2017.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a **layer stack** of twelve bands in order. From the above list of band names, you\n",
    "should be able to see which bands we need to obtain a true colour image.\n",
    "\n",
    "\n",
    "Let's visualise it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "points, spectra_2017 = [], []                   # for click events\n",
    "\n",
    "def redraw_plot():\n",
    "    global points, spectra_2017\n",
    "    ax1 = plt.subplot(1,2,2)\n",
    "    ax1.clear()\n",
    "    for spectrum in spectra_2017:\n",
    "        ax1.plot(S2_LAMBDAS, spectrum, '-')\n",
    "    ax1.set_ylabel('Normalised value')\n",
    "    ax1.set_xticks(S2_LAMBDAS, S2_BANDS, rotation=90)\n",
    "    plt.margins(0.05)\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "\n",
    "def click(event):\n",
    "     global tile, points, spectra_2017\n",
    "     if event.xdata is not None and event.ydata is not None:\n",
    "        x = int(np.clip(event.xdata, 0, data_2017.shape[2]))\n",
    "        y = int(np.clip(event.ydata, 0, data_2017.shape[1]))\n",
    "        spectral_vals_2017 = data_2017[:,y,x]\n",
    "        spectra_2017.append(spectral_vals_2017)\n",
    "        points.append([x,y])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x, y)\n",
    "        redraw_plot()\n",
    "\n",
    "figure = plt.figure(figsize=(16, 6))\n",
    "figure.canvas.mpl_connect('button_press_event', click)\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        red=S2_BANDS,\n",
    "        green=S2_BANDS,\n",
    "        blue=S2_BANDS,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_composite(red=S2_BANDS[3],\n",
    "                  green=S2_BANDS[2],\n",
    "                  blue=S2_BANDS[1],\n",
    "                  brightness=1.0):\n",
    "    global tile_file_name, tile, ts, points, spectra_2017\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    plt.subplot(1,2,1)\n",
    "    arr = brightness * np.transpose(data_2017[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title('2017-06-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the controls of the above widget as follows:\n",
    "1. Drag the brightness slider up or down.\n",
    "2. Select different band combinations for visualisation. Tip: besides red: red, green: green, blue:\n",
    "   blue (true colour), try also red: NIR, green: red, blue: green (false colour near-infrared).\n",
    "3. Click into the image at different locations. A second plot should appear to the right showing\n",
    "   intensity values for each point and wavelength.\n",
    "\n",
    "\n",
    "ðŸ–Œï¸  What does this scene depict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ–Œï¸  What are typical band reflectance characteristics of the different **land cover** classes you\n",
    "can see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spectral Indices\n",
    "\n",
    "From the plots above, you may have noticed that some land cover categories show very strong\n",
    "characteristics in how they reflect radiation for each wavelength. Moreover, it can be tricky to\n",
    "disentangle land cover types based on absolute reflectance values â€“ it often is much more\n",
    "straightforward to look at the _relative relation_ between spectral bands.\n",
    "\n",
    "This is the principle of **spectral indices**, that is, quotients between spectral bands.\n",
    "\n",
    "The perhaps most famous spectral index ever proposed is the **Normalised Difference Vegetation Index\n",
    "(NDVI)**.\n",
    "\n",
    "ðŸ–Œï¸ Look up the definition of NDVI and complete the code below to calculate it for our Sentinel-2\n",
    "scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndvi(data_array):\n",
    "    return ...          # implement NDVI computation here\n",
    "\n",
    "\n",
    "# let's test it\n",
    "ndvi_2017 = ndvi(data_2017)\n",
    "\n",
    "# check whether the output shape is correct\n",
    "assert ndvi_2017.shape == data_2017.shape[1:], \\\n",
    "    f'Error: output shape should be {data_2017.shape[1:]}, got {ndvi_2017.shape}.'\n",
    "\n",
    "# check whether the values are within the right range\n",
    "assert np.nanmin(ndvi_2017) >= -1 and np.nanmax(ndvi_2017) <= 1, \\\n",
    "        'Error: NDVI output values should be within [0, 1].'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        red=S2_BANDS,\n",
    "        green=S2_BANDS,\n",
    "        blue=S2_BANDS,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_composite_ndvi(red=S2_BANDS[3],\n",
    "                       green=S2_BANDS[2],\n",
    "                       blue=S2_BANDS[1],\n",
    "                       brightness=1.0):\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(data_2017[band_indices,...], (1,2,0))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.title('Sentinel-2 Image')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(ndvi_2017, cmap='vanimo', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('NDVI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, NDVI shows very high values for vegetation and zero to negative ones for anything\n",
    "else. This allows us not only to perform a simple classification of the pixels into vegetation/rest,\n",
    "but also to estimate plant productivity, trace phenologic cycles, _etc._ NDVI is used for many\n",
    "downstream analyses (one might argue a few too many)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus: extra indices**\n",
    "\n",
    "As said, there are many more indices to compute. For example, [this\n",
    "list](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/indexdb/) gives a good\n",
    "overview over indices available for Sentinel-2.\n",
    "\n",
    "ðŸ–Œï¸ Pick three indices, look them up, implement them below and observe the result. Note: keep the\n",
    "function names as they are (`index_1`, `index_2`, `index_3`). Each function should return two\n",
    "variables: the calculated index and a string denoting the name of the index chosen. You may also\n",
    "need to modify the `vmin` and `vmax` parameters in the code cell afterwards, depending on whether\n",
    "the index returns values outside the default `[-1,1]` range or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_1(data_array):\n",
    "    val = ...\n",
    "    name = 'Index 1'\n",
    "    return val, name\n",
    "\n",
    "\n",
    "def index_2(data_array):\n",
    "    val = ...\n",
    "    name = 'Index 2'\n",
    "    return val, name\n",
    "\n",
    "\n",
    "def index_3(data_array):\n",
    "    val = ...\n",
    "    name = 'Index 3'\n",
    "    return val, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        red=S2_BANDS,\n",
    "        green=S2_BANDS,\n",
    "        blue=S2_BANDS,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_composite_indices(red=S2_BANDS[3],\n",
    "                       green=S2_BANDS[2],\n",
    "                       blue=S2_BANDS[1],\n",
    "                       brightness=1.0):\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(data_2017[band_indices,...], (1,2,0))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.title('Sentinel-2 Image')\n",
    "    plt.subplot(2,2,2)\n",
    "    index_1_vals, index_1_name = index_1(data_2017)\n",
    "    plt.imshow(index_1_vals, cmap='vanimo', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title(index_1_name)\n",
    "    plt.subplot(2,2,3)\n",
    "    index_2_vals, index_2_name = index_2(data_2017)\n",
    "    plt.imshow(index_2_vals, cmap='vanimo', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title(index_2_name)\n",
    "    plt.subplot(2,2,4)\n",
    "    index_3_vals, index_3_name = index_3(data_2017)\n",
    "    plt.imshow(index_3_vals, cmap='vanimo', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title(index_3_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list and your choices, you may be able to make the following observations:\n",
    "* Some indices can be really useful for one or another type of land cover class (especially water\n",
    "  and vegetation).\n",
    "* Some are downright useless for our scene (but remember that they might work in other settings,\n",
    "  such as with more bare soil).\n",
    "* Not all indices are of the same format of \"(band a - band b) / (band a + band b)\"; they can become\n",
    "  quite elaborate.\n",
    "* Some indices have been developed for specific sensors in mind. For example, the Enhanced\n",
    "  Vegetation Index (EVI) is a very popular choice and should perform much better than NDVI over\n",
    "  rainforest (where the latter saturates, as you have seen above). However, EVI has been developed\n",
    "  for the [MODIS](https://modis.gsfc.nasa.gov/about/) mission and contains correction factors that\n",
    "  do not automatically work for other sensors. Thus, if you try to calculate EVI for Sentinel-2\n",
    "  using the default formula, it likely won't give you good results.\n",
    "* Finally, some indices include adjustment factors that we often do not know a priori. For example,\n",
    "  the Soil Adjusted Vegetation Index (SAVI) requires a correction factor known as the \"soil line\".\n",
    "  We could estimate this by taking samples of soil pixels and measuring their reflectance ratio\n",
    "  across the right bands. Other indices are not even directly related to spectral bands but may be\n",
    "  correlated with them. An example of those is the Leaf Area Index (LAI), which is the ratio of the\n",
    "  area of leaves over non-vegetation: LAI can range from 0 (no leaves) over 0.5 (50% area coverage\n",
    "  by leaves) to 1 (only leaves visible in a pixel/mapping unit). We do not know the LAI for our\n",
    "  scene above since we cannot identify individual leaves; the best we could do is to empirically\n",
    "  regress it.\n",
    "\n",
    "\n",
    "Ultimately, spectral indices can be very powerful (and we will see this down below). It can\n",
    "certainly make sense for you to use them, but it's always important to be aware of their\n",
    "limitations, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amazon Deforestation Dataset\n",
    "\n",
    "The above scene showed a rather ominous mixture of natural forest and man-made structures, chief\n",
    "among which clear cuts and plots. Indeed, we are looking at a scene of the Amazon rainforest that\n",
    "has been subject to deforestation.\n",
    "\n",
    "An important task in remote sensing is to map land cover, such as forests. We have seen above that\n",
    "we can do so pretty well visually. Although we cannot use NDVI alone, since it does not allow us to\n",
    "separate forest from pasture (for example), we can use a subset of bands to do so â€“ and a powerful\n",
    "machine learning model.\n",
    "\n",
    "If you think back to Session 4, you will remember that this task is known as **semantic\n",
    "segmentation**, _i.e._, pixel-wise classification. As you can imagine, we would have to download\n",
    "lots of Sentinel-2 data and label them all for forest/non-forest pixel-wise, which is very tedious\n",
    "to do. Luckily, such datasets have been curated and are readily available.\n",
    "\n",
    "In the following, we will be using the [Amazon and Atlantic\n",
    "Forest](https://www.kaggle.com/datasets/catiowiec/amazon-and-atlantic-forest-sentinel-2-multiband/data)\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look into that folder, you will find that the images are already divided into training,\n",
    "validation, and test sets. Also, each folder contains subfolders for images and masks (ground truth\n",
    "annotations).\n",
    "\n",
    "If you look further into the `images` folders, you will find a lot of TIFF files. This time, it's\n",
    "not one for each band, but one for each scene, containing multiple bands in one file.\n",
    "\n",
    "Moreover, the Sentinel-2 images in there contain only four bands instead of the original twelve:\n",
    "blue, green, red, near-infrared (NIR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_BANDS_SUBSET = [\n",
    "    '490 nm (Blue)',\n",
    "    '560 nm (Green)',\n",
    "    '665 nm (Red)',\n",
    "    '842 nm (NIR)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function to load such an image tile as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tile(file_path):\n",
    "    with rasterio.open(file_path, 'r') as f_file:\n",
    "        data = f_file.read()\n",
    "    data = data / 10000.0       # we can use the same normalisation here because it's the same format (that isn't always the case; always double-check)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can visualise them just as we did with the big file above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# find all \".tif\" files in the training subfolder\n",
    "tile_file_names = glob.glob(os.path.join(BASE_FOLDER, 'AMAZON', 'Training', 'images', '*.tif'))\n",
    "\n",
    "# sort them alphabetically\n",
    "tile_file_names.sort()\n",
    "\n",
    "# keep track of the tile we are currently visualising\n",
    "tile_file_name = None\n",
    "tile = None\n",
    "\n",
    "points, spectra = [], []                # for click events\n",
    "\n",
    "def redraw_plot():\n",
    "    global points, spectra\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.gca().clear()\n",
    "    for spectrum in spectra:\n",
    "        plt.plot(range(len(spectrum)), spectrum, '-')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Normalised value')\n",
    "    plt.xticks(range(len(S2_BANDS_SUBSET)), S2_BANDS_SUBSET, rotation=90)\n",
    "    plt.margins(0.05)\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "def hover(event):\n",
    "    global tile\n",
    "    redraw_plot()\n",
    "    if tile is not None and event.xdata is not None and event.ydata is not None:\n",
    "        x = int(np.clip(event.xdata, 0, tile.shape[2]))\n",
    "        y = int(np.clip(event.ydata, 0, tile.shape[1]))\n",
    "        spectral_vals = tile[:,y,x]\n",
    "        plt.plot(range(len(spectral_vals)), spectral_vals, 'k-')\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('Normalised value')\n",
    "        plt.xticks(range(len(spectral_vals)), S2_BANDS_SUBSET, rotation=90)\n",
    "\n",
    "def click(event):\n",
    "     global tile, points, spectra\n",
    "     if event.xdata is not None and event.ydata is not None:\n",
    "        x = int(np.clip(event.xdata, 0, tile.shape[2]))\n",
    "        y = int(np.clip(event.ydata, 0, tile.shape[1]))\n",
    "        spectral_vals = tile[:,y,x]\n",
    "        spectra.append(spectral_vals)\n",
    "        points.append([x,y])\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x, y)\n",
    "        redraw_plot()\n",
    "\n",
    "figure = plt.figure(figsize=(14,7))\n",
    "# figure.canvas.mpl_connect('motion_notify_event', hover)\n",
    "figure.canvas.mpl_connect('button_press_event', click)\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        file_name=tile_file_names,\n",
    "        red=S2_BANDS_SUBSET,\n",
    "        green=S2_BANDS_SUBSET,\n",
    "        blue=S2_BANDS_SUBSET,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_tiles_all_median(file_name=tile_file_names[0],\n",
    "                         red=S2_BANDS_SUBSET[2],\n",
    "                         green=S2_BANDS_SUBSET[1],\n",
    "                         blue=S2_BANDS_SUBSET[0],\n",
    "                         brightness=1.0):\n",
    "    global tile_file_name, tile, ts, points, spectra\n",
    "    plt.subplot(1,2,1)\n",
    "    if file_name != tile_file_name:\n",
    "        plt.gca().clear()\n",
    "        points, spectra = [], []\n",
    "        redraw_plot()\n",
    "        plt.subplot(1,2,1)\n",
    "    plt.subplot(1,2,1)\n",
    "    tile = load_tile(file_name)\n",
    "    band_indices = [S2_BANDS_SUBSET.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(tile[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    if file_name != tile_file_name:\n",
    "        for point in points:\n",
    "            plt.scatter(point[0], point[1])\n",
    "    tile_file_name = file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like above, you can adjust the brightness, band combination, and click into the tile to plot\n",
    "spectra.\n",
    "\n",
    "ðŸ–Œï¸ Some of these files will contain clouds â€“ take a look at them. What can you say about the\n",
    "spectral response of clouds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further have ground truth annotations in the `masks` folder, containing information for each\n",
    "pixel on whether it is forest or not. Let us first create a list of these label classes and a colour\n",
    "map for visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CLASSES = (\n",
    "    \"non-Forest\",\n",
    "    \"Forest\"\n",
    ")\n",
    "\n",
    "LABELCLASS_COLOURS = (\n",
    "    (0, 0, 0),\n",
    "    (0.5, 1, 0.5)\n",
    ")\n",
    "\n",
    "# create colour map for Matplotlib\n",
    "cmap = matplotlib.colors.ListedColormap(LABELCLASS_COLOURS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can visualise the ground truth side-by-side with the image tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        tile_name=tile_file_names,\n",
    "        red=S2_BANDS_SUBSET,\n",
    "        green=S2_BANDS_SUBSET,\n",
    "        blue=S2_BANDS_SUBSET,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_tiles_with_annotations(tile_name=tile_file_names[0],\n",
    "                               red=S2_BANDS_SUBSET[2],\n",
    "                               green=S2_BANDS_SUBSET[1],\n",
    "                               blue=S2_BANDS_SUBSET[0],\n",
    "                               brightness=1.0):\n",
    "    plt.subplot(1,2,1)\n",
    "    tile = load_tile(tile_name)\n",
    "    band_indices = [S2_BANDS_SUBSET.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(tile[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tile_name_anno = tile_name.replace('images', 'masks')\n",
    "    with rasterio.open(tile_name_anno, 'r') as f_anno:\n",
    "        tile_anno = f_anno.read()\n",
    "\n",
    "    plt.imshow(tile_anno.squeeze(), cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see where this is leading: we have many image-ground truth tuples, split into three sets.\n",
    "Let's train a model to predict forest masks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Segmentation with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 U-net\n",
    "\n",
    "The next major ingredient we need for predicting forest masks is a prediction model.\n",
    "\n",
    "In classical remote sensing, this often consisted in a simple model, such as a random forest, that\n",
    "would take pixel-wise spectral values as inputs and predict outputs individually. This usually works\n",
    "if our spatial resolution is \"low enough\" (_e.g._, Landsat: 30m). However, as spatial resolution\n",
    "increased and land cover (or use) classes became more fine-scale, per-pixel approaches didn't cut it\n",
    "anymore: a grey pixel could be a road or top of a building; to really be sure, spatial **texture**\n",
    "started to become important, too.\n",
    "\n",
    "ðŸ’¡ This is why pixel-wise semantic segmentation was historically just referred to as\n",
    "\"classification\" in remote sensing.\n",
    "\n",
    "\n",
    "Many semantic segmentation ideas have been proposed over time, and as you would have guessed, deep\n",
    "learning has spawned the most successful models. Among those, the arguably number one model is\n",
    "called [U-net](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28):\n",
    "\n",
    "ðŸ–Œï¸ Read up about U-net online and provide a brief explanation of its working principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-net has originally been proposed for segmentation of biomedical images. However, it is now used in\n",
    "many other fields, including remote sensing.\n",
    "\n",
    "ðŸ’¡ Have you heard of Dall-E2 and (in particular) Stable Diffusion? Well, at their core lies... a\n",
    "U-net.ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ–Œï¸ Find an online implementation (_e.g._ on GitHub) of U-net in PyTorch and copy the relevant code\n",
    "blocks into the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate U-net model instance with correct number of input channels and number of predicted\n",
    "# classes.\n",
    "# Of course, you may need to find an implementation of U-net first (class UNet ...)\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "s2_img = load_tile(tile_file_names[0])\n",
    "\n",
    "# prepare image for prediction with U-net:\n",
    "# 1. Convert from numpy.array to torch.Tensor\n",
    "# 2. Add leading dimension for batch index (.unsqueeze(0))\n",
    "# 3. Convert to 32-bit float (single precision)\n",
    "data = torch.from_numpy(s2_img).unsqueeze(0).float()\n",
    "\n",
    "# obtain model prediction (forward pass)\n",
    "with torch.no_grad():\n",
    "    pred = model(data)\n",
    "\n",
    "print(f'Input size:\\t\\t{data.size()}')\n",
    "print(f'Prediction size:\\t{pred.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked correctly, you should get a prediction of size `BxCxHxW` (batch size x no.\n",
    "classes x height x width), with height and width being identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Deep Learning Ingredients\n",
    "\n",
    "Everything coming should be very familiar to you: we have seen it all in Sessions 2-4 where we\n",
    "learnt how to train and test deep learning models.\n",
    "\n",
    "The next ingredient we need is a Dataset class definition. Here, we first find all the images and\n",
    "then load them together with the corresponding ground truth mask in the `__getitem__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class S2Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 transform,\n",
    "                 split='Training'):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # find all label files in data dir\n",
    "        self.label_files = glob.glob(os.path.join(data_dir, split, 'masks', '*.tif'))\n",
    "        self.image_files = [file.replace('masks', 'images') for file in self.label_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load and normalise image tile\n",
    "        img = load_tile(self.image_files[idx])\n",
    "\n",
    "        # convert to torch.Tensor\n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        # transform\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # load segmentation ground truth\n",
    "        with rasterio.open(self.label_files[idx], 'r') as f_label:\n",
    "            target = f_label.read()\n",
    "        target = torch.from_numpy(target.squeeze()).long()\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a loss function. Since semantic segmentation is nothing else than (pixel-wise)\n",
    "classification, we should be able to use a cross-entropy loss. Luckily, PyTorch's [Cross-Entropy\n",
    "loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) can handle\n",
    "multidimensional data easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# load target mask for our sample image\n",
    "target_path = tile_file_names[0].replace('images', 'masks')\n",
    "with rasterio.open(target_path, 'r') as f_target:\n",
    "    target = f_target.read()\n",
    "\n",
    "# prepare for usage with criterion\n",
    "target = torch.from_numpy(target).long()\n",
    "\n",
    "\n",
    "# initialise cross-entropy loss instance\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# predict loss\n",
    "loss = criterion(pred, target)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `loss` still only contains one value. If you go to the\n",
    "[documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) of the\n",
    "loss function, you will notice argument `reduction`. By default, this is set to `\"mean\"`, _i.e._,\n",
    "per-pixel cross-entropy loss values are averaged. This is important for gradient computation (which\n",
    "requires one single loss value to begin with). If you want you can try setting `reduction` to\n",
    "something else and observing what happens with the output.\n",
    "\n",
    "ðŸ’¡ Setting `reduction='none'` can be used to _e.g._ modify loss values for each pixel individually.\n",
    "This can be useful when you have areas in an image you don't want the model to learn from.\n",
    "\n",
    "ðŸ’¡ Since we are technically doing binary classification, we could also use a sigmoid + binary\n",
    "cross-entropy instead of softmax + multi-class cross-entropy.\n",
    "\n",
    "\n",
    "The following code block just contains some convenience functions for us to load and save model\n",
    "states after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re       # package for regular expressions\n",
    "\n",
    "\n",
    "# this is a default dict of what we store after completion of each epoch\n",
    "DEFAULT_STATE_DICT = {\n",
    "    'model': None,\n",
    "    'loss_train': [],\n",
    "    'loss_val': [],\n",
    "    'oa_train': [],\n",
    "    'oa_val': []\n",
    "}\n",
    "\n",
    "\n",
    "def load_model(model,\n",
    "               save_path='model_states',\n",
    "               epoch='latest'):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # find all model states in directory\n",
    "    model_files = glob.glob(os.path.join(save_path, '*.pt'))\n",
    "    # extract epoch from file names\n",
    "    model_epochs = [int(re.sub(r'.*/([0-9]+)\\.pt', '\\\\1', file)) for file in model_files]\n",
    "\n",
    "    if len(model_epochs) == 0 or (isinstance(epoch, int) and epoch <= 0):\n",
    "        # nothing saved yet or forcing creation of new model\n",
    "        print(f'Initialising new model...')\n",
    "        return model, 0, DEFAULT_STATE_DICT.copy()\n",
    "\n",
    "    if epoch == 'latest':\n",
    "        model_epoch = max(model_epochs)\n",
    "    \n",
    "    elif epoch not in model_epochs:\n",
    "        raise Exception(f'Invalid model epoch specified (epoch {epoch} not found).')\n",
    "    else:\n",
    "        model_epoch = epoch\n",
    "\n",
    "    # load model state\n",
    "    print(f'Loading model state at epoch {model_epoch}...')\n",
    "    with open(os.path.join(save_path, f'{model_epoch}.pt'), 'rb') as f_state:\n",
    "        state_dict = torch.load(f_state, map_location='cpu', weights_only=False)\n",
    "        model.load_state_dict(state_dict['model'])\n",
    "\n",
    "    return model, model_epoch, state_dict\n",
    "\n",
    "\n",
    "def save_model(state_dict,\n",
    "               epoch,\n",
    "               save_path='model_states'):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    with open(os.path.join(save_path, f'{epoch}.pt'), 'wb') as f_state:\n",
    "        torch.save(state_dict, f_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below follows the main training block. Again, pretty much all of this should be very familiar to\n",
    "you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torchvision import transforms as T\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "NUM_EPOCHS = 10\n",
    "START_EPOCH = 'latest'      # set to \"latest\", a number (for specific epoch) or zero (to start training a new model)\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0\n",
    "MOMENTUM = 0.9              # momentum keeps parts of gradients from previous batches, which can help to \"keep the ball rolling\" over minor undulations of the gradient landscape\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "\n",
    "# init transforms\n",
    "transforms = T.Normalize(mean=0.5*torch.ones(len(S2_BANDS_SUBSET)),     # see below for an explanation of these values\n",
    "                         std=torch.ones(len(S2_BANDS_SUBSET)))\n",
    "\n",
    "# init dataset and data loader\n",
    "data_folder = os.path.join(BASE_FOLDER, 'AMAZON')\n",
    "dl_train = DataLoader(S2Dataset(data_folder, transforms, 'Training'),\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      shuffle=True)\n",
    "dl_val = DataLoader(S2Dataset(data_folder, transforms, 'Validation'),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False)\n",
    "\n",
    "# init optimiser\n",
    "optimiser = SGD(model.parameters(),\n",
    "                lr=LEARNING_RATE,\n",
    "                weight_decay=WEIGHT_DECAY,\n",
    "                momentum=MOMENTUM)\n",
    "\n",
    "# load model from pre-trained state if available\n",
    "model, start_epoch, state_dict = load_model(model, 'model_states', epoch=START_EPOCH)\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# init criterion (with reduction=\"mean\" as default)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# helper function to plot training progress\n",
    "plt.figure(figsize=(8, 5))\n",
    "def plot_training_progress():\n",
    "    clear_output(wait=True)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(state_dict['loss_train'], 'b-')\n",
    "    plt.plot(state_dict['loss_val'], 'r-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(state_dict['oa_train'], 'b-', label='train')\n",
    "    plt.plot(state_dict['oa_val'], 'r-', label='val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Overall Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_progress()\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(start_epoch+1, NUM_EPOCHS+1):\n",
    "    # train\n",
    "    loss_epoch_train, accuracy_epoch_train = 0.0, 0.0\n",
    "\n",
    "    # put model in training mode (never forget!)\n",
    "    model.train()\n",
    "    with tqdm(dl_train) as pbar:\n",
    "        for idx, (data, target) in enumerate(dl_train):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            # standard deep learning training here, just like you have seen in Session 3\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, target)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loss_epoch_train += loss.item()\n",
    "\n",
    "            # overall accuracy:\n",
    "            # 1. predicted label (y_hat) = position of the predicted max logits in class dimension (pred.argmax(1))\n",
    "            # 2. pixel-wise comparison with ground truth (y_hat == target): returns tensor of ones where identical and zeros elsewhere\n",
    "            # 3. convert from bool (True/False) to float (1.0/0.0)\n",
    "            # 4. flatten into 1-D tensor (.view(-1))\n",
    "            # 5. take average across all pixels (torch.mean)\n",
    "            accuracy_epoch_train += torch.mean((pred.argmax(1) == target).float().view(-1)).item()\n",
    "\n",
    "            pbar.set_description(f'[Ep. {epoch} train] Loss: {loss_epoch_train/(idx+1):.2f}, OA: {accuracy_epoch_train/(idx+1):.2%}')\n",
    "            pbar.update(1)\n",
    "\n",
    "    # average loss and overall accuracy values by number of batches (length of data loader),\n",
    "    # append to correct list\n",
    "    state_dict['loss_train'].append(loss_epoch_train / len(dl_train))\n",
    "    state_dict['oa_train'].append(accuracy_epoch_train / len(dl_train))\n",
    "\n",
    "    # validate\n",
    "    loss_epoch_val, accuracy_epoch_val = 0.0, 0.0\n",
    "\n",
    "    # put model in evaluation mode (never forget!)\n",
    "    model.eval()\n",
    "    with tqdm(dl_val) as pbar:\n",
    "        for idx, (data, target) in enumerate(dl_val):\n",
    "            with torch.no_grad():       #Â skip calculating gradients; we don't need them for predictions\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "                pred = model(data)\n",
    "                loss = criterion(pred, target)\n",
    "\n",
    "                loss_epoch_val += loss.item()\n",
    "                accuracy_epoch_val += torch.mean((pred.argmax(1) == target).float().view(-1)).item()\n",
    "\n",
    "                pbar.set_description(f'[Ep. {epoch}   val] Loss: {loss_epoch_val/(idx+1):.2f}, OA: {accuracy_epoch_val/(idx+1):.2%}')\n",
    "                pbar.update(1)\n",
    "    state_dict['loss_val'].append(loss_epoch_val / len(dl_val))\n",
    "    state_dict['oa_val'].append(accuracy_epoch_val / len(dl_val))\n",
    "\n",
    "    # save model\n",
    "    state_dict['model'] = model.state_dict()        # get model parameters & assign under correct key\n",
    "    save_model(state_dict, epoch)\n",
    "\n",
    "    # plot\n",
    "    plot_training_progress()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ In the code above you may notice that we use mean values of 0.5 and standard deviation values of\n",
    "1.0 for normalising the tiles instead of per-band statistics calculated a priori, as we usually\n",
    "would. This is just down to empirical experimentation â€“ because remote sensing datasets are not\n",
    "\"standardised\" like natural images (_i.e._, with values from 0 to 255), there is no proper guidance\n",
    "on how to perform data normalisation. Values still should be zero-centred and at unit norm for deep\n",
    "learning models, though. When loading Sentinel-2 tiles, we divided values by 10,000 because that's\n",
    "how reflectance is encoded ([see\n",
    "here](https://docs.sentinel-hub.com/api/latest/data/sentinel-2-l2a/)); other products might need\n",
    "different treatment.\n",
    "\n",
    "ðŸ’¡ We didn't use any other data transforms this time. However, if you want you can absolutely do\n",
    "data augmentation like we did in Session 4. Just be careful to also augment the ground truth the\n",
    "same way: if you perform a horizontal flip of an image, you also need to flip the ground truth this\n",
    "time. There are libraries to help you with that, such as\n",
    "[Albumentations](https://github.com/albumentations-team/albumentations).\n",
    "\n",
    "\n",
    "Running the above code you may notice that performance shoots up rather quickly, even after just a\n",
    "few epochs. Let's see how well our model works on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# load tiles from test set\n",
    "tile_file_names = glob.glob(os.path.join(BASE_FOLDER, 'AMAZON', 'Test', 'images', '*.tif'))\n",
    "\n",
    "# sort alphabetically\n",
    "tile_file_names.sort()\n",
    "\n",
    "# cache data for current tile\n",
    "current_file_name = None\n",
    "tile = None\n",
    "confidence, y_hat = None, None\n",
    "tile_anno = None\n",
    "\n",
    "# put model on right device and into evaluation mode\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "\n",
    "@widgets.interact(\n",
    "        tile_name=tile_file_names,\n",
    "        red=S2_BANDS_SUBSET,\n",
    "        green=S2_BANDS_SUBSET,\n",
    "        blue=S2_BANDS_SUBSET,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_tiles_with_predictions(tile_name=tile_file_names[0],\n",
    "                               red=S2_BANDS_SUBSET[2],\n",
    "                               green=S2_BANDS_SUBSET[1],\n",
    "                               blue=S2_BANDS_SUBSET[0],\n",
    "                               brightness=1.0):\n",
    "\n",
    "    global current_file_name, tile, tile_median, confidence, y_hat, tile_anno\n",
    "\n",
    "    if tile_name != current_file_name:\n",
    "        # load tile\n",
    "        tile = load_tile(tile_name)\n",
    "\n",
    "        # get prediction\n",
    "        with torch.no_grad():\n",
    "            # prepare model input: convert to torch.Tensor, convert to 32-bit float,\n",
    "            # add leading batch dimension, put tensor on right device\n",
    "            data = torch.from_numpy(tile).float().unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            # remember that we have a normalisation transform to apply.\n",
    "            # If you use data augmentation, make sure to not apply that during testing.\n",
    "            data = transforms(data)\n",
    "\n",
    "            # obtain prediction/model logits (forward pass)\n",
    "            pred = model(data)\n",
    "\n",
    "            # 1. get pseudo-probabilities via softmax\n",
    "            # 2. get confidence (per-pixel max value) and predicted class y_hat (argument of max)\n",
    "            # In PyTorch, specifying a dimension with max (.max(1)) returns both the values and\n",
    "            # arguments together.\n",
    "            confidence, y_hat = pred.softmax(dim=1).max(1)\n",
    "\n",
    "            # convert back: remove any extra dimensions (e.g., batch), move tensors back to CPU,\n",
    "            # convert to NumPy array\n",
    "            confidence = confidence.squeeze().cpu().numpy()\n",
    "            y_hat = y_hat.squeeze().cpu().numpy()\n",
    "        \n",
    "        # load ground truth: same file name, but different folder, so we can simply replace that\n",
    "        tile_name_anno = tile_name.replace('images', 'masks')\n",
    "        with rasterio.open(tile_name_anno, 'r') as f_label:\n",
    "            tile_anno = f_label.read().squeeze()\n",
    "\n",
    "        plt.clf()\n",
    "\n",
    "    # show image tile\n",
    "    plt.subplot(2,2,1)\n",
    "    band_indices = [S2_BANDS_SUBSET.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(tile[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.title('Sentinel-2 image')\n",
    "\n",
    "    # show ground truth\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(tile_anno, cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)\n",
    "    plt.title(r'Ground truth $y$')\n",
    "\n",
    "    # show model confidence\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(confidence, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Model confidence')\n",
    "\n",
    "    # show model predictions\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(y_hat, cmap=cmap)\n",
    "    plt.title(r'Model prediction $\\hat{y}$')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ–Œï¸ Can you explain the significance and model behaviour under the \"Model confidence\" panel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Answer:_\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we should of course calculate accuracy metrics. During training above, we calculated overall\n",
    "accuracy on the training and validation set already. While this gives us an indicator as to whether\n",
    "the model under- or overfits, it may not tell us the whole story.\n",
    "\n",
    "Remember accuracy metrics from Session 2 and how biased they can be? For example, if 80% of our\n",
    "pixels are \"forest\", the model can always trivially predict that class and still score an overall\n",
    "accuracy of 80%.\n",
    "\n",
    "Traditionally, you would often find the following metrics in remote sensing tasks:\n",
    "* A confusion matrix\n",
    "* User's accuracy (precision)\n",
    "* Producer's accuracy (recall)\n",
    "* Cohen's kappa / kappa coefficient: this measures the degree of chance agreement between a model\n",
    "  prediction and ground truth and is very popular in remote sensing (but see [this recent\n",
    "  paper](https://www.sciencedirect.com/science/article/abs/pii/S0034425719306509)).\n",
    "\n",
    "\"User's accuracy\" and \"producer's accuracy\" are very common terms in remote sensing but really\n",
    "denote nothing else than precision and recall.\n",
    "\n",
    "See [here](https://gsp.humboldt.edu/olm/courses/GSP_216/lessons/accuracy/metrics.html) for more\n",
    "information on accuracy metrics in remote sensing.\n",
    "\n",
    "\n",
    "Our model outputs confidence scores via softmax (as shown above). Thus, we can do one of the most\n",
    "complete analyses and calculate a precision-recall curve for all confidence thresholds. You have\n",
    "seen this in Session 2 also. To do so, we can predict all test set images and \"flatten\" the\n",
    "prediction and ground truth to 1-D tensors. If we combine them across all images, we can then\n",
    "calculate precision and recall values for all test set pixels together. The code block below does\n",
    "this and then plots the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare empty lists of confidence scores and ground truth labels\n",
    "predictions, targets = [], []\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "# iterate over test set\n",
    "dl_test = DataLoader(S2Dataset(data_folder, transforms, 'Test'),\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     shuffle=False)\n",
    "for data, target in tqdm(dl_test):\n",
    "    with torch.no_grad():\n",
    "        # get prediction (forward pass)\n",
    "        pred = model(data.to(DEVICE))\n",
    "        # get pseudo-probabilities with Softmax\n",
    "        pred = pred.softmax(dim=1)\n",
    "        # take second channel (\"forest\")\n",
    "        pred = pred[:,1,:,:]\n",
    "        # flatten into 1-D array (batch size * number of pixels per image)\n",
    "        pred = pred.flatten()\n",
    "        target = target.flatten()\n",
    "        # append to lists\n",
    "        predictions.append(pred.cpu())\n",
    "        targets.append(target)\n",
    "\n",
    "# concatenate prediction and target lists across images\n",
    "predictions = torch.cat(predictions, 0)\n",
    "targets = torch.cat(targets, 0)\n",
    "\n",
    "\n",
    "# calculate precision-recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true=targets.numpy(),\n",
    "                                      y_score=predictions.numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, 'k-')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have let the model train for long enough, that should give you a really good performance.\n",
    "\n",
    "What's the catch?\n",
    "\n",
    "Well, maybe there is none. In this case, the problem might simply be _too easy_. Remember from above\n",
    "where we had taken a look at spectral values of forested _vs._ non-forested areas? We saw pretty\n",
    "strong differences. We also took a look at NDVI, and although the contrast wasn't super clear\n",
    "between forest and other vegetated areas, it was still there. Perhaps we could use NDVI to predict\n",
    "forest cover, _i.e._, we assume any pixel with NDVI >= pre-defined threshold is forest?\n",
    "\n",
    "ðŸ–Œï¸ Run the widget below and try adjusting the `ndvi_threshold` slider to match the thresholded\n",
    "image in the bottom left with the ground truth in the bottom right. Try doing this for multiple\n",
    "images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "# we need to redefine the NDVI function for our tiles, since the required bands now sit at different\n",
    "# positions in the layer stack compared to the full scenes\n",
    "def ndvi_tile(tile):\n",
    "    return (tile[3,...] - tile[2,...]) / (tile[3,...] + tile[2,...])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        tile_name=tile_file_names,\n",
    "        red=S2_BANDS_SUBSET,\n",
    "        green=S2_BANDS_SUBSET,\n",
    "        blue=S2_BANDS_SUBSET,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0),\n",
    "        ndvi_threshold=widgets.FloatSlider(value=0.5, min=-1.0, max=1.0, step=0.01)\n",
    ")\n",
    "def vis_tiles_with_ndvi_threshold(tile_name=tile_file_names[0],\n",
    "                                  red=S2_BANDS_SUBSET[2],\n",
    "                                  green=S2_BANDS_SUBSET[1],\n",
    "                                  blue=S2_BANDS_SUBSET[0],\n",
    "                                  brightness=1.0,\n",
    "                                  ndvi_threshold=0.5):\n",
    "    plt.subplot(2,2,1)\n",
    "    tile = load_tile(tile_name)\n",
    "    band_indices = [S2_BANDS_SUBSET.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(tile[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.title('Sentinel-2 image')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    tile_ndvi = ndvi_tile(tile)\n",
    "    plt.imshow(tile_ndvi, cmap='vanimo', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.title('NDVI')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    tile_ndvi_threshold = (tile_ndvi >= ndvi_threshold).astype(int)\n",
    "    plt.imshow(tile_ndvi_threshold, cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)\n",
    "    plt.title('NDVI thresholded')\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    tile_name_anno = tile_name.replace('images', 'masks')\n",
    "    with rasterio.open(tile_name_anno, 'r') as f_anno:\n",
    "        tile_anno = f_anno.read()\n",
    "\n",
    "    plt.imshow(tile_anno.squeeze(), cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)\n",
    "    plt.title('Ground Truth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not perfect, the result can be really good â€“ and this for a **non-machine learning** model\n",
    "that is orders of magnitude cheaper to compute than a U-net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Change Detection & Deforestation Monitoring\n",
    "\n",
    "For this last part, let us go back to our trained U-net, as well as the larger Sentinel-2 image we\n",
    "investigated at the beginning of the exercise.\n",
    "\n",
    "Mapping forest cover is more important than ever, but in wake of our alterations to the environment,\n",
    "predicting **change** is at yet another level. Wouldn't it be great if we could map _deforestation_\n",
    "across time?\n",
    "\n",
    "Above, we looked at a single Sentinel-2 scene from 2017. The satellites have been in orbit since\n",
    "2014 and still are â€“ and you are provided with another dataset over the exact same area, but from\n",
    "2024. Perhaps we could see how much has changed in seven years?\n",
    "\n",
    "Let us first re-load the twelve bands and create a composite (layer stack) for both timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_2017 = os.path.join(BASE_FOLDER, 'Sentinel-2_stripes', '2017-06-28-00:00_2017-06-28-23:59_Sentinel-2_L2A')\n",
    "FOLDER_2024 = os.path.join(BASE_FOLDER, 'Sentinel-2_stripes', '2024-06-06-00:00_2024-06-06-23:59_Sentinel-2_L2A')\n",
    "\n",
    "# we're re-using the load_composite function we defined above\n",
    "data_2017 = load_composite(FOLDER_2017)\n",
    "data_2024 = load_composite(FOLDER_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to visualise our data first. The following widget shows both scenes\n",
    "side-by-side.\n",
    "\n",
    "Just as above, you can adjust brightness and band configuration, and click into either image to\n",
    "display reflectance spectra for both (underneath each respective scene).\n",
    "\n",
    "Click into areas of change and see how the spectra differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "points, spectra_2017, spectra_2024 = [], [], []                # for click events\n",
    "\n",
    "def redraw_plot():\n",
    "    global points, spectra_2017, spectra_2024\n",
    "    ax1 = plt.subplot(2,2,3)\n",
    "    ax1.clear()\n",
    "    for spectrum in spectra_2017:\n",
    "        ax1.plot(S2_LAMBDAS, spectrum, '-')\n",
    "    ax1.set_ylabel('Normalised value')\n",
    "    ax1.set_xticks(S2_LAMBDAS, S2_BANDS, rotation=90)\n",
    "    ax2 = plt.subplot(2,2,4, sharey=ax1)\n",
    "    ax2.clear()\n",
    "    for spectrum in spectra_2024:\n",
    "        ax2.plot(S2_LAMBDAS, spectrum, '-')\n",
    "    ax2.set_ylabel('Normalised value')\n",
    "    ax2.set_xticks(S2_LAMBDAS, S2_BANDS, rotation=90)\n",
    "    plt.margins(0.05)\n",
    "    plt.tight_layout(pad=2)\n",
    "\n",
    "\n",
    "def click(event):\n",
    "     global tile, points, spectra_2017, spectra_2024\n",
    "     if event.xdata is not None and event.ydata is not None:\n",
    "        x = int(np.clip(event.xdata, 0, data_2017.shape[2]))\n",
    "        y = int(np.clip(event.ydata, 0, data_2017.shape[1]))\n",
    "        spectral_vals_2017 = data_2017[:,y,x]\n",
    "        spectra_2017.append(spectral_vals_2017)\n",
    "        spectral_vals_2024 = data_2024[:,y,x]\n",
    "        spectra_2024.append(spectral_vals_2024)\n",
    "        points.append([x,y])\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.scatter(x, y)\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.scatter(x, y)\n",
    "        redraw_plot()\n",
    "\n",
    "figure = plt.figure(figsize=(15, 10))\n",
    "figure.canvas.mpl_connect('button_press_event', click)\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        red=S2_BANDS,\n",
    "        green=S2_BANDS,\n",
    "        blue=S2_BANDS,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_composites(red=S2_BANDS[3],\n",
    "                   green=S2_BANDS[2],\n",
    "                   blue=S2_BANDS[1],\n",
    "                   brightness=1.0):\n",
    "    global tile_file_name, tile, ts, points, spectra_2017, spectra_2024\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    plt.subplot(2,2,1)\n",
    "    arr = brightness * np.transpose(data_2017[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title('2017-06-28')\n",
    "    plt.subplot(2,2,2)\n",
    "    arr = brightness * np.transpose(data_2024[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title('2024-06-06')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use our trained U-net model from above to predict these changes. To do so, we will:\n",
    "1. Obtain pixel-wise predictions for each timestep.\n",
    "2. Create a difference map by subtracting predicted classes (forest/non-forest) from each other.\n",
    "\n",
    "\n",
    "ðŸ’¡ This is a very crude way of performing change detection. It may not always work due to different\n",
    "dataset characteristics (\"domain shift\", see final section of previous practical). Many other ways\n",
    "have been proposed instead, including:\n",
    "* Unsupervised change detection: trying to compare scenes without any labels.\n",
    "* Models that ingest two inputs at once and predict changes directly.\n",
    "* _etc._\n",
    "\n",
    "\n",
    "We now just have one obstacle: our two scenes are way larger than the $512\\times512$ tiles our U-net\n",
    "was trained on. The general solution thus is to split our full scenes up into **patches** (or tiles,\n",
    "windows) of correct size, predict each tile individually (or in batches), and stitch the predictions\n",
    "back together:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif\" />\n",
    "\n",
    "[image source](https://github.com/obss/sahi)\n",
    "\n",
    "To do so, we have to perform the following steps:\n",
    "1. Create target tensor/array of same size as full scene to store model predictions in\n",
    "2. Create list of positions in East/North (x/y) direction that denote the top-left corner of each\n",
    "   patch.\n",
    "3. For each coordinate in x/y: extract patch, obtain prediction, \"burn\" prediction into output\n",
    "   tensor\n",
    "\n",
    "\n",
    "Finally, remember that our U-net has only been trained on blue, green, red, and NIR bands, while our\n",
    "scenes contain twelve of them. However, since we know exactly which band is where, we can just take\n",
    "a subset of the required bands in correct order.\n",
    "\n",
    "Let's do all of that in one go below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_composite(composite,\n",
    "                      model,\n",
    "                      tile_size):\n",
    "    model = model.eval().to(DEVICE)\n",
    "\n",
    "    # normalise satellite data for model by z-scoring over entire composite\n",
    "    data_mean = np.nanmean(composite.reshape((composite.shape[0], -1)), 1)[:,np.newaxis,np.newaxis]\n",
    "    data_std = np.nanstd(composite.reshape((composite.shape[0], -1)), 1)[:,np.newaxis,np.newaxis]\n",
    "\n",
    "    data = (np.copy(composite) - data_mean) / data_std\n",
    "\n",
    "    # create positions in East/North (x/y) from 0 to shape (width/height) of data in tile_size steps\n",
    "    pos_x = np.arange(0, data.shape[2], tile_size[0])\n",
    "    pos_y = np.arange(0, data.shape[1], tile_size[1])\n",
    "\n",
    "    # prepare arrays to store predictions in\n",
    "    conf_comp = np.zeros(data.shape[1:])\n",
    "    y_hat_comp = np.zeros(data.shape[1:], dtype=int)\n",
    "\n",
    "    # iterate over positions\n",
    "    for loc_x in pos_x:\n",
    "        for loc_y in pos_y:\n",
    "            # determine patch size: we may overshoot the data boundaries at the end, so we take\n",
    "            # the minimum of either position + tile size or else width/height of data array\n",
    "            end_x = min(loc_x+tile_size[0], data.shape[2])\n",
    "            end_y = min(loc_y+tile_size[1], data.shape[1])\n",
    "\n",
    "            # extract patch: all bands, y position to end in height, x position to end in width\n",
    "            patch = data[:,loc_y:end_y,loc_x:end_x]\n",
    "\n",
    "            # at the scene borders, some of our patches might be smaller than the target size;\n",
    "            # to prevent this from happening, we will pad them with zeros.\n",
    "            # This is important so that the input to our U-net always is the same size.\n",
    "            patch_shape = patch.shape\n",
    "            patch = np.pad(patch, [(0,0),                               # no padding in band dim\n",
    "                                   (0,tile_size[1]-patch_shape[1]),     # remainder in height\n",
    "                                   (0,tile_size[0]-patch_shape[2])],    # remainder in width\n",
    "                                   mode='constant')\n",
    "\n",
    "            # prepare patch as usual: convert to Tensor, 32-bit float, apply transforms,\n",
    "            # add leading batch dimension and move to device\n",
    "            patch = torch.from_numpy(patch).float()\n",
    "            patch = transforms(patch)\n",
    "            patch = patch.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            # predict\n",
    "            with torch.no_grad():\n",
    "                pred = model(patch)\n",
    "                conf, y_hat = pred.softmax(dim=1).max(1)\n",
    "\n",
    "            # throw away padded zeros again (if there are any): take only the subset of the\n",
    "            # actual patch shape as originally noted pre-zero padding\n",
    "            conf = conf[:,:patch_shape[1],:patch_shape[2]]\n",
    "            y_hat = y_hat[:,:patch_shape[1],:patch_shape[2]]\n",
    "\n",
    "            # store in target arrays at correct position\n",
    "            conf_comp[loc_y:end_y,loc_x:end_x] = conf.squeeze().cpu().numpy()\n",
    "            y_hat_comp[loc_y:end_y,loc_x:end_x] = y_hat.squeeze().cpu().numpy()\n",
    "    return conf_comp, y_hat_comp\n",
    "\n",
    "\n",
    "# we only need four bands out of the twelve for our deep learning model\n",
    "composite_bands = [1,2,3,7]     # Blue, Green, Red, NIR\n",
    "\n",
    "# predict two composites\n",
    "conf_2017, y_hat_2017 = predict_composite(data_2017[composite_bands,...],\n",
    "                                          model,\n",
    "                                          tile_size=[512, 512])\n",
    "\n",
    "conf_2024, y_hat_2024 = predict_composite(data_2024[composite_bands,...],\n",
    "                                          model,\n",
    "                                          tile_size=[512, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualised side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "        red=S2_BANDS,\n",
    "        green=S2_BANDS,\n",
    "        blue=S2_BANDS,\n",
    "        brightness=widgets.FloatSlider(value=1.0, min=0.1, max=5.0)\n",
    ")\n",
    "def vis_composites(red=S2_BANDS[3],\n",
    "                   green=S2_BANDS[2],\n",
    "                   blue=S2_BANDS[1],\n",
    "                   brightness=1.0):\n",
    "    plt.subplot(2,2,1)\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(data_2017[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title('2017-06-28')\n",
    "    plt.subplot(2,2,2)\n",
    "    band_indices = [S2_BANDS.index(band) for band in [red, green, blue]]\n",
    "    arr = brightness * np.transpose(data_2024[band_indices,...], (1,2,0))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title('2024-06-06')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(y_hat_2017, cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(y_hat_2024, cmap=cmap)\n",
    "    cbar = plt.colorbar(ticks=np.arange(len(LABEL_CLASSES)))\n",
    "    cbar.ax.set_yticklabels(LABEL_CLASSES)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your U-net has been trained long enough, that should look pretty good! Now for the final part:\n",
    "the difference map.\n",
    "\n",
    "For starters, we could just subtract the 2017 prediction from the 2024 prediction. Remember that we\n",
    "assigned value 0 as \"non-forest\" and value 1 as \"forest\". That difference map would then contain\n",
    "three possible values:\n",
    "* -1: forest loss (0 in 2024, 1 in 2017)\n",
    "* 0: no change\n",
    "* 1: forest gain (1 in 2024, 0 in 2017)\n",
    "\n",
    "However, we can go one step further: the \"no change\" case could either be because there has always\n",
    "been forest, or because both timestamps show \"no forest\". Thus, we can shift case 1 (forest gain)\n",
    "one value up to 2 and introduce two new values:\n",
    "* 0: no change (no forest)\n",
    "* 1: no change (forest)\n",
    "\n",
    "We'll do all of that below and then visualise the result with a custom colour map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in prediction\n",
    "pred_diff = y_hat_2024 - y_hat_2017                 # difference: -1 = forest loss; 0 = no change; 1 = forest gain\n",
    "\n",
    "# improve information content w.r.t. no change\n",
    "pred_diff[pred_diff > 0] = 2                        # new value: 2 = forest gain\n",
    "no_change = pred_diff == 0                          # find pixels with zero-difference (no change)\n",
    "pred_diff[no_change] = y_hat_2024[no_change]        # new \"no change\" scenario: 0 = no forest; 1 = forest\n",
    "\n",
    "#Â create custom colour map\n",
    "cmap_diff = matplotlib.colors.ListedColormap([\n",
    "    [1.0, 0.7, 0.4],        # forest loss\n",
    "    [0.6, 0.6, 0.6],        # no change (no forest)\n",
    "    [0.2, 0.6, 0.1],        # no change (forest)\n",
    "    [0.1, 0.9, 0.9]         # forest gain\n",
    "])\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(pred_diff, cmap=cmap_diff)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks(ticks=[-1, 0, 1, 2],\n",
    "               labels=['loss', 'no change (no forest)', 'no change (forest)', 'gain'])\n",
    "plt.title('Predicted change 2017-2024')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, just like that, we have computed a change map for forest cover.\n",
    "\n",
    "Finally, we can calculate the total area gained or lost in forest. \n",
    "\n",
    "ðŸ–Œï¸ Implement this below. Remember that Sentinel-2 has a resolution of 10m, so you should be able to\n",
    "calculate the total area predicted as forest gain, respectively loss, in $km^2$, as well as in\n",
    "percentage of pixels. Report all four quantities to a precision of two decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_gain_km2 = ...\n",
    "forest_gain_percentage = ...\n",
    "forest_loss_km2 = ...\n",
    "forest_loss_percentage = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the values you obtain sound reasonable? Also look at where (and why) difference maps predict\n",
    "forest gain/loss. Depending on the model you have used, you might for example see those diagonal\n",
    "stripes again erroneously influencing the result. Compare with your neighbours to see what kind of\n",
    "spread you get, and think about what could have influenced this variation.\n",
    "\n",
    "\n",
    "**Bonus: geospatial rasters**\n",
    "\n",
    "We can save the change map now as a GeoTIFF that you can open in QGIS, ArcGIS Pro, _etc._:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open one of the GeoTIFFs in read mode to get all the metadata (we chose this file at the start)\n",
    "with rasterio.open(file_path, 'r') as f_band:\n",
    "    meta = f_band.meta\n",
    "    # let's print some of the metadata\n",
    "    print(f'Data type: {meta[\"dtype\"]}')\n",
    "    print(f'Size: {meta[\"width\"]} x {meta[\"height\"]}')\n",
    "    print(f'Count: {meta[\"count\"]}')            # number of bands\n",
    "    print(f'CRS: {meta[\"crs\"]}')                # CRS: coordinate reference system (https://docs.qgis.org/3.34/en/docs/gentle_gis_introduction/coordinate_reference_systems.html)\n",
    "    print(f'Transform:\\n{meta[\"transform\"]}')   # info: https://pygis.io/docs/d_affine.html\n",
    "\n",
    "# we need to update some of the metadata for our change map\n",
    "meta.update({\n",
    "    'count': 1,         # number of bands: we only have one (the change map)\n",
    "    'dtype': 'uint8'    # data type: 8-bit unsigned integer is enough for the four values we have\n",
    "})\n",
    "\n",
    "# save change detection map\n",
    "with rasterio.open('change_map.tiff', 'w', **meta) as f_out:\n",
    "    f_out.write(pred_diff[np.newaxis,...].astype(np.uint8) + 1)     # +1 to make values start at zero (required for unsigned integer data type & colour map)\n",
    "    f_out.write_colormap(\n",
    "        1,                                                          # define colour map for the first (and only) band\n",
    "        dict([idx, [int(255*val) for val in col] + [255]]\n",
    "             for idx, col in enumerate(cmap_diff.colors))           # take all colours in order, convert values to 0-255 int, add extra 255 for alpha value (opacity)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ The metadata we see and print above is part of the (Geo-) TIFF format. It not only defines basic\n",
    "image properties, such as size (width, height), count (number of bands/colour channels) and data\n",
    "type, but also _geospatial_ properties. There are two main ones to know in this context:\n",
    "1. The [Coordinate Reference System\n",
    "   (CRS)](https://docs.qgis.org/3.34/en/docs/gentle_gis_introduction/coordinate_reference_systems.html):\n",
    "   this informs about the _ellipsoid_ that serves as a \"reference\" (a zero-point, if you will) for\n",
    "   the image in geographic space. As you know, Earth isn't a perfect round ball, but an irregular\n",
    "   shape with undulations (valleys, mountains); therefore, an ellipsoid is used that approximates\n",
    "   its shape. Multiple such ellipsoids have been defined, such as [WGS84](https://epsg.io/4326) (a\n",
    "   global ellipsoid, used here) or other ones that are more accurate in some particular areas in the\n",
    "   world. Almost every country has multiple of them ([references for the\n",
    "   UK](https://www.gov.uk/guidance/uk-geospatial-data-standards-coordinate-reference-systems)).\n",
    "2. Once the CRS is defined, we need to know where in relation to it the image comes to lie. For\n",
    "   regularly taken datasets, this can be done with an [affine transformation\n",
    "   matrix](https://pygis.io/docs/d_affine.html) (as used above). Affine matrices allow linear\n",
    "   transformations of images with respect to geospace, such as scaling, translation, rotation,\n",
    "   skewing and shearing. That's accurate enough for a satellite that always follows a steady orbit\n",
    "   around the Earth. As soon as acquisitions can get non-linear, more advanced geocoding methods are\n",
    "   required (think about an aeroplane or drone being caught by a gust of wind during data\n",
    "   acquisition).\n",
    "\n",
    "If you are interested: the above information allows us to easily convert between pixel and\n",
    "geospatial coordinates! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel coordinate we want to translate to geospatial one\n",
    "pixel_coord = [400, 565]        # pixel at 400th column, 565th row\n",
    "\n",
    "# convert to geospatial coordinate: simply multiply with affine transform\n",
    "spatial_coord = meta['transform'] * pixel_coord\n",
    "\n",
    "print(f'Geospatial coordinates: {spatial_coord[1]}, {spatial_coord[0]}')\n",
    "\n",
    "# convert back to pixel coordinates: multiply with inverse transform\n",
    "pixel_coord_back = ~meta['transform'] * spatial_coord\n",
    "\n",
    "print(f'Backtransformed to pixel coordinates: {pixel_coord_back[0]}, {pixel_coord_back[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a map viewer (_e.g._, [Google Maps](https://www.google.co.uk/maps)) and copy-paste the\n",
    "geospatial coordinate pair into the search function. Check whether you end up in the same location.\n",
    "\n",
    "ðŸ’¡ Those spatial coordinates have come out in degrees lat/lon, because that's the format that our\n",
    "CRS (WGS84) defines. Other systems may return metres or something else.\n",
    "\n",
    "ðŸ’¡ Now you know how to sample pixel values from geospatial rasters based on _e.g._ GPS\n",
    "coordinates. ðŸ˜Š\n",
    "\n",
    "\n",
    "You could now go ahead and calculate change maps for other timestamps of the same area if you\n",
    "wanted. Of course, if you want to go back before 2014, you would have to use a different sensor\n",
    "(Landsat for example goes back to the 1970s). Some approaches even allow you to do **data fusion**,\n",
    "such as combining scenes from different satellites, maybe even modalities (for example Synthetic\n",
    "Aperture Radar/SAR, which also allows you to map forests and can see through clouds). This and many,\n",
    "many more things are possible with remote sensing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 5. Summary and Outlook\n",
    "\n",
    "We have taken a look at:\n",
    "1. Optical, multispectral satellite data (from Sentinel-2)\n",
    "2. Spectral indices (in this case, NDVI)\n",
    "3. Semantic segmentation with deep learning (using U-net)\n",
    "4. Mapping of forest cover in satellite imagery\n",
    "5. Change detection (mapping & quantifying deforestation across time)\n",
    "\n",
    "\n",
    "As you have also seen, machine (and especially deep) learning isn't always the optimal solution.\n",
    "Sometimes it's just an overkill. Nonetheless, there are countless cases where a more sophisticated\n",
    "model is needed, also in remote sensing. For example:\n",
    "* Detecting objects of very complicated appearance or very small size (_e.g._, cars): here, spectral\n",
    "  reflectance signatures are not sufficient anymore to distinguish them from the rest, we need (high\n",
    "  spatial resolution and) texture (and a model that can cope with it, such as a U-net).\n",
    "* Mapping vegetation phenology throughout a year: such cases require explicit time series of\n",
    "  satellite products (we cannot reliably measure biomass in winter, for example).\n",
    "\n",
    "\n",
    "###Â Further resources\n",
    "\n",
    "**Forest Monitoring with Remote Sensing**\n",
    "* Nguyen, T.A., RuÃŸwurm, M., Lenczner, G. and Tuia, D., 2024. Multi-temporal forest monitoring in\n",
    "  the Swiss Alps with knowledge-guided deep learning. Remote Sensing of Environment, 305, p.114109.\n",
    "  [https://www.sciencedirect.com/science/article/pii/S0034425724001202](https://www.sciencedirect.com/science/article/pii/S0034425724001202).\n",
    "* Waldeland, A.U., Trier, Ã˜.D. and Salberg, A.B., 2022. Forest mapping and monitoring in Africa\n",
    "  using Sentinel-2 data and deep learning. International Journal of Applied Earth Observation and\n",
    "  Geoinformation, 111, p.102840.\n",
    "  [https://doi.org/10.1016/j.jag.2022.102840](https://doi.org/10.1016/j.jag.2022.102840).\n",
    "  _After today's exercise, this work should sound very intuitive to you._\n",
    "\n",
    "\n",
    "**Deep Learning and Remote Sensing in general**\n",
    "\n",
    "These are some useful publications:\n",
    "* Camps-Valls, G., Tuia, D., Zhu, X.X. and Reichstein, M. eds., 2021. Deep learning for the Earth\n",
    "  Sciences: A comprehensive approach to remote sensing, climate science and geosciences. John Wiley\n",
    "  & Sons. [Google books\n",
    "  preview](https://books.google.ch/books?hl=en&lr=&id=e2c4EAAAQBAJ&oi=fnd&pg=PR16&dq=deep+learning+for+remote+sensing+camps-valls&ots=gHjF752TBl&sig=YYRWkkXFOuYg2GYALeeibJbEuNc#v=onepage&q=deep%20learning%20for%20remote%20sensing%20camps-valls&f=false).\n",
    "* Zhu, X.X., Tuia, D., Mou, L., Xia, G.S., Zhang, L., Xu, F. and Fraundorfer, F., 2017. Deep\n",
    "  learning in remote sensing: A comprehensive review and list of resources. IEEE geoscience and\n",
    "  remote sensing magazine, 5(4), pp.8-36.\n",
    "  [https://doi.org/10.1109/MGRS.2017.2762307](https://doi.org/10.1109/MGRS.2017.2762307).\n",
    "\n",
    "There also is an active community hosting conferences and workshops in the field:\n",
    "* [EarthVision](https://www.grss-ieee.org/events/earthvision-2025/): yearly workshop at the CVPR\n",
    "  conference\n",
    "* [ML4RS](https://ml-for-rs.github.io/iclr2025/): yearly workshop at ICLR\n",
    "\n",
    "\n",
    "Ultimately, there are things we cannot do with remote sensing, as is the case with everything and\n",
    "every modality we have seen in the course. However, when used correctly and in conjunction with\n",
    "other data (if necessary), remote sensing can be an extremely powerful tool for all sorts of\n",
    "ecological analyses. I invite you to think about research questions you would like to answer, and\n",
    "how geospatial analyses and remote sensing can contribute. The sky's the limit (no pun intended)! ðŸ˜ƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bios0032",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
