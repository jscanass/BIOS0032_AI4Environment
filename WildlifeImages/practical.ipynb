{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c0425e",
   "metadata": {},
   "source": [
    "# AI & Wildlife Images\n",
    "\n",
    "In this notebook, you'll explore how AI can be used for large-scale ecological research.\n",
    "We'll use a real-world case study: a camera trap project conducted by a UCL team in Kenya's Maasai Mara ecosystem.\n",
    "\n",
    "Here‚Äôs a brief overview of what you will cover:\n",
    "\n",
    "* Camera Traps: What they are and how they're used in ecology.\n",
    "* Image Annotation: The process of labelling camera trap images.\n",
    "* MegaDetector: An AI tool for automatically detecting animals in photos.\n",
    "* SpeciesNet: An AI tool for automatically identifying the species of those animals.\n",
    "* Model Evaluation: How to measure the performance of these AI tools.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbfc08",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before starting, you need to load the lab's dataset to this colab instance and download some utility scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588b21e",
   "metadata": {},
   "source": [
    "## Run the setup script\n",
    "\n",
    "Run the cell below to complete the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c160cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Setup Script\"\n",
    "%%bash\n",
    "\n",
    "# Need to avoid using the system python\n",
    "export UV_SYSTEM_PYTHON=false\n",
    "\n",
    "# Download utility scripts\n",
    "wget -q https://github.com/mbsantiago/AI-Intervene-Training-Material/raw/refs/heads/main/CameraTrapsAI/colab_utils.py\n",
    "wget -q https://github.com/mbsantiago/AI-Intervene-Training-Material/raw/refs/heads/main/CameraTrapsAI/ct_notebook_utils.py\n",
    "\n",
    "# Create virtual environment for megadetector\n",
    "uv venv .mdvenv/\n",
    "\n",
    "# Activate virtual environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "uv pip install \"setuptools==81.0.0\"\n",
    "uv pip install megadetector\n",
    "uv pip install speciesnet\n",
    "\n",
    "# Download and unzip data\n",
    "gdown https://drive.google.com/uc?id=1iLf6H9Ck87D-g43pVpmz0f_mk0U7dlbF\n",
    "unzip data.zip\n",
    "\n",
    "# Download speciesnet model\n",
    "mkdir -p models/speciesnet\n",
    "curl -L -o model.tar.gz https://www.kaggle.com/api/v1/models/google/speciesnet/pyTorch/v4.0.1a/1/download\n",
    "tar -xvf model.tar.gz --directory models/speciesnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72532afb",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a038d",
   "metadata": {},
   "source": [
    "## Camera traps\n",
    "\n",
    "**Camera traps** are static cameras set up in the wild to monitor animal populations.\n",
    "Typically, they are triggered by motion, capturing an image whenever an animal moves within the camera's field of view.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Wildlife_camera_%2851121829491%29.jpg/960px-Wildlife_camera_%2851121829491%29.jpg\" alt=\"camera trap\" width=\"500\"/>\n",
    "\n",
    "The images from camera traps show which species are in an area and what they're doing.\n",
    "This data reveals how often animals appear, their activity times, and their general behaviour.\n",
    "Camera traps are also relatively easy to set up and are great for capturing a wide range of medium to large animals.\n",
    "\n",
    "By combining this animal data with environmental information, ecologists can answer important questions.\n",
    "For example, a key goal is to understand how wildlife reacts to human pressure.\n",
    "By comparing animal communities in areas with low versus high human impact, it's possible to find thresholds, or tipping points, where the ecosystem changes significantly.\n",
    "Finding these thresholds helps inform conservation decisions, like where to create a protected area or what land-use rules to set.\n",
    "Placing cameras strategically along these areas of varying human impact provides the data needed to answer these kinds of questions.\n",
    "\n",
    "After the cameras are collected, the biggest challenge is going through all the photos.\n",
    "A huge number of these images are \"false triggers\" with no animals, set off by things like waving grass.\n",
    "Even when an animal is in the photo, it can be hard to identify if the view is bad, it's too far away, or it's an unfamiliar species.\n",
    "On top of that, a single project can produce hundreds of thousands or even millions of images.\n",
    "Manually checking every photo is extremely slow and often impossible for large studies.\n",
    "\n",
    "This is one of the key ways AI is changing ecological work.\n",
    "It helps by automating the slow task of sorting photos, which in turn makes large-scale studies using camera traps possible.\n",
    "In this notebook, you will work with an example dataset from the Biome Health Project, learning how to use AI to process and analyse the collected imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bac57e",
   "metadata": {},
   "source": [
    "## Biome Health Project\n",
    "\n",
    "The dataset you'll use comes from the Biome Health Project.\n",
    "A key goal of this project is to study how wildlife responds to different levels of human pressure.\n",
    "By understanding this response in detail, the project aims to identify specific pressure thresholds that can be used to guide conservation actions.\n",
    "\n",
    "<img src=\"https://static.wixstatic.com/media/d56724_6d6b60fecd174d24a714672dafcf00cf~mv2.png/v1/crop/x_8,y_0,w_829,h_588/fill/w_829,h_588,al_c,q_90,enc_avif,quality_auto/Gradient_3.png\" alt=\"Curves of wildlife response to human pressure\" width=\"500\" />\n",
    "\n",
    "This notebook focuses on data from the Greater Maasai Mara Ecosystem in Kenya, a savanna famous for its abundant wildlife.\n",
    "Below you see consecutive frames captured by a camera trap in Kenya.\n",
    "It shows a hyena entering the scene and checking out a buffalo!\n",
    "\n",
    "<img src=\"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/WildlifeImages/hyena.gif?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "The study area contains a mix of zones with different management rules.\n",
    "It includes the Mara Triangle, a highly protected part of the Maasai Mara National Reserve, alongside community-run conservancies.\n",
    "In the National Reserve, protection is very strict and the area is actively patrolled.\n",
    "In the community conservancies, Maasai landowners partner with tourism companies and follow specific rules for grazing their cattle.\n",
    "This setup creates a clear gradient of human and livestock pressure, from highly protected land to areas with more grazing and human presence.\n",
    "\n",
    "To monitor wildlife, a team from UCL and local conservancy staff placed camera traps across the landscape.\n",
    "A systematic approach was used to ensure the entire area was sampled evenly.\n",
    "Over 180 motion-activated cameras were set up across a huge 1,200 km¬≤ area in a 2x2 km grid pattern.\n",
    "In the centre of each grid square, one camera was mounted on a tree or post about 50 cm off the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39bc54",
   "metadata": {},
   "source": [
    "### Explore the landscape\n",
    "\n",
    "Let's start by getting a feel for the landscape where the data was collected.\n",
    "\n",
    "Run the cell below to generate an interactive map of the study area.\n",
    "Each point on the map marks the location of a camera trap.\n",
    "\n",
    "Take a moment to explore the map.\n",
    "Zoom in, zoom out, and pan around the region.\n",
    "Use the layer selection tool on the top right to switch between different views, like satellite imagery, topography, and street maps.\n",
    "To build a better mental picture, search online for images of the \"Maasai Mara National Reserve\".\n",
    "\n",
    "*Note*: Feel free to skip the code for generating the map.\n",
    "We'll explore this in more depth during the AI & Remote Sensing week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce700ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Camera Trap Location Map\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# To generate the map we will use folium for generating an interactive map\n",
    "import folium\n",
    "\n",
    "# Geopandas is another spatial library we are using to read spatial info.\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the information about where the cameras were placed\n",
    "cameras = pd.read_csv(\"data/metadata/cameras.csv\")\n",
    "\n",
    "areas = {\n",
    "    \"Mara Triangle\": {\n",
    "        \"path\": \"data/gis/Triangle/\",\n",
    "        \"color\": \"green\",\n",
    "    },\n",
    "    \"Mara North Conservancy\": {\n",
    "        \"path\": \"data/gis/MaraNorthConservancy/\",\n",
    "        \"color\": \"blue\",\n",
    "    },\n",
    "    \"Motorogi Conservancy\": {\n",
    "        \"path\": \"data/gis/MotorogiConservancy/\",\n",
    "        \"color\": \"orange\",\n",
    "    },\n",
    "    \"Olare Orok Conservancy\": {\n",
    "        \"path\": \"data/gis/OlareOrokConservancy/\",\n",
    "        \"color\": \"orange\",\n",
    "    },\n",
    "    \"Naboisho Conservancy\": {\n",
    "        \"path\": \"data/gis/NaboishoConservancy/\",\n",
    "        \"color\": \"purple\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def style_fn(feature, color):\n",
    "    return {\"fillColor\": color, \"color\": color}\n",
    "\n",
    "\n",
    "def get_regime(location_id):\n",
    "    conservancy = location_id[:-2]\n",
    "    return {\n",
    "        \"MT\": \"Mara Triangle\",\n",
    "        \"MN\": \"Mara North Conservancy\",\n",
    "        \"OMC\": \"Motorogi Conservancy\",\n",
    "        \"NB\": \"Naboisho Conservancy\",\n",
    "    }[conservancy]\n",
    "\n",
    "\n",
    "m = folium.Map(tiles=None)\n",
    "\n",
    "folium.TileLayer(\"OpenTopoMap\", overlay=False).add_to(m)\n",
    "folium.TileLayer(\"Esri.WorldImagery\", overlay=False).add_to(m)\n",
    "folium.TileLayer(\"OpenStreetMap\", overlay=False).add_to(m)\n",
    "\n",
    "crs = \"EPSG:4326\"\n",
    "\n",
    "m.fit_bounds(\n",
    "    [\n",
    "        [cameras.Latitude.min(), cameras.Longitude.min()],\n",
    "        [cameras.Latitude.max(), cameras.Longitude.max()],\n",
    "    ]\n",
    ")\n",
    "\n",
    "for _, row in cameras.iterrows():\n",
    "    regime = get_regime(row[\"Location ID\"])\n",
    "    color = areas[regime][\"color\"]\n",
    "    folium.Marker(\n",
    "        location=[row.Latitude, row.Longitude],\n",
    "        icon=folium.Icon(prefix=\"fa\", icon=\"camera\", color=color),\n",
    "        popup=f\"Location ID = {row['Location ID']}\",\n",
    "    ).add_to(m)\n",
    "\n",
    "for name, data in areas.items():\n",
    "    area = gpd.read_file(data[\"path\"]).to_crs(crs)\n",
    "    layer = folium.GeoJson(\n",
    "        area.to_json(),\n",
    "        style_function=partial(style_fn, color=data[\"color\"]),\n",
    "        name=name,\n",
    "    )\n",
    "    folium.Popup(name).add_to(layer)\n",
    "    layer.add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be42f9a",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Build some initial hypothesis*\n",
    "\n",
    "By looking at the map, consider these questions:\n",
    "\n",
    "1. What major geographical features can you see?\n",
    "   Look for things like rivers, hills, and potential changes in vegetation.\n",
    "2. How might these features affect which animals live there?\n",
    "   For example, would a leopard prefer a rocky outcrop or an open plain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f6edb",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "\n",
    "The dataset for this lab comes from a single year, 2018.\n",
    "For this year alone we collected a total of 2.4 million images.\n",
    "\n",
    "At this stage, we have no idea what's in these photos.\n",
    "They could be rare animals or just empty shots triggered by moving branches.\n",
    "A good first step in any camera trap project is to analyse the metadata (the information about the images), like when and where they were taken.\n",
    "This helps us understand the data collection process itself.\n",
    "\n",
    "Let's visualise the entire 2018 data collection effort.\n",
    "Run the cell below to generate a plot that gives an overview of when and where photos were taken.\n",
    "\n",
    "Each column represents a single camera trap site.\n",
    "Each row is a day of the year (from 1 to 365, but we focus on 260-340).\n",
    "The color of each pixel shows the number of images taken at that site on that day.\n",
    "White means no images were captured.\n",
    "\n",
    "Notice that the site names tell you which area they belong to (e.g., the Mara Triangle or a specific conservancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Number of Images per Day\"\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
    "\n",
    "images_per_day = (\n",
    "    images_metadata.groupby(\n",
    "        [\n",
    "            images_metadata.datetime.dt.day_of_year.rename(\"day_of_year\"),\n",
    "            images_metadata.site_id,\n",
    "        ]\n",
    "    )\n",
    "    .size()\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "images_per_day = images_per_day[images_per_day.index >= 260]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "sns.heatmap(\n",
    "    images_per_day,\n",
    "    ax=ax,\n",
    "    mask=images_per_day == 0,\n",
    "    cmap=\"flare\",\n",
    ")\n",
    "ax.set(xlabel=\"Site ID\", ylabel=\"Day of Year\")\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6772c",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Reflect on the collection effort*\n",
    "\n",
    "Take a close look at the plot and think about these questions:\n",
    "\n",
    "1. Why are there gaps and \"noise\" in the plot?\n",
    "2. Do you see any broad patterns between the different areas?\n",
    "   For example, did data collection start and stop at the same time everywhere?\n",
    "3. Look at the color bar to see the range of values.\n",
    "   Some sites have days with tens of thousands of images.\n",
    "   Does a high image count for a site directly translate to high animal abundance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde3b2f",
   "metadata": {},
   "source": [
    "# Data annotation\n",
    "\n",
    "So, how do you find the animals in 2.4 million photos?\n",
    "Before jumping into AI solutions, it's helpful to understand the traditional, manual approach.\n",
    "\n",
    "The process of reviewing images to record information about them, like which species are present, is called **annotation** or **labelling**.\n",
    "\n",
    "Before AI became common, all camera trap research relied on researchers, experts, or citizen scientists, manually annotating every single photo.\n",
    "While this is incredibly slow, it has a major benefit: it gives the annotator a much closer familiarity with the data.\n",
    "By looking through thousands of images, you gain an intuitive sense of what the data looks like, what to expect, and where potential issues might arise.\n",
    "This hands-on experience is invaluable for correctly interpreting the final results of any analysis.\n",
    "\n",
    "Manual annotation is also essential for building AI models.\n",
    "The human-labelled images serve as the \"ground truth\" used to both train an AI model and test how well it performs.\n",
    "We'll cover that in more detail later.\n",
    "For now, it's time to get a feel for the annotation process yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e5e6f",
   "metadata": {},
   "source": [
    "### Manual annotation\n",
    "\n",
    "Your task is to review a small set of 10 images and detect any animals you see.\n",
    "\n",
    "Run the cell below to launch the annotation tool.\n",
    "\n",
    "Here are your instructions:\n",
    "\n",
    "* Draw a box around every animal you can find.\n",
    "  If there are multiple animals in one photo, make sure to box each one.\n",
    "* Just detect, don't identify.\n",
    "  For this exercise, your only task is to find the animals, not to name their species.\n",
    "* Try to make your boxes as tight as possible around the animal, without including too much background.\n",
    "* Remember, some images will be empty.\n",
    "  If you don't see any animals, just move on to the next one.\n",
    "* Be thorough!\n",
    "  Do your best to find every animal, even if it's small, far away, or partially hidden.\n",
    "\n",
    "When you've finished all 10 images, click the Submit button to save your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Annotator\"\n",
    "from pathlib import Path\n",
    "\n",
    "from ct_notebook_utils import Annotator\n",
    "\n",
    "# path for detection data\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "\n",
    "image_dir = data_dir / \"images\"\n",
    "\n",
    "selected_images = [\n",
    "    Path(\"data/images/2018_NB01_001794.JPG\"),\n",
    "    Path(\"data/images/2018_NB40_002921.JPG\"),\n",
    "    Path(\"data/images/2018_MT22_020230.JPG\"),\n",
    "    Path(\"data/images/2018_OMC11_009862.JPG\"),\n",
    "    Path(\"data/images/2018_NB26_025049.JPG\"),\n",
    "    Path(\"data/images/2018_MN33_009632.JPG\"),\n",
    "    Path(\"data/images/2018_NB26_000679.JPG\"),\n",
    "    Path(\"data/images/2018_MT27_005639.JPG\"),\n",
    "    Path(\"data/images/2018_NB05_002216.JPG\"),\n",
    "    Path(\"data/images/2018_NB47_006890.JPG\"),\n",
    "]\n",
    "\n",
    "# create a list with the numpy arrays that correspond to each of the\n",
    "# images to annotate\n",
    "annotator = Annotator(selected_images)\n",
    "\n",
    "annotator.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a87ae",
   "metadata": {},
   "source": [
    "### Annotation costs\n",
    "\n",
    "That short annotation task gives you a feel for the process.\n",
    "But how does that effort scale up from 10 images to 2.4 million?\n",
    "\n",
    "Run the cell below to see a report on your work from the last exercise.\n",
    "It will show you how long it took to annotate the 10 images and how many animals you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9126cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Annotation Report\"\n",
    "# run this cell straight after your annotation\n",
    "annotation_duration = annotator.duration\n",
    "tagged_image_boxes = annotator.annotations\n",
    "\n",
    "total_images = len(selected_images)\n",
    "\n",
    "# convert time to minutes\n",
    "print(f\"The annotation of the {total_images} images took {annotation_duration}\")\n",
    "\n",
    "# given produced annotation list, we calculate total animals and the number\n",
    "# of images with animals\n",
    "num_animals = sum(x.shape[0] for x in tagged_image_boxes if x is not None)\n",
    "num_non_empty_images = sum(x is not None for x in tagged_image_boxes)\n",
    "annotation_speed = annotation_duration / float(len(selected_images))\n",
    "print(\n",
    "    f\"In total, you found {num_animals} animals across {num_non_empty_images} images while \"\n",
    "    f\"{total_images - num_non_empty_images} out of the {total_images} images were tagged as empty.\\n\"\n",
    "    f\"You tagged 1 image every {annotation_speed}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc000997",
   "metadata": {},
   "source": [
    "Now, let's see what it would take to annotate the entire 2018 dataset.\n",
    "Run the next cell to launch an interactive tool that estimates the total time and cost.\n",
    "\n",
    "Play around with the sliders in the tool to see how the numbers change.\n",
    "You can control:\n",
    "\n",
    "* Number of annotators: How many people are working on the project?\n",
    "* Dataset percentage: Do you need to annotate all the images (100%), or just a smaller fraction?\n",
    "* Hourly pay rate ($): How much would you pay an annotator per hour?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"Extrapolating to the whole dataset\"\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "    num_annotators=(1, 20),\n",
    "    dataset_percentage=(0.0, 1.0, 0.05),\n",
    "    cost_per_hour=(6, 50),\n",
    ")\n",
    "def report_annotation_costs(num_annotators=1, dataset_percentage=1.0, cost_per_hour=17):\n",
    "    total_images = len(images_metadata)\n",
    "\n",
    "    num_images_to_annotate = int(total_images * dataset_percentage)\n",
    "\n",
    "    total_duration = annotation_speed * (num_images_to_annotate / num_annotators)\n",
    "    total_cost = total_duration.total_seconds() * cost_per_hour * num_annotators / 3600\n",
    "    print(\n",
    "        f\"At the same speed it would take {num_annotators} person(s) a total \"\n",
    "        f\"of {total_duration} to annotate {dataset_percentage:.1%} of the data ({num_images_to_annotate:,d} images).\\n\"\n",
    "        f\"This would cost {total_cost:,.2f}¬£ at an hourly rate of {cost_per_hour}¬£.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7a56d",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Annotation costs of a MSc project*\n",
    "\n",
    "Imagine you are analysing a dataset of 1 million images for your Master's thesis.\n",
    "What percentage of the data can you afford to annotate in this timeframe?\n",
    "Consider that this step is only one part of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d950c",
   "metadata": {},
   "source": [
    "# MegaDetector\n",
    "\n",
    "Manually annotating millions of images is clearly not practical.\n",
    "This is where a pre-trained AI model can save a huge amount of time.\n",
    "\n",
    "For this notebook, we will use [**MegaDetector**](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md), a model initially developed by Microsoft's AI for Earth program.\n",
    "It was trained on millions of camera trap images from many different ecosystems, so it's generally robust and reliable.\n",
    "\n",
    "**Note:** Like any AI, MegaDetector learned to detect animals by looking at a huge library of training examples.\n",
    "It works best when your data is similar to what it was trained on. It might struggle with images from unique ecosystems or with different types of imagery (like from drones).\n",
    "No AI is perfect, and it will make mistakes.\n",
    "You can see some examples of known failure cases [here](https://github.com/agentmorris/MegaDetector/blob/main/megadetector-challenges.md).\n",
    "\n",
    "Here are a few key things about it:\n",
    "\n",
    "* MegaDetector's purpose is to find and draw boxes around three types of objects: animals, people, and vehicles.\n",
    "  It doesn't identify the species of the animal, it just finds it.\n",
    "\n",
    "* MegaDetector is \"pretrained\".\n",
    "  This means it has already been trained on a large, general dataset.\n",
    "  We can take this general, pre-trained model and apply it directly to our own images without any additional train.\n",
    "\n",
    "* For every box it draws, MegaDetector provides a confidence score between 0 and 1.\n",
    "  A score close to 1.0 means the model is very certain about its detection (e.g., \"I'm 98% sure this is an animal\").\n",
    "\n",
    "* We can use these scores to automatically filter out empty images.\n",
    "  By setting a confidence threshold, we can decide that any detection below a certain score isn't a \"real\" detection.\n",
    "  Choosing this threshold is a trade-off: a lower threshold means you might find more animals but will also have to check more false positives.\n",
    "\n",
    "* Modern versions of MegaDetector use the YOLOv5 architecture.\n",
    "  YOLO (You Only Look Once) is a family of convolutional neural networks that has become an industry standard for real-time object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c747b",
   "metadata": {},
   "source": [
    "## Running MegaDetector\n",
    "\n",
    "It's time to run MegaDetector on the same 10 images you annotated manually.\n",
    "This will give you a direct comparison between your work and the AI's predictions.\n",
    "\n",
    "Using an AI model isn't always simple.\n",
    "In general, you have to find the model's code repository (like on GitHub) and read the developers' notes to get it working.\n",
    "Most models are built with deep learning frameworks (like [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/)), and using them often requires some Python scripting.\n",
    "\n",
    "Fortunately, because MegaDetector is so popular, there are many ways to use it:\n",
    "\n",
    "* Desktop Apps: User-friendly tools like [Addax](https://addaxdatascience.com/addaxai/) provide a graphical interface.\n",
    "* Cloud Services: Platforms like [Wildlife¬†Insights](https://wildlifeinsights.org/) have integrated MegaDetector into their workflow.\n",
    "* Code: You can always run it directly from a Python script, see its [official¬†docs](https://megadetector.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd2940",
   "metadata": {},
   "source": [
    "### The command line\n",
    "\n",
    "Here we'll use another common method: the command line.\n",
    "This approach allows us to run the model by typing a \"command\" with a few parameters, telling it where the images are and how to process them.\n",
    "It's a great way to run tools without needing a graphical interface or writing a full script.\n",
    "\n",
    "**Note:** If you're new to the command line, it is a text-based way to give instructions to your computer.\n",
    "You type commands into an application called a \"terminal\" to run programs or manage files.\n",
    "\n",
    "Run the cell below.\n",
    "It contains the command-line instruction to run MegaDetector on the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18779669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# activate the virtual environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# Run the megadetector command\n",
    "python -m megadetector.detection.run_detector_batch MDV5A \\\n",
    "  \"data/images/\" \\\n",
    "  \"data/results/md_detections.json\" \\\n",
    "  --output_relative_filenames \\\n",
    "  --threshold 0.2 \\\n",
    "  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b91497",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Getting help with a CLI command*\n",
    "\n",
    "As we've seen, running a command line tool often requires specific \"arguments\".\n",
    "Luckily, most tools include a built-in manual to help you out.\n",
    "Run the cell below to print the help message.\n",
    "Read the descriptions and explain \n",
    "1. What does the MDV5A in the previous command refer to?\n",
    "2. What do the `threshold` and `batch_size` arguments control?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# activate the virtual environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# Run the command with the --help flag\n",
    "python -m megadetector.detection.run_detector_batch --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e787ec",
   "metadata": {},
   "source": [
    "### Inference speed\n",
    "\n",
    "Once the process is finished, the cell output will show some statistics about how fast the model ran.\n",
    "\n",
    "Find the line that looks like this:\n",
    "\n",
    "    Finished inference for N images in M minutes and x seconds (y images per second)\n",
    "\n",
    "Take note of the processing speed (the \"images per second\" value), as we'll use in the cell below.\n",
    "\n",
    "**Note:** The first time you run the model, it takes a bit longer because the model files have to be downloaded and prepared for processing.\n",
    "This is a one-time setup cost.\n",
    "To get a more accurate measure of the model's true processing speed, it's a good idea to run the cell a second time and use that value.\n",
    "\n",
    "Now that you have the processing speed, let's extrapolate.\n",
    "How long would it take MegaDetector to process the entire 2.4 million image dataset?\n",
    "\n",
    "Run the cell below to launch another interactive tool.\n",
    "Input your \"images per second\" value from the previous step.\n",
    "You can also use the slider to see how the total time changes if you only need to process a fraction of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"MegaDetector Speed\"\n",
    "import datetime\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "images_metadata = pd.read_parquet(\"data/metadata/images.parquet\")\n",
    "\n",
    "\n",
    "@widgets.interact(\n",
    "    images_per_sec=widgets.BoundedFloatText(\n",
    "        value=0.28,\n",
    "        min=0,\n",
    "        max=10.0,\n",
    "        step=0.01,\n",
    "        description=\"Images per Sec:\",\n",
    "        disabled=False,\n",
    "    ),\n",
    "    dataset_percentage=(0.0, 1.0, 0.05),\n",
    ")\n",
    "def compute_megadetector_costs(\n",
    "    images_per_sec=0.28,\n",
    "    dataset_percentage=1,\n",
    "):\n",
    "    duration = datetime.timedelta(seconds=1 / images_per_sec)\n",
    "    images_to_process = int(dataset_percentage * len(images_metadata))\n",
    "    total_duration = duration * images_to_process\n",
    "    print(\n",
    "        f\"MegaDetector would take {total_duration} to process {dataset_percentage:.0%} of the dataset (i.e. {images_to_process} images)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b33b5",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Inference cost of an MSc project*\n",
    "\n",
    "Think back to the 1 million image MSc project scenario.\n",
    "Based on the estimates we just calculated:\n",
    "\n",
    "- Would you be able to process the whole dataset with MegaDetector in time for your deadline?\n",
    "  How long would it take?\n",
    "- Would it be possible to use Google Drive + Colab to store and process the images?\n",
    "  Consider the total file size of 1 million images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ca038",
   "metadata": {},
   "source": [
    "### Hardware choices\n",
    "\n",
    "It's important to know that the model's processing speed heavily depends on the computer's hardware.\n",
    "\n",
    "Most AI models run massively faster on a GPU compared to a standard CPU.\n",
    "Not every computer has a compatible GPU, and they can be expensive.\n",
    "However, using one can accelerate the process by orders of magnitude, turning a task that takes weeks into one that takes only a day.\n",
    "\n",
    "**Note:** Curious to see the difference?\n",
    "You can change the hardware for this Colab notebook.\n",
    "Go to the \"Runtime\" menu at the top, select \"Change runtime type\", and choose a different \"Hardware accelerator\" (like GPU or CPU).\n",
    "\n",
    "**Warning:** Changing the runtime will disconnect you and start a new session, so you'll have to run all the setup steps from the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bbf95",
   "metadata": {},
   "source": [
    "## MegaDetector outputs\n",
    "\n",
    "When MegaDetector finished, it saved all its findings into a single output file located at: `data/results/md_detections.json`.\n",
    "\n",
    "This is a `JSON` file, a common format for storing structured data.\n",
    "While researchers often work with tables (like `CSV` files or Excel spreadsheets), `JSON` is widely used in software and on the web.\n",
    "It's a text file, but the information is organised in a specific, nested way that can look a bit confusing at first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21142e46",
   "metadata": {},
   "source": [
    "### Raw JSON\n",
    "\n",
    "Let's get a feel for what this raw data format looks like.\n",
    "\n",
    "Use the file browser on the left to navigate to the `md_detections.json` file and double-click to open it.\n",
    "It will appear in a new tab.\n",
    "\n",
    "Don't worry about understanding every detail.\n",
    "The goal is just to see how the information is structured.\n",
    "When you're done, close the tab to come back to this notebook.\n",
    "\n",
    "Since `JSON` is such a common format, it's worth getting familiar with it.\n",
    "If you're curious to learn more, you can read about it [here](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976811a2",
   "metadata": {},
   "source": [
    "### Read the file with code\n",
    "\n",
    "Now, let's use code to read that same JSON file and pull out a summary of what MegaDetector found.\n",
    "Did you get the same number of animals?\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \"MegaDetector Report\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to the output JSON file\n",
    "data_dir = Path(\"data\")\n",
    "output_file_path = data_dir / \"results\" / \"md_detections.json\"\n",
    "\n",
    "# Read the file as text\n",
    "json_text = Path(output_file_path).read_text()\n",
    "\n",
    "# \"Parse\" or convert the json text into a Python dictionary\n",
    "md_output = json.loads(json_text)\n",
    "\n",
    "selected_images = [\n",
    "    \"2018_NB01_001794.JPG\",\n",
    "    \"2018_NB40_002921.JPG\",\n",
    "    \"2018_MT22_020230.JPG\",\n",
    "    \"2018_OMC11_009862.JPG\",\n",
    "    \"2018_NB26_025049.JPG\",\n",
    "    \"2018_MN33_009632.JPG\",\n",
    "    \"2018_NB26_000679.JPG\",\n",
    "    \"2018_MT27_005639.JPG\",\n",
    "    \"2018_NB05_002216.JPG\",\n",
    "    \"2018_NB47_006890.JPG\",\n",
    "]\n",
    "\n",
    "# Create variables to keep track of empty images and total num of animals detected\n",
    "total_animals = 0\n",
    "empty_images = 0\n",
    "\n",
    "# Read the info on each of the images\n",
    "for file_info in md_output[\"images\"]:\n",
    "\n",
    "    # We will only extract info of the selected images\n",
    "    if not Path(file_info[\"file\"]).name in selected_images:\n",
    "        continue\n",
    "\n",
    "    # Count the number of detections in the image\n",
    "    detections = len(file_info[\"detections\"])\n",
    "\n",
    "    # Add it to the total tally\n",
    "    total_animals += detections\n",
    "\n",
    "    if detections == 0:\n",
    "        # If no detections add this image to the empty images tally\n",
    "        empty_images += 1\n",
    "\n",
    "# Compute the number of images with animals\n",
    "total_images = len(selected_images)\n",
    "non_empty_images = total_images - empty_images\n",
    "\n",
    "# Print a report\n",
    "print(\n",
    "    f\"In total, MegaDetector found {total_animals} animals across \"\n",
    "    f\"{non_empty_images} images while {empty_images} out of the \"\n",
    "    f\"{total_images} images were tagged as empty.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54e138",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Are all detections animals?*\n",
    "\n",
    "In the previous step, we counted every single detection.\n",
    "However, MegaDetector identifies animals, humans, and vehicles.\n",
    "Write code below to ensure we are counting only the animals.\n",
    "\n",
    "*Hint:* Every detection has a category label.\n",
    "Look inside `md_output['detection_categories']` to find out which label ID belongs to animals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f6f5d",
   "metadata": {},
   "source": [
    "### Visualise the outputs\n",
    "\n",
    "While the JSON file contains all the information, it's not easy to interpret on its own.\n",
    "Visualizing the model's detections is always a good idea to make inspecting the results much easier.\n",
    "\n",
    "First, run the cell below.\n",
    "It will create a new folder with copies of the original images, but with MegaDetector's detections (the bounding boxes, labels, and confidence scores) drawn on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# activate the virtual environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# run another megadetector command to generate visualisations\n",
    "python -m megadetector.visualization.visualize_detector_output \\\n",
    "  --images_dir \"data/images/\" \\\n",
    "  \"data/results/md_detections.json\" \\\n",
    "  \"data/results/md_viz/\" \\\n",
    "  --output_image_width 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef6c5b",
   "metadata": {},
   "source": [
    "Now, run the next cell to display those annotated images in an interactive viewer right here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c29760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualise MegaDetector Outputs\n",
    "from pathlib import Path\n",
    "\n",
    "from ct_notebook_utils import image_tabs\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "md_viz_dir = data_dir / \"results\" / \"md_viz\"\n",
    "\n",
    "selected_images = [\n",
    "    \"2018_NB01_001794.JPG\",\n",
    "    \"2018_NB40_002921.JPG\",\n",
    "    \"2018_MT22_020230.JPG\",\n",
    "    \"2018_OMC11_009862.JPG\",\n",
    "    \"2018_NB26_025049.JPG\",\n",
    "    \"2018_MN33_009632.JPG\",\n",
    "    \"2018_NB26_000679.JPG\",\n",
    "    \"2018_MT27_005639.JPG\",\n",
    "    \"2018_NB05_002216.JPG\",\n",
    "    \"2018_NB47_006890.JPG\",\n",
    "]\n",
    "\n",
    "image_tabs(\n",
    "    [\n",
    "        path\n",
    "        for path in md_viz_dir.glob(\"*.JPG\")\n",
    "        if path.name.replace(\"anno_\", \"\") in selected_images\n",
    "    ],\n",
    "    width=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ac7db",
   "metadata": {},
   "source": [
    "Use the viewer to look through the 10 images.\n",
    "Compare the AI's detections with the annotations you made earlier and think about the following questions:\n",
    "\n",
    "1. How did the AI do?\n",
    "   Did MegaDetector find the same animals that you did?\n",
    "2. Did the AI miss anything?\n",
    "   If you found an animal that MegaDetector missed, why do you think it failed?\n",
    "3. Did the AI find anything you missed?\n",
    "   Sometimes the model can spot things that are easy for a human eye to overlook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9277a",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Lowest confidence detection*\n",
    "\n",
    "Can you find the least confident guess the model made?\n",
    "Write code below to search through the detections in `md_output` and identify the minimum score.\n",
    "Provide the filename in which it appears and the confidence value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505afc47",
   "metadata": {},
   "source": [
    "# Species classification\n",
    "\n",
    "MegaDetector is a great first step for processing camera trap data, as it helps filter out all the empty images.\n",
    "But if you want to identify which species are present, MegaDetector can't help.\n",
    "The next step is typically to manually review all the non-empty images, or perhaps to build a custom species classifier.\n",
    "However, a new tool called [**SpeciesNet**](https://github.com/google/cameratrapai) offers another powerful, pre-trained solution, similar to MegaDetector but for species identification.\n",
    "\n",
    "Both models were trained on huge amounts of data.\n",
    "SpeciesNet, for example, was trained on around 65 million images from all over the globe (see [this¬†paper](http://doi.org/10.1049/cvi2.12318) for more details).\n",
    "These images were collected, manually annotated by many different research teams, and then shared on platforms like Wildlife Insights.\n",
    "The result is a model that synthesises a vast amount of expert knowledge to identify animals in photos.\n",
    "Both models are also free and open-source, meaning you can inspect their code and use them however you like, as long as you attribute them correctly.\n",
    "\n",
    "SpeciesNet is different from MegaDetector because it doesn't locate where an animal is in a photo.\n",
    "Instead, it looks at the *whole image* and tries to identify the species shown.\n",
    "Its current version can recognise 2,000 different classes of animals.\n",
    "If it can't identify the exact species, it will attempt to name a higher taxonomic category (like genus or family).\n",
    "Like MegaDetector, SpeciesNet also outputs a confidence score for each classification.\n",
    "\n",
    "In most scenarios, the two models are best used together.\n",
    "The ideal workflow is to run MegaDetector first to find and locate all the animals, then crop the image around each detection, and finally, run SpeciesNet on those smaller, cropped images to identify the species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c179f82",
   "metadata": {},
   "source": [
    "## Using SpeciesNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f735672",
   "metadata": {},
   "source": [
    "### Run the SpeciesNet command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb55f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# activate the virtual environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# this command runs megadetector and speciesnet in tandem\n",
    "# note that we are specifying that the images are from Kenya\n",
    "python -m megadetector.detection.run_md_and_speciesnet \\\n",
    "    --classification_model models/speciesnet \\\n",
    "    \"data/images/\" \\\n",
    "    \"data/results/speciesnet_predictions.json\" \\\n",
    "    --country KEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a5721",
   "metadata": {},
   "source": [
    "### Visualise the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5718cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Activate the environment\n",
    "source .mdvenv/bin/activate\n",
    "\n",
    "# Run the visualise command\n",
    "python -m megadetector.visualization.visualize_detector_output \\\n",
    "    --images_dir \"data/images/\" \\\n",
    "    \"data/results/speciesnet_predictions.json\" \\\n",
    "    \"data/results/speciesnet_viz\" \\\n",
    "    --output_image_width 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a82dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualise SpeciesNet Outputs\n",
    "from pathlib import Path\n",
    "\n",
    "from ct_notebook_utils import image_tabs\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "speciesnet_viz_dir = data_dir / \"results\" / \"speciesnet_viz\"\n",
    "\n",
    "image_tabs([path for path in speciesnet_viz_dir.glob(\"*.JPG\")], show_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c8ad8",
   "metadata": {},
   "source": [
    "## SpeciesNet outputs\n",
    "\n",
    "The output from SpeciesNet is saved in a JSON file, very similar to the one MegaDetector produced.\n",
    "\n",
    "This format is useful because it's compatible with other tools.\n",
    "For example, you could load these results into a graphical interface like [TimeLapse](https://timelapse.ucalgary.ca/) to review them visually.\n",
    "For analysis, however, it's often easier to work with a simple table (like a CSV file).\n",
    "\n",
    "Let's convert the JSON output into a more familiar tabular format.\n",
    "The cell below extracts all the detection and classification information and saves it as a CSV file named `ai_outputs.csv`.\n",
    "Run the cell, then use the file browser on the left to find and open the new CSV file to see what the final data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9aaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Turn Detections into Table\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the speciesnet output file\n",
    "data_dir = Path(\"data\")\n",
    "outputs_path = data_dir / \"results\" / \"speciesnet_predictions.json\"\n",
    "\n",
    "# Read the JSON file as text\n",
    "outputs_txt = outputs_path.read_text()\n",
    "\n",
    "# Parse to python dictionary\n",
    "speciesnet_results = json.loads(outputs_txt)\n",
    "\n",
    "# Get the list of species categories (names) known to the model\n",
    "classes = speciesnet_results[\"classification_categories\"]\n",
    "\n",
    "# Create an empty list to store our simplified detection data\n",
    "species_df = []\n",
    "\n",
    "# Loop through the results for each image\n",
    "for file_predictions in speciesnet_results[\"images\"]:\n",
    "\n",
    "    # Loop through every detection found in that image\n",
    "    for detection in file_predictions[\"detections\"]:\n",
    "\n",
    "        # Skip if the detection is not an animal\n",
    "        if detection[\"category\"] != \"1\":\n",
    "            continue\n",
    "\n",
    "        # Skip if the model didn't provide any species classification\n",
    "        if \"classifications\" not in detection:\n",
    "            continue\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        minx, miny, width, height = detection[\"bbox\"]\n",
    "\n",
    "        # Get the top species prediction.\n",
    "        # The model returns a list of guesses sorted by confidence; \n",
    "        # we take the first one (index 0) as it is the most likely.\n",
    "        class_num, class_score = detection[\"classifications\"][0]\n",
    "\n",
    "        # Map the class ID to the actual species name\n",
    "        class_name = classes[class_num]\n",
    "\n",
    "        # Skip if speciesnet thinks the \"animal\" detected by megadetector\n",
    "        # is actually just blank background\n",
    "        if class_name == \"blank\":\n",
    "            continue\n",
    "\n",
    "        # Append the relevant details to our list\n",
    "        species_df.append(\n",
    "            {\n",
    "                \"filepath\": file_predictions[\"file\"],\n",
    "                \"detection_score\": detection[\"conf\"],\n",
    "                \"class\": class_name,\n",
    "                \"class_score\": class_score,\n",
    "                \"minx\": minx,\n",
    "                \"miny\": miny,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert the list into a pandas DataFrame\n",
    "species_df = pd.DataFrame(species_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "species_df.to_csv(\"ai_outputs.csv\", index=False)\n",
    "print(\"CSV saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7be27",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Most common species*\n",
    "\n",
    "Use the `species_df` dataframe to answer these two questions:\n",
    "\n",
    "1. Which species (or class) was detected most frequently?\n",
    "2. Is there a noticeable difference in the average confidence scores between species?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121c815",
   "metadata": {},
   "source": [
    "# Evaluating AI outputs\n",
    "\n",
    "After running the AI models, we have a neat table of species detections.\n",
    "But are they correct?\n",
    "As you've probably noticed from the examples, the model's outputs aren't always accurate.\n",
    "Even models trained on millions of images will make mistakes, especially if your data looks very different from their training data.\n",
    "\n",
    "It is **critical** to get a quantitative measure of how well a model performs on your specific data.\n",
    "Without this step, you risk basing your research on \"model hallucinations\" rather than real ecological patterns.\n",
    "A good starting point is to read the performance reports from the model's creators, like the [accompanying¬†paper](https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.12318) for SpeciesNet.\n",
    "However, it is **strongly recommended** that you always perform your own independent evaluation on your own dataset.\n",
    "\n",
    "Evaluating a model means comparing its predictions to a set of correct or *\"ground truth\"* answers.\n",
    "To create dataset a dataset for testing the model you need to manually annotate a subset of your own images.\n",
    "These annotations represent what you want the model to output.\n",
    "By comparing the model's predictions to this ground truth, you can calculate a suite of performance metrics that measure how well the model is doing its job. This is why manual annotation remains a vital step in any AI-assisted study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c5919",
   "metadata": {},
   "source": [
    "## Designing an evaluation dataset\n",
    "\n",
    "How should you choose which images to manually annotate?\n",
    "The way you select your sample can significantly impact your understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96c9b5",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Comparing two common sampling strategies*\n",
    "\n",
    "Consider the pros and cons of these two common methods for selecting images to annotate for your ground truth:\n",
    "\n",
    "* Method A: Random Image Sample\n",
    "\n",
    "    Select a completely random sample of 1,000-2,000 images from the entire dataset and manually annotate everything in them.\n",
    "\n",
    "* Method B: Species-Stratified Sample\n",
    "\n",
    "    For each species you care about, select 20-30 random images that the AI has already labelled as that species, and then manually check if the AI was correct.\n",
    "\n",
    "Think about what each method would allow you to measure.\n",
    "For example:\n",
    "\n",
    "- Which method is better for evaluating how well the model filters out empty images?\n",
    "- What are the potential biases of Method B?\n",
    "- What are the risks of Method A?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b46b2",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*What else needs consideration?*\n",
    "\n",
    "A random sample, while seemingly unbiased, can easily miss or underrepresent important conditions in your data.\n",
    "For example, even if 30% of your images were taken at night, a small random sample might happen to include very few of them.\n",
    "If the model performs poorly in the dark, your evaluation would fail to represent this weakness, giving you a misleadingly optimistic view of its overall performance.\n",
    "\n",
    "Beyond just day vs. night, what other factors should you consider to ensure your evaluation set is representative and tests the model under different conditions?\n",
    "List a few examples of how you might stratify your sample to get a more complete picture of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f3d5a",
   "metadata": {},
   "source": [
    "## Measuring model performance\n",
    "\n",
    "Now, let's compare the AI's predictions to a \"ground truth\" dataset of annotations already prepared by the UCL team.\n",
    "\n",
    "There are many ways to measure performance, and the \"best\" method depends on your research question.\n",
    "The AI pipeline above gives us a lot of detail: it detects multiple animals, provides their exact locations with bounding boxes, and identifies their species.\n",
    "While this level of detail is useful for some studies, many ecological questions only require knowing that a certain species was present at a site.\n",
    "For analyses like occupancy modeling, it doesn't matter how many individuals were in the photo or exactly where they were.\n",
    "\n",
    "For this exercise, let's assume that's our goal: we just want to know if the AI can correctly tell us which species were present in each image, matching the answers provided by human experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c855c",
   "metadata": {},
   "source": [
    "### Species labels\n",
    "\n",
    "An important and surprisingly tricky step is to make sure we are comparing apples to apples.\n",
    "This means ensuring that the species labels from our ground truth match the labels used by the AI model.\n",
    "\n",
    "Because the manual annotations and the SpeciesNet model were created independently, their labels don't perfectly align.\n",
    "Run the cell below to see a comparison of the two sets of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a79dae",
   "metadata": {
    "id": "f7d2ded6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Label Comparison\n",
    "from itertools import zip_longest\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "annotations = pd.read_csv(\"data/metadata/labels.csv\")\n",
    "\n",
    "ann_labels = annotations.label.sort_values().unique()\n",
    "pred_labels = species_df[\"class\"].sort_values().unique()\n",
    "\n",
    "print(f\"{'Annotation Labels':^20} | {'SpeciesNet Labels':^20} \")\n",
    "print(f\"{'-' * 20} | {'-' * 20} \")\n",
    "for ann_lab, pred_lab in zip_longest(ann_labels, pred_labels, fillvalue=\"\"):\n",
    "    print(f\"{ann_lab:^20} | {pred_lab:^20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa999ff",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Mapping species labels*\n",
    "\n",
    "Take a look at the two lists and think about the following questions:\n",
    "\n",
    "* How would you match these two different sets of labels to allow for a fair comparison?\n",
    "* If you were starting a new camera trap project today, how could you plan your annotation process to avoid this problem from the start?\n",
    "  is it always possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519adfa",
   "metadata": {},
   "source": [
    "### Understanding different types of errors\n",
    "\n",
    "Let's focus our analysis on just one species: the impala.\n",
    "For this exercise, we'll keep it simple and only count a prediction as correct if SpeciesNet gives the specific \"impala\" label, ignoring higher-level classifications like \"bovidae family\".\n",
    "\n",
    "Our AI pipeline gives us two confidence scores for each detection, but for now, we'll just focus on the final species classification score from SpeciesNet.\n",
    "\n",
    "Run the cell below.\n",
    "It will go through each image in our ground truth set and calculate two things:\n",
    "1. Does the image actually contain an impala (based on the manual labels)?\n",
    "2. What was the AI's highest confidence score for \"impala\" in that image?\n",
    "   (This will be 0 if the AI found no impalas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37355800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Get Per Image Info\n",
    "gt_classes = [\"impala\"]\n",
    "pred_classes = [\"impala\"]\n",
    "\n",
    "\n",
    "def get_pred_score(group):\n",
    "    preds = group[group[\"class\"].isin(pred_classes)]\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        return 0\n",
    "\n",
    "    return preds[\"class_score\"].max()\n",
    "\n",
    "\n",
    "comparison = (\n",
    "    annotations.groupby(\"filename\")\n",
    "    .label.apply(lambda group: (group.isin(gt_classes)).any())\n",
    "    .rename(\"ground_truth\")\n",
    "    .to_frame()\n",
    "    .join(\n",
    "        species_df.groupby(\"filepath\")\n",
    "        .apply(get_pred_score, include_groups=False)\n",
    "        .rename(\"score\")\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d111f68",
   "metadata": {},
   "source": [
    "Now we can use a confidence threshold to make a decision.\n",
    "If an image's score is above the threshold, we'll say the AI predicted \"impala\".\n",
    "If the score is below, we'll say it did not.\n",
    "\n",
    "By comparing the AI's prediction to the ground truth for each image, we get one of four possible outcomes:\n",
    "\n",
    "* True Positive (TP): The image has an impala, and the AI correctly predicted it.\n",
    "* False Negative (FN): The image has an impala, but the AI missed it.\n",
    "* False Positive (FP): The image does not have an impala, but the AI said it did.\n",
    "* True Negative (TN): The image does not have an impala, and the AI correctly said it didn't.\n",
    "\n",
    "Using these four outcomes, we can calculate two standard performance metrics:\n",
    "\n",
    "* **Precision**: Of all the times the AI predicted \"impala\", what fraction was it correct?\n",
    "\n",
    "  $$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "* **Recall**: Of all the images that truly contained an impala, what fraction did the AI find?\n",
    "\n",
    "  $$ Precision = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "The values of Precision and Recall depend entirely on the confidence threshold you choose.\n",
    "Play with the slider in the next cell to see how they change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1397d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# @title Compute Precision and Recall\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "@widgets.interact(threshold=(0, 0.99, 0.01))\n",
    "def compute_precision_recall(threshold=0.8):\n",
    "    gt = comparison[\"ground_truth\"]\n",
    "    preds = comparison[\"score\"] >= threshold\n",
    "    tp = (preds & gt).sum()\n",
    "    fp = (preds & ~gt).sum()\n",
    "    fn = (~preds & gt).sum()\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 1\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    print(f\"{precision=:.1%} {recall=:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757624a",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Precision-recall curve and Average Precision*\n",
    "\n",
    "To understand model performance across all possible confidence thresholds, we use a Precision-Recall (PR) curve.\n",
    "This plot visualises the trade-off: as you lower the threshold to find more animals (increasing recall), you typically get more false positives (decreasing precision).\n",
    "We can summarise this entire curve into a single number called Average Precision (AP).\n",
    "Formally, this is the area under the PR curve.\n",
    "It allows us to compare different models without having to pick a specific threshold first.\n",
    "\n",
    "Use `scikit-learn` to plot the Precision-Recall curve for our model.\n",
    "Compute the `average_precision_score`.\n",
    "\n",
    "*Hint*: Search online for \"scikit-learn precision recall curve\" or look for the `PrecisionRecallDisplay` documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3454f88",
   "metadata": {},
   "source": [
    "### Final reflections\n",
    "\n",
    "Reflect on these final questions:\n",
    "\n",
    "* Think back to your own manual annotation exercise.\n",
    "  How do you think your performance at identifying impalas would compare to the AI's?\n",
    "* We've been assuming that the human annotations are 100% correct.\n",
    "  Is that a safe assumption in a real-world project?\n",
    "  What factors could affect the reliability of human annotators?\n",
    "* We focused on evaluating species classification.\n",
    "  How would you design an evaluation for the first step‚Äîthe detection of animals (did MegaDetector find the animal and draw an accurate box)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f64836",
   "metadata": {},
   "source": [
    "# Ecological analysis\n",
    "\n",
    "We now assume that we have processed the full dataset using both MegaDetector and SpeciesNet.\n",
    "We have also applied a confidence threshold to filter out uncertain predictions, leaving us with a clean, high-quality dataset.\n",
    "\n",
    "With the technical work behind us, we can shift our focus to the ecology.\n",
    "Recall our main objective from the introduction: How does human pressure affect wildlife distribution in the Mara?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b7e7",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Let's start by loading the detection data.\n",
    "\n",
    "*Note*: We will be loading the manually annotated data here, rather than the SpeciesNet output.\n",
    "However, apart from the confidence scores, the data is almost identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa86c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the detections\n",
    "detections = pd.read_parquet(\"data/metadata/all_detections.parquet\")\n",
    "\n",
    "detections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7ac3b",
   "metadata": {},
   "source": [
    "## Covariates\n",
    "\n",
    "Our team has calculated the following environmental covariates for each camera site using field measurements, remote sensing data, and the images themselves:\n",
    "\n",
    "1. `water_distance`: Distance to the closest water body (in meters).\n",
    "2. `structure_distance`: Distance to the closest man-made structure (in meters).\n",
    "3. `human_distance`: Distance to the closest human settlement (in meters).\n",
    "4. `fence_distance`: Distance to the closest fence fragment (in meters).\n",
    "5. `savi_mean`: Average SAVI values across the year.\n",
    "   SAVI (Soil Adjusted Vegetation Index) is a remote sensing metric that measures vegetation \"greenness\" while correcting for soil brightness.\n",
    "6. `shoat_rate`: The relative abundance of sheep and goats.\n",
    "   Computed as `(Independent sightings / Days active) * 100` where, \"Independent\" means sightings are at least 30 minutes apart.\n",
    "\n",
    "7. `cattle_rate`: The relative abundance of cattle.\n",
    "   Computed as `(Independent sightings / Days active) * 100`\n",
    "\n",
    "*Note:* In the Remote Sensing week, we will learn exactly how to compute indices like SAVI from satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = pd.read_csv(\"data/metadata/covariates.csv\")\n",
    "\n",
    "covariates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf7d3b",
   "metadata": {},
   "source": [
    "## Absence and presence data\n",
    "\n",
    "As a first approach, let's simplify our data to understand presence vs. absence.\n",
    "Instead of counting how many times a species appears, we just want to know if it appears at a site.\n",
    "\n",
    "Raw counts can sometimes be misleading.\n",
    "For example, a single curious individual hanging around a camera trap for an hour might generate 100 images, which makes it look like the site is teeming with animals when it's actually just one very photogenic hyena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by calculating how many times each species appears at each site\n",
    "# 1. Group by site and species (label)\n",
    "# 2. Count the size of each group (number of detections)\n",
    "# 3. 'Unstack' the index to pivot the table:\n",
    "#    - Rows = Sites\n",
    "#    - Columns = Species\n",
    "# 4. Fill missing values with 0 (since no detection implies 0 presence)\n",
    "counts_matrix = (\n",
    "    detections.groupby([\n",
    "        \"site_id\",\n",
    "        \"label\",\n",
    "    ])\n",
    "    .size()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Convert to a binary matrix:\n",
    "# True if the species was seen at least once, False otherwise.\n",
    "presence_matrix = counts_matrix > 0\n",
    "\n",
    "presence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cc0f5",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Species co-occurrence*\n",
    "\n",
    "Do certain animals hang out in the same places, while others avoid each other?\n",
    "\n",
    "We can measure this by calculating the **correlation** between the presence patterns of different species.\n",
    "A correlation close to 1 suggests that species tend to appear at the same sites (perhaps sharing a habitat), while a value near -1 indicates they generally avoid each other (due to predator avoidance or different habitat needs).\n",
    "A correlation near 0 implies there is no visible pattern of grouping or avoidance.\n",
    "\n",
    "First, filter your presence_matrix to include only the species listed in the code below.\n",
    "Then, use the pandas `.corr()` method to calculate the correlation matrix for these specific animals.\n",
    "Finally, visualise your results using a `seaborn.heatmap` (you can check this [example](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)).\n",
    "\n",
    "Once your heatmap is generated, identify which pair of species has the highest positive correlation and which has the lowest (most negative).\n",
    "Why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [\n",
    "    \"zebra\",\n",
    "    \"waterbuck\",\n",
    "    \"wildebeest\",\n",
    "    \"baboon\",\n",
    "    \"giraffe\",\n",
    "    \"elephant\",\n",
    "    \"cheetah\",\n",
    "    \"leopard\",\n",
    "    \"impala\",\n",
    "    \"gazelle_grants\",\n",
    "    \"gazelle_thomsons\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14ee48",
   "metadata": {},
   "source": [
    "## Ecological modelling\n",
    "\n",
    "We can now investigate how environmental variables (including human pressure) influence species presence.\n",
    "\n",
    "To do this, we will build a Generalized Linear Model (GLM), a technique you encountered in BIOS0002.\n",
    "Since our response variable is binary (present/absent), we will use a Binomial family with a Logit link function.\n",
    "\n",
    "*Note*: A note on tools (Python vs. R).\n",
    "To be honest, traditional statistical modelling is often smoother in R, which has a massive ecosystem for this specific work.\n",
    "Python is catching up, and there is a growing trend of using deep learning frameworks for statistics, but standard statistical libraries here are still a bit less developed.\n",
    "Ideally, you might switch to R for this step, but to keep our workflow simple today, we will stick to Python.\n",
    "\n",
    "Let's focus on a single species and write a formula that relates its presence to all our covariates.\n",
    "We start by simply listing the predictors we think are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_0 = \"presence ~ savi_mean + water_distance + human_distance + shoat_rate + cattle_rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c2467",
   "metadata": {},
   "source": [
    "As a general rule of thumb, it is good practice to scale your predictors.\n",
    "This transforms all values so they have a mean of 0 and a standard deviation of 1.\n",
    "It makes the model coefficients comparable.\n",
    "You can immediately see which variable has the strongest effect just by looking at the size of the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_1 = (\n",
    "    \"presence ~ scale(savi_mean) + scale(water_distance) + scale(human_distance) + scale(shoat_rate) + scale(cattle_rate)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd46866",
   "metadata": {},
   "source": [
    "Finally, consider the distribution of the data, especially the rate variables (livestock abundance).\n",
    "For these, a logarithmic scale makes more sense ecologically.\n",
    "The impact of livestock is often non-linear.\n",
    "An increase from 10 to 20 cows/day is a massive doubling of pressure.\n",
    "However, an increase from 100 to 110 cows/day is barely noticeable to wildlife."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = (\n",
    "    \"presence ~ scale(savi_mean) + scale(water_distance) + scale(human_distance) + scale(np.log1p(shoat_rate)) + scale(np.log1p(cattle_rate))\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdc426",
   "metadata": {},
   "source": [
    "*Note:* We use `np.log1p` (which calculates $log(1+x)$) to handle cases where the rate is zero, since $log(0)$ is undefined.\n",
    "\n",
    "Let's test our formula on the Giraffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11852074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the giraffe data\n",
    "# We select the 'giraffe' column, rename it to generic 'presence', \n",
    "# and ensure it is an integer (0 or 1)\n",
    "giraffe_presence = presence_matrix[\"giraffe\"].rename(\"presence\").astype(int)\n",
    "\n",
    "# Merge with environmental covariates\n",
    "# We match rows based on the 'site_id'\n",
    "joined_giraffe_data = (\n",
    "    pd.merge(\n",
    "        left=covariates,\n",
    "        right=giraffe_presence,\n",
    "        left_on=\"site_id\",\n",
    "        right_on=\"site_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the model structure\n",
    "# We use the formula we defined above and specify a Binomial family (for binary data)\n",
    "giraffe_model = smf.glm(\n",
    "    formula,\n",
    "    data=joined_giraffe_data,\n",
    "    family=sm.families.Binomial(),\n",
    ")\n",
    "\n",
    "# Fit the model to the data\n",
    "giraffe_results = giraffe_model.fit()\n",
    "\n",
    "# View the results\n",
    "giraffe_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c0398",
   "metadata": {},
   "source": [
    "Take a close look at the summary table.\n",
    "Here are the key takeaways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bbbf3",
   "metadata": {},
   "source": [
    "1. Model fit is good.\n",
    "   The Deviance is lower than the Degrees of Freedom (Residual).\n",
    "   This is generally a good sign‚Äîit suggests our model fits the data reasonably well without major overdispersion.\n",
    "2. Relatively low explanatory power.\n",
    "   The Pseudo R-squared is roughly 0.16.\n",
    "   Is this actually not bad in ecology.\n",
    "   Animals are complex and unpredictable.\n",
    "   Explaining 16% of a wild animal's movement with just a few variables is actually a solid result.\n",
    "   It tells us there is still a lot of \"noise\" (unexplained behavior), but we are definitely capturing a signal.\n",
    "3. Look at the cattle_rate.\n",
    "   The p-value is very low (statistically significant).\n",
    "   The coefficient is positive.\n",
    "   Interpretation: This suggests giraffes are more likely to be found in areas with higher cattle density.\n",
    "   This is counter-intuitive!\n",
    "   Do they actually like cows, or do they just both prefer the same type of habitat (open grassy plains)?\n",
    "4. Most other covariates have high p-values (>0.05).\n",
    "   This implies that, in this specific model, factors like distance to water or fences don't strongly relate to whether a giraffe will be present or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06761155",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Other species*\n",
    "\n",
    "Now it's your turn.\n",
    "Use the helper function below to investigate the behavior of the following species:\n",
    "\n",
    "- Elephant\n",
    "- Ostrich\n",
    "- Buffalo\n",
    "- Grants gazelle\n",
    "- Lion\n",
    "\n",
    "For each species, examine the model summary and report:\n",
    "\n",
    "- Which variables have a significant effect (p-value < 0.05)?\n",
    "- What is the direction of the effect (positive or negative coefficient)?\n",
    "- What does this tell you about where the animal likes to be?\n",
    "\n",
    "For each species give a summary of which are the significant effects, what is their effect size, and a brief interpretation of the result.\n",
    "\n",
    "Note that the female and male lions were labeled separtely, so you need to provide a list to the function like so `fit_presence_binomial_glm([\"lion_male\", \"lion_female\"])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97344488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def fit_presence_binomial_glm(species, formula=formula):\n",
    "    # Prepare the presence/absence data\n",
    "\n",
    "    if isinstance(species, str):\n",
    "        # Case A: Single species (e.g., \"elephant\")\n",
    "        # Extract the column and ensure it is an integer (0/1)\n",
    "        presence = presence_matrix[species].rename(\"presence\").astype(int)\n",
    "\n",
    "    elif isinstance(species, list):\n",
    "        # Case B: List of species (e.g., [\"lion_male\", \"lion_female\"])\n",
    "        # We consider the group \"present\" if any of the species in the list is found at the site.\n",
    "        # We use axis=1 to check across columns (species) for each row (site).\n",
    "        presence = presence_matrix[species].any(axis=1).rename(\"presence\").astype(int)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected species format: {type(species)}\")\n",
    "\n",
    "    # Merge with site covariates\n",
    "    joined = (\n",
    "        pd.merge(\n",
    "            left=covariates,\n",
    "            right=presence,\n",
    "            left_on=\"site_id\",\n",
    "            right_on=\"site_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Fit the GLM (Binomial family for binary data)\n",
    "    model = smf.glm(\n",
    "        formula,\n",
    "        data=joined,\n",
    "        family=sm.families.Binomial(),\n",
    "    )\n",
    "\n",
    "    # And fit it to the data\n",
    "    results = model.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example: How to run it for lions\n",
    "# lion_results = fit_presence_binomial_glm([\"lion_male\", \"lion_female\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d022b7",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "While our initial model gives us some insights, it has several major limitations common in basic ecological analysis:\n",
    "\n",
    "1. By reducing data to binary \"presence/absence,\" we treat a site with 1 sighting the same as a site with 100 sightings.\n",
    "   We lose the information contained in the repeated visits (days the camera was active).\n",
    "   Advanced methods like **Occupancy Modelling** use this repetition to build a more accurate picture.\n",
    "2. We only included a few human pressure variables.\n",
    "   Critical natural factors like Canopy Cover (shade/protection) and other human infrastructure details were ignored, likely biasing our results.\n",
    "3. Our model assumes that if a camera didn't see an animal, the animal wasn't there.\n",
    "   In reality, animals are elusive.\n",
    "   We failed to account for detection probability‚Äîan animal might be present but hidden by bush or avoiding the camera.\n",
    "4. Why not Counts?\n",
    "   Using raw counts is problematic because, a single animal walking back and forth can generate 50 \"counts,\" misleading the model.\n",
    "   Also, cameras were active for different lengths of time.\n",
    "   A camera active for 30 days will naturally have higher counts than one active for 3 days, regardless of actual animal abundance.\n",
    "   We would need to normalise for this \"effort.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca2f6a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully, this notebook has given you an overview to how AI can be used for large-scale ecological research.\n",
    "Tools like MegaDetector and SpeciesNet can be incredibly helpful, and new models are constantly being developed.\n",
    "However, the most important takeaway is that AI is just one part of the process.\n",
    "Understanding your data, the ecological context, and critically validating the model's outputs are fundamental steps.\n",
    "AI models are powerful tools, but only when used carefully and responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c54701",
   "metadata": {},
   "source": [
    "## Final thoughts & discussion\n",
    "\n",
    "Consider the following questions:\n",
    "\n",
    "1. What are the limits of this workflow?\n",
    "   When would the MegaDetector + SpeciesNet pipeline be a great solution, and for what kinds of research questions would it be unsuitable?\n",
    "2. How could this apply to your research?\n",
    "   Could a similar AI workflow be useful in your specific field of study?\n",
    "   If you don't use camera traps, are you aware of similar AI tools for your type of data (e.g., for audio, satellite imagery, or genetic data)?\n",
    "3. What are the potential downsides?\n",
    "   Beyond just getting incorrect predictions, what are some of the other risks or drawbacks of relying on AI for ecological research?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
