{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaca7044",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/blob/main/Text/practical_solutions.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b53e1",
   "metadata": {},
   "source": [
    "# Week 4 AI & Text\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "This week, we will:\n",
    "\n",
    "- Learn how to interact with Large Language Models (LLMs) via code.\n",
    "- Explore different prompting techniques and identify common prompting pitfalls.\n",
    "- Apply LLMs to the extraction of metadata from scientific paper abstracts.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43894444",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac1c7e",
   "metadata": {},
   "source": [
    "## Setup script\n",
    "\n",
    "Before anything please run the following cell to install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL \"https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/raw/refs/heads/main/Text/setup.sh\" | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3ff2a",
   "metadata": {},
   "source": [
    "This may take some time to finish so continue reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a5c95",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Running a **Large Language Model (LLM)** is not easy, and usually, it's hidden from us behind web chat interfaces like ChatGPT or Gemini.\n",
    "However, we can run the models ourselves, and this is what we will be doing in this notebook using Ollama.\n",
    "\n",
    "[**Ollama**](https://ollama.com/) is an open-source tool that allows us to run Large Language Models (LLMs) on our own computer (or in this case, on the Google Colab server) rather than sending our data to a company like OpenAI or Google via the internet.\n",
    "In research, this is very useful because it gives us more control over the models and data privacy, and allows us to use \"open-source\" models for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5325a24",
   "metadata": {},
   "source": [
    "## Choosing an LLM model\n",
    "\n",
    "On Ollama, and other platforms like Hugging Face, you will find hundreds if not thousands of models to choose from.\n",
    "Have a look at [Ollama‚Äôs¬†model¬†library](https://ollama.com/search).\n",
    "This can be overwhelming!\n",
    "However, there are a few considerations to keep in mind when choosing a model:\n",
    "\n",
    "- **Model Size**: This refers to the number of parameters the model has.\n",
    "  It is directly related to how big the model file is, how much memory (RAM) it needs, and how long it takes to generate a response.\n",
    "  A larger model is likely to generate better responses, but you may not have the hardware to run it.\n",
    "  In our environment, we have limited resources, so we will favour smaller, efficient models.\n",
    "\n",
    "- **Intent and Specialisation**: Some models are \"generic,\" while others are trained for specific purposes like coding, mathematical reasoning, or safety reviews.\n",
    "  Choose one that aligns with your specific research goal.\n",
    "\n",
    "- **Context Size**: Depending on the task, you may need a model with a large context window to process large chunks of text (like a full scientific paper) at once.\n",
    "  This is especially important for multi-step interactions where a long conversation needs to fit into the model's context.\n",
    "\n",
    "- **Model Capabilities**: As we will see below, some models have \"thinking\" abilities, others can use \"tools\" (like calculators), and others can analyse images.\n",
    "\n",
    "- **Recency**: More recent models are likely to perform better due to novel research and innovation in training techniques.\n",
    "\n",
    "üí° Safety Note: Downloading unknown models can be risky, but any model from well-known AI labs or those that are highly popular on these platforms are safe to test.\n",
    "\n",
    "Today we will use two models:\n",
    "\n",
    "- **Gemma 3 (by Google)** is an open-source model.\n",
    "  We are using a lightweight version today that features a 128k token context window (approx. 100,000 words), allowing it to process large chunks of text at once.\n",
    "  While it is multimodal and can analyse images, we will focus on its text-processing abilities.\n",
    "\n",
    "- **Qwen 3 (by Alibaba)** is a specialised \"thinking model.\" Unlike standard LLMs that answer instantly, a thinking model generates a hidden chain of reasoning first.\n",
    "  This helps it handle complex logic and multi-step problems without making mistakes.\n",
    "  It is also designed for tool use, meaning it can work with external functions to increase its accuracy for specific tasks.\n",
    "\n",
    "*Important note:* Because these are \"smaller\" models, they are more limited than the commercial versions, which are usually massive.\n",
    "However, all models share the main limitations, such as hallucinations (making up facts).\n",
    "The prompting tips we cover today are relevant to all models, large or small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede803c0",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "Here we will use the ollama Python library to interact with the models.\n",
    "The first step is to create a \"client\" that connects to the ollama program running in the background.\n",
    "\n",
    "Since we have ollama installed on this specific Colab machine, we use localhost (which means \"this computer\").\n",
    "However, this same code could be used to connect to a remote server or a high-performance computer elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Create the client to talk to the local Ollama service\n",
    "client = ollama.Client(host=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20fbc6",
   "metadata": {},
   "source": [
    "The easiest way to prompt an LLM using Ollama is with the generate method.\n",
    "This is a \"one-shot\" interaction: you send a prompt, and the model sends back a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple response using the Gemma model\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What is ecology?\")\n",
    "\n",
    "# The result is a complex object with metadata, but we can print just the text\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541404b",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Ask anything*\n",
    "\n",
    "Ask `gemma3` anything you want.\n",
    "Try asking it to define a specific ecological term or to write a short story about a forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8123a7",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ There's no right answer to this question.\n",
    "Let's ask it to write a story about writen from the perspective of a tropical rainforest on a scorching day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c266ae5",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "result = client.generate(\n",
    "    model=\"gemma3\",\n",
    "    prompt=\"Write a story from the prespective of a tropical rainforest waking up in a scorching day.\",\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e1afc",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Compare models*\n",
    "\n",
    "Run the same prompt (\"What is ecology?\") on `qwen3`.\n",
    "Are the results different?\n",
    "How is the tone or structure different?\n",
    "Is there one that seems more \"useful\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae41b7",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The code block below prompts `qwen3` with the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3868465",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "result = client.generate(model=\"qwen3\", prompt=\"What is ecology?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f1f78",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Gemma uses a friendly and energetic tone, often including additional motivational sentences.\n",
    "In contrast, Qwen3 provides a shorter, more direct definition that is straight to the point and avoids conversational filler.\n",
    "\n",
    "Both models share a similar structure, making heavy use of Markdown headers and bullet points with bold headings.\n",
    "However, Qwen3 tends to use a more structured list format compared to Gemma's more expansive layout.\n",
    "\n",
    "In terms of helpfulness, Gemma is useful as it provides extra resources, though its verbosity can make it harder to quickly find specific information.\n",
    "Qwen3 is more useful for a quick reference due to its conciseness, even though it did not provide further resources.\n",
    "\n",
    "*Note*: These models are stochastic, and the outputs may vary across different runs of the same prompt.\n",
    "There is no single \"right\" answer; the intent of this exercise is to identify the discriminating features of LLM responses and observe how these characteristics change between different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046189e",
   "metadata": {},
   "source": [
    "# Usual pitfalls\n",
    "\n",
    "In this section, we will explore different ways LLMs can generate unhelpful or even harmful outputs, and look at simple strategies to mitigate these risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe9dee",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "The most significant challenge when using LLMs is their tendency to generate **hallucinations**: outputs that are perfectly coherent but factually false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(model=\"gemma3\", prompt=\"What happened to the Parisian tiger?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7021a1c",
   "metadata": {},
   "source": [
    "If you use model outputs uncritically, it is very likely you will include something false that looks correct.\n",
    "In academic research, text is scrutinised heavily by peers and supervisors.\n",
    "Scientists are specifically trained to spot inconsistencies, so relying on unverified AI output can be a big risk.\n",
    "**Always** double-check any output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f79a09",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Reference checking*\n",
    "\n",
    "A common and dangerous type of hallucination is the creation of fake scientific references.\n",
    "Run the following prompt and using [Google¬†Scholar](https://scholar.google.com/) verify if any of the papers found exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(\n",
    "    model=\"gemma3\",\n",
    "    prompt=\"Give me a list of recent impactful papers that explore the relation between bats migration and climate change.\",\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c34067",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Gemma3 provided six references divided by topic.\n",
    "Upon checking Google Scholar, none of the suggested papers actually exist.\n",
    "Since the journals mentioned are well-known, it is highly unlikely they would be missing from a major database, confirming the references are hallucinations.\n",
    "\n",
    "While the titles and descriptions sounded plausible and interesting, they are ultimately made up. The model did, however, helpfully suggest that the user verify the results using Google Scholar or other databases.\n",
    "Notably, the model claimed today is November 2, 2023, which is false and likely reflects its training data cutoff.\n",
    "\n",
    "Here is the answer provided by Gemma3 for reference:\n",
    "\n",
    "```\n",
    "Okay, here's a list of recent impactful papers exploring the relationship between bat migration and climate change, categorized for clarity and with brief summaries. Note that \"impactful\" is subjective and based on citation counts, journal prestige, and overall influence within the field. As of today, November 2, 2023, these are some of the most discussed and influential:\n",
    "\n",
    "**1. Long-Term Trends & Broad Scale Impacts:**\n",
    "\n",
    "* **Title:** \"Climate change alters the spatial range of migratory bats\"\n",
    "   * **Authors:**  Ryan McAllister, et al.\n",
    "   * **Journal:** *Nature*, 2021\n",
    "   * **Impact:** This is arguably *the* landmark paper on this topic. It used a combination of bat tracking data (radar, radio tags), climate modeling, and species distributions to demonstrate a significant northward shift in the ranges of multiple migratory bat species across North America. It strongly links range shifts directly to warming temperatures and altered wind patterns. It‚Äôs been hugely influential. \n",
    "   * **Key Findings:** Demonstrated predictable northward shifts in bat ranges, linked to changes in wind regimes, which bats use for efficient flight. Highlighted the vulnerability of bats at the southern edges of their ranges.\n",
    "\n",
    "* **Title:** ‚ÄúClimate change influences migration routes and stopover sites of migratory bats‚Äù\n",
    "   * **Authors:** David A. Smith, et al.\n",
    "   * **Journal:** *Global Ecology and Biogeography*, 2020\n",
    "   * **Impact:** Utilizes a large dataset of bat migration tracking to reveal how changing climate conditions are altering migration routes and the timing of stopover arrival.\n",
    "   * **Key Findings:** Highlighted shifts in timing and routes, especially linked to changes in temperature and rainfall, demonstrating how bats are actively adapting to new conditions.\n",
    "\n",
    "**2. Physiological & Behavioral Responses:**\n",
    "\n",
    "* **Title:** \"Climate-induced shifts in bat migration timing and energetic demands\"\n",
    "   * **Authors:**  Elizabeth H. Baker, et al.\n",
    "   * **Journal:** *Proceedings of the National Academy of Sciences*, 2020\n",
    "   * **Impact:**  A highly cited paper exploring the physiological consequences of climate-induced shifts in bat migration.\n",
    "   * **Key Findings:** Showed that bats responding to warmer temperatures were experiencing earlier migration onset and increased energetic demands (higher metabolic rates) during the journey.  This suggests bats are energetically ‚Äòrunning‚Äô to stay ahead of climate change.\n",
    "\n",
    "* **Title:** \"Bat migration under climate change: a meta-analysis of movement ecology data\"\n",
    "   * **Authors:** Michael A. O‚ÄôNeal, et al.\n",
    "   * **Journal:** *Biology Letters*, 2020\n",
    "   * **Impact:** A quantitative meta-analysis that pulls together data from numerous bat tracking studies to provide a more robust understanding of migration patterns and how they‚Äôre changing.\n",
    "   * **Key Findings:** Offered insights into the key drivers of bat migration, including temperature, wind, and landscape features, quantifying the strength of these influences.\n",
    "\n",
    "**3. Specific Species & Regional Studies:**\n",
    "\n",
    "* **Title:** \"Changes in the migration phenology of Mexican free-tailed bats under a warming climate\"\n",
    "   * **Authors:**  Joshua J. Evans, et al.\n",
    "   * **Journal:** *Ecology*, 2019\n",
    "   * **Impact:** Focused on a well-studied bat species (Mexican free-tailed bats) and demonstrated the impacts of climate change on migration phenology (timing) with robust data.\n",
    "   * **Key Findings:**  Demonstrated earlier arrival at and departure from breeding grounds in response to warmer temperatures, with potential implications for reproductive success.\n",
    "\n",
    "* **Title:** ‚ÄúClimate change is reshaping the migratory landscape of horseshoe bats‚Äù\n",
    "   * **Authors:** Matthew R. Davis, et al.\n",
    "   * **Journal:** *Science*, 2022\n",
    "   * **Impact:** This study examined the effects of climate change on horseshoe bats (a group facing significant declines), revealing shifts in migration timing and range, and highlighting the importance of understanding how changes in climate are impacting these vulnerable populations.\n",
    "   * **Key Findings:** Showed that changing wind patterns and temperature are affecting migration routes and the availability of suitable habitat for horseshoe bats, potentially exacerbating existing population declines.\n",
    "\n",
    "**4.  Modeling and Predictive Research:**\n",
    "\n",
    "* **Title:** \"Climate-driven shifts in bat migration routes: A multi-species model\"\n",
    "   * **Authors:**  Sarah J. Roulier, et al.\n",
    "   * **Journal:** *Ecography*, 2021\n",
    "   * **Impact:** Utilizes modeling to predict future bat migration shifts under different climate change scenarios.\n",
    "   * **Key Findings:** Provided projections of range shifts and altered migration routes, offering valuable information for conservation planning.\n",
    "\n",
    "**Resources for Finding More Papers:**\n",
    "\n",
    "* **Google Scholar:** [https://scholar.google.com/](https://scholar.google.com/) -  Use keywords like \"bat migration climate change,\" \"bat tracking climate,\" \"bat range shift climate.\"\n",
    "* **Web of Science:** [https://www.webofscience.com/](https://www.webofscience.com/) - A subscription-based database, but often accessible through university libraries.\n",
    "* **ResearchGate:** [https://www.researchgate.net/](https://www.researchgate.net/) ‚Äì Researchers often share their pre-prints and publications on this platform.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "* **Ongoing Research:** This field is rapidly evolving, with new studies published frequently.\n",
    "* **Data Limitations:** Bat tracking data has limitations (e.g., tag weight, battery life).\n",
    "* **Complex Interactions:** The relationship between bats and climate change is complex and influenced by many factors, including habitat loss, disease, and human activity.\n",
    "\n",
    "To help me refine this list further and provide even more relevant papers, could you tell me:\n",
    "\n",
    "*   Are you interested in a specific bat species or region?\n",
    "*   What aspects of the migration-climate change relationship are you most interested in (e.g., timing, range shifts, physiology, conservation)?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24c64c",
   "metadata": {},
   "source": [
    "This happens because LLMs operate on statistical probability.\n",
    "During training, the model learns the structural patterns of language; since scientific citations follow a highly predictable format, the model can \"simulate\" a reference by mixing common researcher names with relevant keywords.\n",
    "Because its internal knowledge is static (frozen at the moment its training ended) it isn't searching a real library.\n",
    "Instead, it is simply predicting what a plausible citation looks like based on the patterns it has seen.\n",
    "\n",
    "Nowadays, commercial models try to fix this by using web search to ground their answers in real-time data.\n",
    "While this helps significantly, it doesn't solve the problem entirely.\n",
    "Models can still misinterpret the search results or hallucinate details even when they have the correct source in front of them.\n",
    "**Always double check the outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1058802",
   "metadata": {},
   "source": [
    "## Stochasticity\n",
    "\n",
    "LLMs generate text by \"sampling\" one word at a time.\n",
    "Instead of simply predicting a single next token, the model assigns a probability score to every possible token in its vocabulary, giving higher scores to those that are most likely to follow.\n",
    "It then samples a word at random based on these scores.\n",
    "This means the model's output can change from one run to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb585e2e",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Word predictability*\n",
    "\n",
    "Take the following sentence from Daan's paper:\n",
    "\n",
    "> The body of ecological literature, which informs much of our knowledge of the global loss of biodiversity, has been experiencing rapid growth in recent decades.\n",
    "\n",
    "Which of these words are most predictable based on the preceding text?\n",
    "Use the function below to test this on different segments, such as \"The body of...\" or \"The body of ecological...\".\n",
    "Prompt the model several times with the same segment to see if, and how often, the response changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence_chunk):\n",
    "    result = client.generate(\n",
    "        model=\"gemma3\",\n",
    "        prompt=f\"Complete the given sentence. One word only. {sentence_chunk}:\",\n",
    "    )\n",
    "\n",
    "    return result.response\n",
    "\n",
    "\n",
    "# How predictable is ecological?\n",
    "print(predict_next_word(\"The body of\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6350e",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ By running the predict_next_word function, I found that \"biodiversity\" and \"growth\" were the most predictable words in the sentence.\n",
    "When prompted with the preceding segments, Gemma3 consistently predicted these specific words every time, and all other words were never predicted.\n",
    "\n",
    "*Note:* I identified these words by manually testing different sentence segments.\n",
    "However, the process can be automated using the script below, which iterates through the sentence and measures the predictability of each word by running multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5fcf9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# The original sentence to analyze\n",
    "sentence = \"The body of ecological literature, which informs much of our knowledge of the global loss of biodiversity, has been experiencing rapid growth in recent decades.\"\n",
    "\n",
    "# Split the sentence into individual words\n",
    "words = sentence.split()\n",
    "\n",
    "\n",
    "# This function cleans both the predicted and expected word to avoid\n",
    "# counting small variants as different\n",
    "def normalise_word(word):\n",
    "    \"\"\"Lowercase the word and remove trailing punctuation for fair comparison.\"\"\"\n",
    "    return word.lower().strip(\",\").strip(\".\")\n",
    "\n",
    "\n",
    "def measure_predictability(word_index, num_predictions=10):\n",
    "    # Get the target word by its index\n",
    "    target_word = words[word_index]\n",
    "\n",
    "    # Take all the words up to the one we are interested in\n",
    "    preceding_words = words[:word_index]\n",
    "\n",
    "    # Join them into a sentence fragment to use as a prompt\n",
    "    preceding_text = \" \".join(preceding_words)\n",
    "\n",
    "    # Counter for how many times the target word is correctly predicted\n",
    "    num_matches = 0\n",
    "\n",
    "    # List to store all predictions for this segment\n",
    "    predicted_words = []\n",
    "\n",
    "    # Repeat the trial several times to account for model variation\n",
    "    for _ in range(num_predictions):\n",
    "        # Generate the next-word prediction\n",
    "        predicted_word = predict_next_word(preceding_text)\n",
    "\n",
    "        # Normalise the word to ignore case and punctuation\n",
    "        predicted_word = normalise_word(predicted_word)\n",
    "        predicted_words.append(predicted_word)\n",
    "\n",
    "        # Check if the prediction matches the target word\n",
    "        if predicted_word == normalise_word(target_word):\n",
    "            num_matches += 1\n",
    "\n",
    "    # Calculate predictability as the ratio of successful matches\n",
    "    predictability = num_matches / num_predictions\n",
    "\n",
    "    # Return a dictionary containing the results and context\n",
    "    return {\n",
    "        \"word\": target_word,\n",
    "        \"score\": predictability,\n",
    "        \"preds\": predicted_words,\n",
    "        \"preceeding\": preceding_text,\n",
    "    }\n",
    "\n",
    "\n",
    "# Print the predictability of each word (starting from the fourth word)\n",
    "for index in range(3, len(words)):\n",
    "    results = measure_predictability(index)\n",
    "    print(f\"{results['word']} : {results['score']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c0aea",
   "metadata": {},
   "source": [
    "This randomness can be a significant issue for the reproducibility of science.\n",
    "We can address this stochasticity by lowering the **\"temperature\"** of the model, which concentrates the probability on the most likely word, or by setting a **\"seed\"** to ensure the same random choices are made every time.\n",
    "While we will explore this later, note that these settings are often harder to control when using commercial models through their standard web interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b309a2a",
   "metadata": {},
   "source": [
    "## Verbosity\n",
    "\n",
    "Model outputs tend to be very long and verbose.\n",
    "You can see this in the previous examples, but let's test another scenario: imagine you encountered the term \"LLM\" for the first time and wanted a quick definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A broad question often leads to a very long, conversational answer\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What is an LLM?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42b4f60",
   "metadata": {},
   "source": [
    "Verbosity increases the likelihood of the output containing hallucinations or generally unhelpful content.\n",
    "It also makes it harder to find the specific information you need for your research.\n",
    "\n",
    "You can address this in two ways: specify a length limit or be more precise about what you are asking.\n",
    "For example, if you only need a brief summary, tell the model exactly how much to write:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a clear constraint helps\n",
    "result = client.generate(\n",
    "    model=\"gemma3\", prompt=\"What is an LLM? Reply in less than 100 words\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9215f",
   "metadata": {},
   "source": [
    "Often, a small change in the question results in a much more useful and direct answer.\n",
    "Instead of asking a broad \"what is\" question, try to pinpoint the exact information you require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0daa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more specific question leads to a more targeted response\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What does the acronym LLM stand for?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b7370",
   "metadata": {},
   "source": [
    "## Logical errors\n",
    "\n",
    "Models do not reason in the way we understand it‚Äîby constructing solid, valid logical arguments step by step.\n",
    "While they can emulate reasoning, they often fail even in simple scenarios that require basic spatial or logical awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfedf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(\n",
    "    model=\"gemma3\",\n",
    "    prompt=\"I get out on the top floor (third floor) at street level. How many stories is the building above the ground?\",\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc1b37",
   "metadata": {},
   "source": [
    "One way researchers address this is by using models specifically trained to \"reason\" or \"think\".\n",
    "During the fine-tuning process (often using Reinforcement Learning), these models are rewarded for creating accurate reasoning chains, which significantly improves their performance on logical tasks.\n",
    "Another method is \"Chain-of-Thought\" (CoT) prompting, where you explicitly ask the model to explain its thinking process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e19d89",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Reasoning models*\n",
    "\n",
    "`qwen3` is a reasoning model, it generates a long chain of text where it lays out its internal logic.\n",
    "Try the same \"third floor\" prompt with `qwen3`.\n",
    "Does it get it right?\n",
    "Why or why not?\n",
    "Give your interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0dc2f",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea21d8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "result = client.generate(\n",
    "    model=\"qwen3\",\n",
    "    prompt=\"I get out on the top floor (third floor) at street level. How many stories is the building above the ground?\",\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ceb85",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The correct answer is that the building has zero stories above ground (or is just one floor), since the top floor is already at street level.\n",
    "Qwen3 was unable to solve this correctly.\n",
    "\n",
    "During its \"thinking\", the model correctly identified two separate facts: that the third floor is the top floor, and that this floor is at street level.\n",
    "However, it failed to combine these two inferences into a single logical conclusion.\n",
    "\n",
    "While the \"thinking\" process may have helped the model break the problem into steps, and each piece of information was handled correctly, the model could not logically combine them to find the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8350c",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Chain of thought prompting*\n",
    "\n",
    "Now, return to `gemma3`.\n",
    "Try asking the same question again, but this time, specifically instruct it to \"provide a logical explanation step by step before giving the final answer\".\n",
    "Does the quality of the output improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c599d5",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Check solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee00907",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "result = client.generate(\n",
    "    model=\"gemma3\",\n",
    "    prompt=\"I get out on the top floor (third floor) at street level. How many stories is the building above the ground? Provide a logical explanation step by step before giving the final answer.\",\n",
    ")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece63052",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ While Gemma3 followed the instructions to break down its reasoning step-by-step, it still arrived at an incorrect solution.\n",
    "The very first step of its explanation was logically invalid.\n",
    "\n",
    "*Note*: This shows that while \"Chain of Thought\" prompting can encourage a model to be more methodical, it is does not prevent hallucinations.\n",
    "If a model generates an incorrect premise or \"hallucinates\" a logical step early on, the entire reasoning chain will lead to a wrong conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c6b0f",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Since model answers are based on the most statistically likely word, the output is heavily influenced by the training data.\n",
    "If certain words, concepts, or taxa appear together frequently in the text the model was trained on, it will naturally recreate those associations.\n",
    "This can bias the outputs towards common or well-documented examples, often at the expense of less-studied or minority cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bfbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe which taxa the model prioritises based on common literature trends\n",
    "prompt = (\n",
    "    \"Complete the sentence by filling in the blank with a single taxa, only provide the answer: \"\n",
    "    \"Recent years have seen increasing pressures from multiple fronts including \"\n",
    "    \"increased tourism and encroaching deforestation, in particular populations \"\n",
    "    \"of <blank> have been declining.\"\n",
    ")\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1651d6c",
   "metadata": {},
   "source": [
    "Because of this, LLM outputs should rarely be used to answer questions that require a careful consideration of multiple viewpoints or diverse sources.\n",
    "While the model can be a helpful search tool, you must be aware of its biases and always contrast its output with other sources of information.\n",
    "Ultimately, answers to complex ecological questions should incorporate broad evidence and use rigorous statistical approaches to control for known biases in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174af1ee",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Spatial bias*\n",
    "\n",
    "Compare the model's ability to name common bird species in two different ecological contexts.\n",
    "Use the function below to list three common species in the UK, then try it again for a location you suspect is less represented in global ecological literature.\n",
    "Verify if the species provided are actually found in those locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf39ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_bird_species(location, n=3):\n",
    "    result = client.generate(\n",
    "        model=\"gemma3\",\n",
    "        prompt=f\"List {n} common bird species in the {location}. Provide the list only, if possible using both common and scientific names.\",\n",
    "    )\n",
    "    return result.response\n",
    "\n",
    "\n",
    "print(get_common_bird_species(\"UK\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0eae5",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345b589",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(get_common_bird_species(\"Mexico\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c43fe5",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ While all UK species were correct in terms of commonality and scientific names, the response for Mexico contained several errors.\n",
    "\n",
    "```\n",
    "Here are 3 common bird species in Mexico:\n",
    "\n",
    "*   Mexican Warbler (Setophaga phoenicea)\n",
    "*   Blue Jay (Cyanocitta cristata)\n",
    "*   House Finch (Haemorhous mexicanus)\n",
    "```\n",
    "\n",
    "The Blue Jay is not present in Mexico.\n",
    "The Mexican Warbler is not a specific species name; there are multiple types of warblers in Mexico, and the scientific name provided (Setophaga phoenicea) is hallucinated.\n",
    "Only House Finch was correct, as it is common in Mexico and its scientific name is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49001d4",
   "metadata": {},
   "source": [
    "## Writing style\n",
    "\n",
    "Due to the inherent bias in LLMs, it is generally a **bad idea** to ask them to write a research piece from scratch.\n",
    "Doing so often generates generic content that may not align with your specific intent or the nuances of your study.\n",
    "It is much safer to provide the model with a skeleton or a draft of your ideas and use the tool to help you polish the language or structure.\n",
    "While the outputs may look like high-quality academic writing, academic evaluation focuses on the strength of your ideas, narrative, and argumentation, areas where LLMs are not inherently strong.\n",
    "Ultimately, a model will either produce generic information or build upon what you provide.\n",
    "Therefore, it remains **your responsibility to provide the solid reasoning and verified facts** that form the core of the work.\n",
    "\n",
    "When using LLMs to refine or generate text, it is important to account for their default writing style.\n",
    "They are trained to be \"helpful\" and \"polite,\" which often results in a particular tone that may be too wordy or formal for your needs.\n",
    "You can steer the model by being specific about the desired tone and audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310411c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt often results in the model's 'default' academic style\n",
    "prompt = \"\"\"Refine my text:\n",
    "\n",
    "LLMs are being increasingly applied in ecology.\n",
    "They can be used to extract information from papers.\n",
    "This makes large automated sythesis collection feasible.\n",
    "It is very recent but already promising.\n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1596f42",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Guide the style*\n",
    "\n",
    "Assume you want to write a blog post for first-year undergraduate students about LLMs in ecology.\n",
    "Use the previous text as a starting point, but modify the prompt to steer the style so it communicates the ideas effectively for that specific audience.\n",
    "Write your improved prompt below and compare the output to the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc853446",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See prompt below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6656357",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I am writing a blog post for first-year undergraduate students about using Large Language Models (LLMs) in ecology.\n",
    "Please refine the text below for this audience using these guidelines:\n",
    "Assume the reader is a first-year student who may not be familiar with technical AI details or what ecological synthesis is.\n",
    "Use a relaxed and approachable tone.\n",
    "Keep the language simple and precise, and avoid over-enthusiastic wording.\n",
    "Avoid technical breakdowns and favour high-level, intutive explanations.\n",
    "Follow the original narrative structure sentence by sentence.\n",
    "You may expand the text slightly to clarify or explain concepts where needed.\n",
    "Provide only the refined text.\n",
    "\n",
    "Text to refine:\n",
    "\n",
    "LLMs are being increasingly applied in ecology.\n",
    "They can be used to extract information from papers.\n",
    "This makes large automated sythesis collection feasible.\n",
    "It is very recent but already promising.\n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt, options={\"seed\": 42})\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e73c74",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The difference in style is clear.\n",
    "The model adopted a more conversational tone and provided additional context to explain why each statement is important.\n",
    "\n",
    "*Note*: Explicitly defining the target audience helps the model move from its default style to a more helpful and tailored explanation.\n",
    "While this specific prompt is longer than the text itself and takes time to craft, you can reuse it for other sections of the same blog post or for future projects.\n",
    "We recommend building a personal prompt library to save time and promote consistency across your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c18f6a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In general, follow these basic recommendations when working with LLMs:\n",
    "\n",
    "- **Always check the output**: Never assume the model is factually correct.\n",
    "- **Be precise**: Give clear instructions on exactly how you want your output to look.\n",
    "- **Use restrictions**: Add constraints to guide the model, such as \"use a maximum of 100 words\" or \"format as a bulleted list.\"\n",
    "- **Challenge bias**: Models are inherently biased toward their training data; do not rely on them as your sole source of information.\n",
    "- **Account for randomness**: Output may vary from one run to the next.\n",
    "- **Provide context**: The more information and background you provide, the more focused and relevant the outputs will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c8586",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Critique the prompt*\n",
    "\n",
    "Have a look at the following prompt.\n",
    "Based on what we have discussed regarding verbosity, bias, and precision, how could you make it better?\n",
    "\n",
    "> \"Tell me about climate change and birds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5029c37",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The prompt points to a relevant topic, but it is too vague.\n",
    "\n",
    "In terms of precision, it only gives two broad concepts (\"climate change\" and \"birds\") and does not specify what information is needed.\n",
    "A better prompt should define the relationship of interest, the bird group, and the scope.\n",
    "For example, it could ask: \"How do rising peak temperatures affect seabird populations in the UK?\"\n",
    "\n",
    "In terms of bias, a broad prompt makes the model fill in missing details on its own.\n",
    "This can bias the answer toward patterns that are most common in the training data, including which climate effects and bird species are discussed.\n",
    "The model also has a training cutoff (around 2023), so newer evidence may be missing.\n",
    "Being more specific reduces this issue, but external sources may still be needed for recent or less well-covered information.\n",
    "\n",
    "The prompt also does not specify output style, structure, or length.\n",
    "As a result, the model may return a default response that is long and conversational.\n",
    "This can be improved by adding clear constraints, such as: \"Provide a concise executive summary of no more than 200 words.\"\n",
    "\n",
    "An improved prompt could be:\n",
    "\n",
    "> How do rising peak temperatures affect seabird populations in the UK? Provide a concise executive summary of no more than 200 words.\n",
    "\n",
    "*Note:* This type of factual prompt should be used carefully unless the model can access external, validated data sources.\n",
    "Without those tools, factual errors and hallucinations are more likely, so outputs should always be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d80e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "How do rising peak temperatures affect seabird populations in the UK?\n",
    "Provide a concise executive summary of no more than 200 words.\n",
    "Return the summary only, no filler conversation.\n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt, options={\"seed\": 42})\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ceb62",
   "metadata": {},
   "source": [
    "# General prompting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7892b",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "\n",
    "In **zero-shot prompting**, you provide the model with a direct instruction or question without any prior examples or demonstrations.\n",
    "This is the \"usual\" approach to prompting.\n",
    "You are relying on the model's pre-trained knowledge to understand and execute the instruction.\n",
    "\n",
    "While simple, zero-shot prompting is highly effective for general tasks like basic summarisation or sentiment analysis.\n",
    "However, as we saw previously, it is likely to result in hallucinations or formatting errors.\n",
    "In a zero-shot context, the model performs significantly better when your prompt is very specific and rich in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical zero-shot prompt: just an instruction\n",
    "prompt = \"Classify the following research description as 'Field Study' or 'Lab Experiment': We measured the growth rates of 50 oak seedlings in a temperature-controlled greenhouse.\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f024a",
   "metadata": {},
   "source": [
    "## Role prompting\n",
    "\n",
    "A **role prompt** provides the model with a specific persona.\n",
    "By instructing the model to \"Act as a senior research ecologist,\" you steer it away from the generic, conversational tone of a standard \"chatbot\".\n",
    "While this doesn't give the model new knowledge it hasn't been trained on, it can significantly improve the depth and relevance of the information it chooses to present.\n",
    "You are pre-conditioning the output to a particular style or scientific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how the tone and detail change when a role is assigned\n",
    "task = \"Explain why biodiversity loss matters.\"\n",
    "\n",
    "# Standard instruction\n",
    "prompt_simple = f\"Answer this: {task}\"\n",
    "\n",
    "# Role-based instruction\n",
    "prompt_role = f\"You are a Professor of Conservation Biology. Provide a technically rigorous answer to this: {task}\"\n",
    "\n",
    "print(\"--- Simple Prompt ---\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_simple).response)\n",
    "\n",
    "print(\"--- Role Prompt ---\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_role).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ee3f4",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Switching Personas*\n",
    "\n",
    "Try changing the role to something very different, like \"A science journalist writing for a local newspaper\" or \"A critical peer-reviewer\".\n",
    "Notice how the model adjusts its vocabulary and the types of evidence it emphasises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0280f7",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See answer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d93de",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "role_1 = \"A science journalist writing for a local newspaper\"\n",
    "prompt_1 = f\"You are {role_1}. {task}\"\n",
    "print(f\"---- {role_1} ----\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_1).response)\n",
    "\n",
    "role_2 = \"A critical peer-reviewer\"\n",
    "prompt_2 = f\"You are {role_2}. {task}\"\n",
    "print(f\"---- {role_2} ----\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_2).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a0a75",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Changing the role produced clear differences in both style and focus.\n",
    "The journalist focuses on communicating the importance of biodiversity loss matters for a broad audience, while the peer-reviewer adopts a critical tone and gives feedback on an imaginary text that the model assumed it was reviewing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dde746",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "\n",
    "LLMs are great at recognising patterns and we can leverage that.\n",
    "Few-shot prompting involves giving the model 2 or 3 examples of how you want a task done before asking it to perform the task itself.\n",
    "Simplified example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide examples of extracting a 'focal species' from a sentence\n",
    "prompt = \"\"\"\n",
    "Extract the focal species from the text.\n",
    "\n",
    "Text: We monitored the nesting habits of Chelonia mydas in Costa Rica.\n",
    "Species: Chelonia mydas\n",
    "\n",
    "Text: Camera traps were used to detect the presence of Panthera onca.\n",
    "Species: Panthera onca\n",
    "\n",
    "Text: Our study focus was the movement of migrating Passer domesticus across urban gradients.\n",
    "Species: \n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6aae43",
   "metadata": {},
   "source": [
    "## Chain-of-thought prompting\n",
    "\n",
    "For complex tasks, it a helpful strategy is to ask it to decompose the tasks into several steps and provide reasoning for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Identify if a study is 'Experimental' or 'Observational'\n",
    "\n",
    "abstract = \"\"\"\n",
    "Chinese tallowtree, Triadica sebifera (L.) Small (Euphorbiaceae), is one of the worst invasive weeds of the southeastern USA impacting coastal wetlands, forests, and natural areas.\n",
    "Traditional mechanical and chemical controls have been unable to limit the spread, and this invasive species continues to expand its range.\n",
    "A proposed biological control candidate, the flea beetle Bikasha collaris (Baly) (Coleoptera: Chrysomelidae), shows high specificity for the target weed Chinese tallowtree.\n",
    "Results from a series of no-choice and choice feeding tests of B.¬†collaris adults and larvae indicated that this flea beetle was highly specific to Chinese tallowtree.\n",
    "The larvae of B.¬†collaris feed by tunneling in the roots, whereas the adults feed on the leaves of Chinese tallowtree.\n",
    "A total of 77 plant taxa, primarily from members of the tallow plant family Euphorbiaceae, were tested in numerous test designs.\n",
    "Larval no-choice tests indicated that larvae completed development only on two of the non-target taxa.\n",
    "Of 80 B.¬†collaris larvae fed roots of Hippomane mancinella L. and 50 larvae fed roots of Ricinus communis L., two and three larvae completed development, respectively.\n",
    "The emerging adults of these five larvae died within 3¬†days without reproducing.\n",
    "Larval choice tests also indicated little use of these non-target taxa.\n",
    "Adult no-choice tests indicated little leaf damage by B.¬†collaris on the non-targets except for Ditrysinia fruticosa (Bartram) Govaerts & Frodin and Gymnanthes lucida Sw. When given a choice, however, B.¬†collaris adults consumed much less of the non-targets D.¬†fruticosa (7.4%) and G.¬†lucida (6.1%) compared with the control leaves.\n",
    "Finally, no-choice oviposition tests indicated that no eggs were produced when adults were fed all non-target taxa, except those fed G.¬†lucida.\n",
    "These B.¬†collaris adults fed G.¬†lucida leaves produced an average of 4.6 eggs compared with 115.0 eggs per female when fed Chinese tallowtree.\n",
    "The eggs produced from adults fed G.¬†lucida were either inviable or the emerging larvae died within 1¬†day.\n",
    "These results indicate that the flea beetle B.¬†collaris was unable to complete its life cycle on any of the non-target taxa tested.\n",
    "If approved for field release, B.¬†collaris will be the first biological control agent deployed against Chinese tallow tree in the USA.\n",
    "This flea beetle may play an important role in suppressing Chinese tallowtree and contribute to the integrated control of this invasive weed.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Analyze the following study abstract. \n",
    "First, identify the methodology. \n",
    "Second, explain if any variables were manipulated by the researchers. \n",
    "Third, conclude if the study is 'Experimental' or 'Observational'.\n",
    "\n",
    "Abstract: {abstract}\n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa25448",
   "metadata": {},
   "source": [
    "# Advanced control over the generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708ba80",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "To build a multi-turn conversation, we use the `chat` method.\n",
    "Unlike the generate method, chat allows the model to maintain context by keeping track of the history between the user and the LLM model.\n",
    "\n",
    "In a chat every message is assigned a specific role:\n",
    "- System: Sets the persona or \"rules\" for the AI (e.g., \"You are a helpful biologist\").\n",
    "- User: Your instructions or questions.\n",
    "- Assistant: The model's previous responses.\n",
    "\n",
    "The code below shows how to make a simple chat call, similar to the `generate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"gemma3\", messages=[{\"role\": \"user\", \"content\": \"What is ollama?\"}]\n",
    ")\n",
    "\n",
    "# Notice the output is a bit different. It has more metadata, but we can now access the\n",
    "# answer in the `content` of the `message`.\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb01427",
   "metadata": {},
   "source": [
    "A conversation is essentially a list of these message objects.\n",
    "To create a \"memory\" effect, you simply append new messages to the list and send the whole history back to the model.\n",
    "This allows the assistant (LLM) to refer back to things said earlier in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc82d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To maintain memory, we keep the history in a list\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise science communicator.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a trophic cascade?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A process where predators limit the density or behavior of their prey, benefiting the lower trophic levels.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Give me a classic example from North America.\"},\n",
    "]\n",
    "\n",
    "response = ollama.chat(model=\"gemma3\", messages=messages)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98743eb",
   "metadata": {},
   "source": [
    "## Temperature & reproducibility\n",
    "\n",
    "As we discussed in the \"stochasticity\" section, LLMs are probabilistic.\n",
    "If you want the model to be more predictable, you can adjust the temperature and seed.\n",
    "\n",
    "The **temperature** controls the \"randomness\" of the sampling:\n",
    "- When `temperature = 0` the model always picks the most likely next word.\n",
    "  This is best for data extraction and factual tasks.\n",
    "- When `temperature > 1` the model is more likely to choose rare words and becomes more \"creative\" and diverse, which may be better for brainstorming or creative writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b74b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name a common bird in the UK.\"}],\n",
    "    options={\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81259d5",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Up the temperature*\n",
    "\n",
    "Use the LLM to help you pick a title for a paper on the impact of microplastics on urban bumblebees.\n",
    "Experiment with the temperature 0, 0.7, 1.5, and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb59ab",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c6b2e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you suggest 3 alternative titles for a scientific paper on the impact of microplastics on urban bumblebees? Be creative. List the titles only, no conversation filler.\"\n",
    "\n",
    "print(\"--- Temperature = 0 ---\")\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\"temperature\": 0},\n",
    ")\n",
    "print(response.message.content)\n",
    "\n",
    "print(\"--- Temperature = 0.7 ---\")\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\"temperature\": 0.7},\n",
    ")\n",
    "print(response.message.content)\n",
    "\n",
    "print(\"--- Temperature = 1.5 ---\")\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\"temperature\": 1.5},\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95b429",
   "metadata": {},
   "source": [
    "A **seed** is a starting number for the random number generator.\n",
    "By setting a fixed seed the sampling process will make the same \"random\" choices every time.\n",
    "This is the way to achieve reproducibility in your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Pick one random British bird and describe it in 5 words.\",\n",
    "        }\n",
    "    ],\n",
    "    options={\n",
    "        \"temperature\": 0.7,  # We keep temperature up to allow for variety\n",
    "        \"seed\": 42,  # but the seed should keep the 'random' choice consistent.\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d8279",
   "metadata": {},
   "source": [
    "üí° **Note**: While these tools help, true 100% determinism is difficult to achieve in AI due to how computer hardware (GPUs) handles floating-point math.\n",
    "However, for most cases, `temperature=0` or a fixed seed are sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552b139",
   "metadata": {},
   "source": [
    "## Structured outputs\n",
    "\n",
    "Often, you don't want a conversational response.\n",
    "In metadata extraction, you need specific data points (like species names, sample sizes, or coordinates) that can be directly writen into a spreadsheet or database.\n",
    "\n",
    "This is such a common requirement that modern LLM libraries now provide ways to specify exactly how the output should be structured.\n",
    "In Python, the gold standard for this is **Pydantic**.\n",
    "Pydantic is a library used to define **data containers** (models).\n",
    "Each data point in a Pydantic model has a specific \"type\" (e.g., integer, string, list) and can include a description to help the LLM understand what to look for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39afa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic models are defined via \"classes\" in Python. No need to understand that for now, focus on the rest\n",
    "class BirdObservation(BaseModel):\n",
    "    # Each field has a name and a type. The common syntax is field_name: data_type\n",
    "    species_name: str = Field(description=\"The scientific name of the bird\")\n",
    "\n",
    "    # Notice that fields can be integers to.\n",
    "    count: int = Field(description=\"The number of individuals observed\")\n",
    "\n",
    "    # You can provide the list with a description which will help the model parse the relevant information\n",
    "    location: str = Field(description=\"The specific site or park name\")\n",
    "\n",
    "    # Some fields can be optional too\n",
    "    is_migratory: bool | None = Field(\n",
    "        default=None, description=\"Whether the species is known to be migratory\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce6f36",
   "metadata": {},
   "source": [
    "The model defines a schema, which is a textual description the structure of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're curious this is what the schema looks like. No need to understand it.\n",
    "BirdObservation.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca544cc6",
   "metadata": {},
   "source": [
    "When using `ollama`, we can pass this schema directly into the `chat` method using the `format` parameter.\n",
    "This forces the model to ignore its usual conversational output and return only a JSON object that matches your Pydantic model.\n",
    "\n",
    "üí° *A Note on JSON*: JSON (JavaScript Object Notation) is a lightweight text format for storing and transporting data.\n",
    "[Learn¬†more¬†about¬†JSON](https://www.json.org/json-en.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I saw three Erithacus rubecula at Hyde Park yesterday morning.\",\n",
    "        }\n",
    "    ],\n",
    "    format=BirdObservation.model_json_schema(),\n",
    "    options={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "# The output is now a clean JSON string\n",
    "print(response.message.content)\n",
    "\n",
    "# We can then turn that string back into a Python object for easy use\n",
    "data = BirdObservation.model_validate_json(response.message.content)\n",
    "print(f\"Extracted Species: {data.species_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c266385",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Metadata from a scientific abstract*\n",
    "\n",
    "Consider the following text\n",
    "\n",
    "> We conducted a 2-year field study on the growth of 45 individual Quercus robur saplings.\n",
    "> We monitored natural variations in soil moisture and leaf area index.\n",
    "> No experimental treatments were applied.\n",
    "\n",
    "Extract the focal species, the sample size and duration of the study using the previous approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf986d",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb982d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "We conducted a 2-year field study on the growth of 45 individual Quercus robur saplings.\n",
    "We monitored natural variations in soil moisture and leaf area index.\n",
    "No experimental treatments were applied.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AbstractMetadata(BaseModel):\n",
    "    focal_species: str = Field(description=\"Name of the main study species.\")\n",
    "\n",
    "    sample_size: int = Field(description=\"Total number of sampled units reported for the focal species.\")\n",
    "\n",
    "    duration: int = Field(description=\"Total study duration in years as an integer.\")\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text,\n",
    "        }\n",
    "    ],\n",
    "    format=AbstractMetadata.model_json_schema(),\n",
    "    options={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "\n",
    "abstract_data = AbstractMetadata.model_validate_json(response.message.content)\n",
    "print(abstract_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c37649",
   "metadata": {},
   "source": [
    "## Tool usage\n",
    "\n",
    "The final way to expand and control LLM outputs is through **tool usage**.\n",
    "While relatively recent, this is one of the most effective ways to ground a model's response by injecting factual, real-time data.\n",
    "\n",
    "The core idea is simple: an LLM can be given the ability to invoke external functions to enhance its capabilities.\n",
    "Instead of relying solely on its static training data, the model can interact with the external world when it needs more information.\n",
    "\n",
    "A common example is conducting searches of the web via a search engine.\n",
    "During its reasoning, the LLM might think it is worth doing some extra research and so indicates that it wants to use a web search tool.\n",
    "The LLM itself provides all the needed info on how to run that tool, for example what to search for, and the query is then executed.\n",
    "The search engine is not an LLM and the outputs are fed back to the LLM so that it can continue its reasoning with the tool's results now injected into its context.\n",
    "Other tools could include access to a calculator, a specific database like eBird, or any programmatic function you define.\n",
    "\n",
    "An **agent** is simply a loop where the model can access tools until it has enough information to finish the task.\n",
    "Here is an annotated implementation of that logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb14189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_with_tools(prompt, tools, model=\"qwen3\"):\n",
    "    # Start the chat with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Loop until we invoke the \"break\" keyword\n",
    "    while True:\n",
    "        # Given the current state of the chat generate an answer\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=list(tools.values()),  # Provide tools!\n",
    "            think=True,\n",
    "        )\n",
    "\n",
    "        # Append the new response to the chat history\n",
    "        messages.append(response.message)\n",
    "\n",
    "        if response.message.thinking:\n",
    "            print(\"Thinking: \", response.message.thinking)\n",
    "\n",
    "        if response.message.content:\n",
    "            print(\"Content: \", response.message.content)\n",
    "\n",
    "        # Do this in case the agent wants to use a tool\n",
    "        if response.message.tool_calls:\n",
    "            print(\"Tool calls: \", response.message.tool_calls)\n",
    "\n",
    "            # For each tool the agent wants to use\n",
    "            for tool_call in response.message.tool_calls:\n",
    "                # Get the tool by name\n",
    "                function_to_call = tools.get(tool_call.function.name)\n",
    "\n",
    "                if function_to_call:\n",
    "                    # Get the arguments specified by the LLM\n",
    "                    args = tool_call.function.arguments\n",
    "\n",
    "                    # And use the tool\n",
    "                    result = function_to_call(**args)\n",
    "                    print(\"Result: \", str(result)[:200] + \"...\")\n",
    "\n",
    "                    # Append to the chat history the results of the tool\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": str(result)[: 2000 * 4],\n",
    "                            \"tool_name\": tool_call.function.name,\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    # In case the tool wasn't found.\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": f\"Tool {tool_call.function.name} not found\",\n",
    "                            \"tool_name\": tool_call.function.name,\n",
    "                        }\n",
    "                    )\n",
    "        else:\n",
    "            # Break the agent loop if the model provides a final answer without more tool calls\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8adf4",
   "metadata": {},
   "source": [
    "Here is an example using the built-in web_search and web_fetch tools.\n",
    "Note that we are using `qwen3` here; not all models are trained to use tools, and `gemma3` currently is not.\n",
    "\n",
    "You will also need an additional key to use the search functionality.\n",
    "Please ask the instructor for this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API_KEY = \"<ask instructor for the key>\"\n",
    "\n",
    "client = ollama.Client(\n",
    "    host=\"http://localhost:11434\", headers={\"Authorization\": f\"Bearer {OLLAMA_API_KEY}\"}\n",
    ")\n",
    "\n",
    "available_tools = {\"web_search\": client.web_search, \"web_fetch\": client.web_fetch}\n",
    "\n",
    "run_agent_with_tools(\n",
    "    prompt=\"What is the main goal of the BIOS0032 module at UCL?\",\n",
    "    tools=available_tools,\n",
    "    model=\"qwen3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b62cbc",
   "metadata": {},
   "source": [
    "You can also create custom tools.\n",
    "For that, you write a standard Python function and then provide a precise description so the model understands when and how to use it.\n",
    "\n",
    "For an agent to use a tool effectively, it needs to know what the tool does and what arguments it expects.\n",
    "This is done by providing docstrings, the special strings defined with three quotes (\"\"\") immediately below the function definition.\n",
    "\n",
    "As with any prompting, the more precise you are in your description, the better the agent will perform.\n",
    "Following the [Google¬†docstring¬†style¬†guide](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) is highly recommended to make sure the model interprets your description correctly.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ebird_observations(region_code: str, days_back: int = 14):\n",
    "    \"\"\"\n",
    "    Fetches recent bird observations for a specific region from eBird.\n",
    "    Args:\n",
    "        region_code: The subnational code (e.g., 'US-NY' for New York).\n",
    "        days_back: Number of days to look back for records (1-30).\n",
    "    \"\"\"\n",
    "    # This is a mock tool; in a real scenario, you would query an API or database.\n",
    "    return f\"Found 12 observations in {region_code} over the last {days_back} days.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400ed3b8",
   "metadata": {},
   "source": [
    "Once the function is defined, you can include it in the tools dictionary when running your agent.\n",
    "The model will analyse your prompt, realise it needs bird observation data, and hopefully invoke `get_ebird_observations` with the correct `region_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453986a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent_with_tools(\n",
    "    prompt=\"What birds were seen in US-NY lately?\",\n",
    "    tools={\"get_ebird_observations\": get_ebird_observations},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498cb41",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Get the current date and time*\n",
    "\n",
    "First, ask `qwen3` to give you today's date.\n",
    "Since LLMs have a \"knowledge cutoff\" and aren't naturally aware of the passing of time, it will likely struggle to get it right.\n",
    "If it doesn't get it right, implement a tool that gives the current date and time to ground the model in the present.\n",
    "Hint: You can use the `datetime.datetime.now()` function from the built-in `datetime` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904db18",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4563",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_current_date():\n",
    "    \"\"\"Fetch current date and time.\"\"\"\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "# Run without tools\n",
    "run_agent_with_tools(\n",
    "    prompt=\"What day is today?\",\n",
    "    tools={},\n",
    ")\n",
    "\n",
    "# Run with new date tool!\n",
    "run_agent_with_tools(\n",
    "    prompt=\"What day is today?\",\n",
    "    tools={\"get_current_date\": get_current_date},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897bc54",
   "metadata": {},
   "source": [
    "# Extracting info from abstracts\n",
    "\n",
    "In this section we will attempt to replicate sections of [Scheepens¬†et¬†al.¬†2024](https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210x.14341).\n",
    "\n",
    "> Scheepens, D., Millard, J., Farrell, M., & Newbold, T. (2024). Large language models help facilitate the automated synthesis of information on potential pest controllers. Methods in Ecology and Evolution, 15(7), 1261-1273.\n",
    "\n",
    "First let's have a look of a set of 27 abstracts taken from the test set of that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abstracts = pd.read_csv(\"sample_abstracts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c7ae7",
   "metadata": {},
   "source": [
    "These are a small subset of all papers that may be relevant to the research question Daan has.\n",
    "Here is the full Scopus query used.\n",
    "\n",
    "> TITLE-ABS-KEY(‚Äùpest control‚Äù OR ‚Äùbiological control‚Äù OR ‚Äùpest management‚Äù OR ‚Äùnatural enem*‚Äù) AND (LIMIT-TO(DOCTYPE, ‚Äùar‚Äù)) AND (LIMIT-TO(SUBJAREA, ‚ÄùAGRI‚Äù) OR LIMIT-TO(SUBJAREA, ‚ÄùENVI‚Äù)) AND (LIMIT-TO(LANGUAGE, ‚ÄùEnglish‚Äù))\n",
    "\n",
    "Let's take a look at a single random abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstracts.sample(n=1).iloc[0][\"Abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5499ed4",
   "metadata": {},
   "source": [
    "For this section let's focus on getting the list of species mentioned and categorised as either hervibores or natural enemies.\n",
    "Here's the data schema Daan used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89089181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SpeciesList(BaseModel):\n",
    "    herbivores: list[str] = Field(\n",
    "        description=(\n",
    "            \"List of all herbivorous (i.e., phytophagous species) mentioned in the text. \"\n",
    "            \"Only list herbivorous species that are mentioned at the genus or species level (latin binomials).\"\n",
    "        ),\n",
    "    )\n",
    "    natural_enemies: list[str] = Field(\n",
    "        description=(\n",
    "            \"List of all natural enemies (e.g., predators or parasitoids) of herbivores mentioned in the text. \"\n",
    "            \"Only list natural enemies that are mentioned at the genus or species level (latin binomials). \"\n",
    "            \"Ignore species that are solely mentioned as hyperparasitoids or solely as predators of other natural enemies.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02521c46",
   "metadata": {},
   "source": [
    "Usually, you would select a subset of your data for \"training.\" In the context of LLMs, this doesn't mean retraining the model's weights, but rather fine-tuning your prompts and examples to get the best possible performance on that specific dataset.\n",
    "This is an iterative process that often feels like a mix of science and art.\n",
    "You might try different system personas, add few-shot examples, or adjust the Pydantic descriptions until the output matches your requirements.\n",
    "We are lucky Daan has already done that work, providing the optimised prompts and schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f7185",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Analyse the prompt*\n",
    "\n",
    "Have a look at Daan's prompts.\n",
    "Can you list all the prompting techniques he used?\n",
    "\n",
    "Here is the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed8bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert ecologist and taxonomist and you are tasked with extracting information on herbivorous species and their natural enemies in scientific publications.\n",
    "You are to carefully follow the instructions given to you and are to extract information only from the text provided to you.\n",
    "\n",
    "Reasoning: high\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40466266",
   "metadata": {},
   "source": [
    "and here is the user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b235a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_simple = \"\"\"\n",
    "Based on this abstract, you are tasked with identifying any herbivorous species and their natural enemies. Let's think through the following tasks step by step.\n",
    "\n",
    "\n",
    "# Herbivorous species\n",
    "\n",
    "Your first task is to determine if there are any mentions of herbivorous (i.e., phytophagous) species in the text.\n",
    "- If there are mentions of pests, you are looking only for species that are pests due to their herbivory, so ignore livestock pests such as flesh flies, vectors of human diseases, or any pest that isn't related directly to herbivory, such as fungi, bacteria or viruses.\n",
    "- Ideally, the species is mentioned in association with a particular industry (e.g. field crops, greenhouse production, pasture, fruit plantations, forestry and timber production, stored grains, ornamental and horticultural plants, etc.) or affect/host crop or plant (e.g. corn, apple, pine trees, stored wheat, etc.).\n",
    "- Herbivory does not have to be stated explicitly: If the pest belongs to an order or family of known herbivores, then herbivory may be inferred.\n",
    "- You are only interested in herbivores that are mentioned at the genus or species level (latin binomials): Ignore herbivores that are only mentioned at the family or order level, or that are only mentioned with their common name (e.g. spiders, mice, aphids, etc.). \n",
    "\n",
    "If the text mentions any herbivorous species at the genus or species level, list these. Only list herbivores from the main abstract - don't list species that are only mentioned in the keywords.\n",
    "\n",
    "\n",
    "# Natural enemies\n",
    "\n",
    "Your second task is to determine if there are any natural enemies (e.g., predators and parasitoids) of herbivores.\n",
    "- Natural enemies may be biological control agents of a herbivorous pest, but any mention or implication of predation, parasitism, or otherwise preying on, attacking or regulating a herbivore suffices as evidence for being a natural enemy of this herbivore.\n",
    "- Look out for mentions of consumption: If a species is found to consume a herbivorous species, this is evidence for the species being a natural enemy of the herbivore. \n",
    "- Only list natural enemies of herbivorous species: Ignore species that are solely mentioned as hyperparasitoids or predators that predate on other predators. \n",
    "- You are only interested in natural enemies that are mentioned at the genus or species level (latin binomials or genus only): Ignore natural enemies that are only mentioned at the family or order level, or that are only mentioned with their common name (e.g. spiders, mice, aphids, etc.). \n",
    "\n",
    "If the text mentions any natural enemies at the genus or species level, list these. Only list natural enemies from the main abstract - don't list species that are only mentioned in the keywords. \n",
    "\n",
    "\n",
    "# Important\n",
    "\n",
    "- Ignore any species that are plants, bacteria, fungi or viruses.\n",
    "- Only list species that are mentioned at the genus or species level. \n",
    "- Only list species from the main abstract - don't list species that are only mentioned in the keywords.\n",
    "- If the text mentions multiple synonyms of a species, only list one (ideally the more common/newest one).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8b563",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Daan's setup combines several prompting techniques.\n",
    "The system prompt uses role prompting by assigning the model the role of an expert ecologist and taxonomist.\n",
    "It is also zero-shot prompting, since no worked examples are provided.\n",
    "The user prompt applies chain-of-thought style guidance by asking the model to think step by step and by splitting the task into two structured sections (herbivores and natural enemies) with clear criteria.\n",
    "It also uses strong instruction constraints, i.e. what to include, what to exclude, output granularity, and where species must appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c8d1f",
   "metadata": {},
   "source": [
    "Now we can combine everything we've learned (role prompting, structured outputs, and reproducibility settings) into a single function.\n",
    "\n",
    "This function takes a row from our dataset, bundles the title and keywords for context, and asks the model to extract the species.\n",
    "Note how we use `temperature=0` and a `seed` to ensure the results are consistent every time we run the code.\n",
    "It uses the data schema, as well as the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_species_list(row):\n",
    "    # Gather all available information for the paper\n",
    "    title = row[\"Title\"]\n",
    "    abstract_text = row[\"Abstract\"]\n",
    "    author_keys = row[\"Author Keywords\"]\n",
    "    index_keys = row[\"Index Keywords\"]\n",
    "    paper_info = (\n",
    "        f\"{title}. Abstract: {abstract_text}\"\n",
    "        f\"\\nAuthor keywords: {author_keys}\"\n",
    "        f\"\\nIndex keywords: {index_keys}\"\n",
    "    )\n",
    "\n",
    "    # Assemble the final prompt\n",
    "    prompt = f\"\"\"\n",
    "    Here are a title, abstract and keywords: {paper_info}. \n",
    "    {user_prompt_simple}\n",
    "    Explain your reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the conversation with the system role\n",
    "    chat_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = client.chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=chat_history,\n",
    "        format=SpeciesList.model_json_schema(),  # use the schema!\n",
    "        options={\n",
    "            \"temperature\": 0,  # Notice temperature = 0\n",
    "            \"seed\": 42,  # and seed is set\n",
    "            \"think\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Convert the JSON string response back into a Python object\n",
    "    return SpeciesList.model_validate_json(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820bcd6",
   "metadata": {},
   "source": [
    "You can test this on a single random abstract from your collection to see how well it performs before running it on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single random row\n",
    "row = abstracts.sample(n=1).iloc[0]\n",
    "print(f\"Abstract: {row['Abstract']}\")\n",
    "\n",
    "# Run our extraction pipeline\n",
    "species_list = extract_species_list(row)\n",
    "print(species_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962edd1e",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Compare against ground truth.*\n",
    "\n",
    "Load the `test_set.xlsx` file.\n",
    "It contains all the human annotations of each of the abstracts of the test set.\n",
    "You can find all 27 of our subset here.\n",
    "You can identify them by their \"EID\" (electronic identifier).\n",
    "Extract the species on all of the sample set and compare to the ground truth.\n",
    "How many did we get right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499389ed",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e181c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the 'ground truth' test data from an Excel file\n",
    "test_set = pd.read_excel(\"test_set.xlsx\")\n",
    "\n",
    "# Filter the main 'abstracts' DataFrame to only keep rows with an EID that is present in our test set.\n",
    "# The .isin() method checks if each EID in abstracts exists in the test_set's EID column.\n",
    "test_abstracts = abstracts[abstracts[\"EID\"].isin(test_set[\"EID\"])]\n",
    "\n",
    "\n",
    "# Helper function to combine separate Genus and Species columns\n",
    "# into a single string representing the full binomial scientific name.\n",
    "def combine_binomial(row):\n",
    "    genus = row[\"Genus\"]\n",
    "    species = row[\"Species\"]\n",
    "    return f\"{genus} {species}\"\n",
    "\n",
    "\n",
    "# Initialize counters for evaluation metrics\n",
    "total_correct = 0\n",
    "total_species = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Loop through each abstract row\n",
    "for index, row in test_abstracts.iterrows():\n",
    "    eid = row[\"EID\"]\n",
    "\n",
    "    # Extract predicted species using our custom function\n",
    "    species_list = extract_species_list(row)\n",
    "\n",
    "    # Combine herbivores and natural enemies into one list.\n",
    "    # Using set() removes duplicates; sorted() sorts them alphabetically.\n",
    "    predicted_species = sorted(set(species_list.herbivores + species_list.natural_enemies))\n",
    "\n",
    "    # Get the ground truth\n",
    "    # Filter the test set for the current abstract's EID\n",
    "    subset = test_set[test_set[\"EID\"] == eid]\n",
    "\n",
    "    # Apply combine_binomial row-by-row to get scientific names\n",
    "    species_names = subset.apply(combine_binomial, axis=1)\n",
    "\n",
    "    # Remove duplicates and sort them alphabetically\n",
    "    ground_truth_species = sorted(set(species_names))\n",
    "\n",
    "    # Update Counters\n",
    "    total_species += len(ground_truth_species)\n",
    "    total_predictions += len(predicted_species)\n",
    "\n",
    "    # Evaluate each predicted species\n",
    "    for pred_species in predicted_species:\n",
    "        # Count a prediction true if it appears in the ground truth list\n",
    "        if pred_species in ground_truth_species:\n",
    "            total_correct += 1\n",
    "\n",
    "# Print a summary of the extraction performance\n",
    "print(f\"Summary: The model extracted {total_predictions} species ({total_correct} correct). Actual species count: {total_species}.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
