{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3818b6",
   "metadata": {},
   "source": [
    "# Week 4 AI & Text\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "This week, we will:\n",
    "\n",
    "- Learn how to interact with Large Language Models (LLMs) via code.\n",
    "- Explore different prompting techniques and identify common prompting pitfalls.\n",
    "- Apply LLMs to the extraction of metadata from scientific paper abstracts.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d498df",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e25854",
   "metadata": {},
   "source": [
    "## Setup script\n",
    "\n",
    "Before anything please run the following cell to install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL URL | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee2264",
   "metadata": {},
   "source": [
    "This may take some time to finish so continue reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0fc5a",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Running a **Large Language Model (LLM)** is not easy, and usually, it's hidden from us behind web chat interfaces like ChatGPT or Gemini.\n",
    "However, we can run the models ourselves, and this is what we will be doing in this notebook using Ollama.\n",
    "\n",
    "[**Ollama**](https://ollama.com/) is an open-source tool that allows us to run Large Language Models (LLMs) on our own computer (or in this case, on the Google Colab server) rather than sending our data to a company like OpenAI or Google via the internet.\n",
    "In research, this is very useful because it gives us more control over the models and data privacy, and allows us to use \"open-source\" models for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fda36c",
   "metadata": {},
   "source": [
    "## Choosing an LLM model\n",
    "\n",
    "On Ollama, and other platforms like Hugging Face, you will find hundreds if not thousands of models to choose from.\n",
    "Have a look at [Ollama‚Äôs¬†model¬†library](https://ollama.com/search).\n",
    "This can be overwhelming!\n",
    "However, there are a few considerations to keep in mind when choosing a model:\n",
    "\n",
    "- **Model Size**: This refers to the number of parameters the model has.\n",
    "  It is directly related to how big the model file is, how much memory (RAM) it needs, and how long it takes to generate a response.\n",
    "  A larger model is likely to generate better responses, but you may not have the hardware to run it.\n",
    "  In our environment, we have limited resources, so we will favour smaller, efficient models.\n",
    "\n",
    "- **Intent and Specialisation**: Some models are \"generic,\" while others are trained for specific purposes like coding, mathematical reasoning, or safety reviews.\n",
    "  Choose one that aligns with your specific research goal.\n",
    "\n",
    "- **Context Size**: Depending on the task, you may need a model with a large context window to process large chunks of text (like a full scientific paper) at once.\n",
    "  This is especially important for multi-step interactions where a long conversation needs to fit into the model's context.\n",
    "\n",
    "- **Model Capabilities**: As we will see below, some models have \"thinking\" abilities, others can use \"tools\" (like calculators), and others can analyse images.\n",
    "\n",
    "- **Recency**: More recent models are likely to perform better due to novel research and innovation in training techniques.\n",
    "\n",
    "üí° Safety Note: Downloading unknown models can be risky, but any model from well-known AI labs or those that are highly popular on these platforms are safe to test.\n",
    "\n",
    "Today we will use two models:\n",
    "\n",
    "- **Gemma 3 (by Google)** is an open-source model.\n",
    "  We are using a lightweight version today that features a 128k token context window (approx. 100,000 words), allowing it to process large chunks of text at once.\n",
    "  While it is multimodal and can analyse images, we will focus on its text-processing abilities.\n",
    "\n",
    "- **Qwen 3 (by Alibaba)** is a specialised \"thinking model.\" Unlike standard LLMs that answer instantly, a thinking model generates a hidden chain of reasoning first.\n",
    "  This helps it handle complex logic and multi-step problems without making mistakes.\n",
    "  It is also designed for tool use, meaning it can work with external functions to increase its accuracy for specific tasks.\n",
    "\n",
    "*Important note:* Because these are \"smaller\" models, they are more limited than the commercial versions, which are usually massive.\n",
    "However, all models share the main limitations, such as hallucinations (making up facts).\n",
    "The prompting tips we cover today are relevant to all models, large or small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34d244",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "\n",
    "Here we will use the ollama Python library to interact with the models.\n",
    "The first step is to create a \"client\" that connects to the ollama program running in the background.\n",
    "\n",
    "Since we have ollama installed on this specific Colab machine, we use localhost (which means \"this computer\").\n",
    "However, this same code could be used to connect to a remote server or a high-performance computer elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Create the client to talk to the local Ollama service\n",
    "client = ollama.Client(host=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efe204",
   "metadata": {},
   "source": [
    "The easiest way to prompt an LLM using Ollama is with the generate method.\n",
    "This is a \"one-shot\" interaction: you send a prompt, and the model sends back a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple response using the Gemma model\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What is ecology?\")\n",
    "\n",
    "# The result is a complex object with metadata, but we can print just the text\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a90ea3",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Ask anything*\n",
    "\n",
    "Ask `gemma3` anything you want.\n",
    "Try asking it to define a specific ecological term or to write a short story about a forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250aa1ed",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Compare models*\n",
    "\n",
    "Run the same prompt (\"What is ecology?\") on `qwen3.\n",
    "Are the results different?\n",
    "How is the tone or structure different?\n",
    "Is there one that seems more \"useful\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29449e59",
   "metadata": {},
   "source": [
    "# Usual pitfalls\n",
    "\n",
    "In this section, we will explore different ways LLMs can generate unhelpful or even harmful outputs, and look at simple strategies to mitigate these risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18918c2a",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "\n",
    "The most significant challenge when using LLMs is their tendency to generate **hallucinations**: outputs that are perfectly coherent but factually false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(model=\"gemma3\", prompt=\"What happened to the Parisian tiger?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1986f",
   "metadata": {},
   "source": [
    "If you use model outputs uncritically, it is very likely you will include something false that looks correct.\n",
    "In academic research, text is scrutinised heavily by peers and supervisors.\n",
    "Scientists are specifically trained to spot inconsistencies, so relying on unverified AI output can be a big risk.\n",
    "**Always** double-check any output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae56478",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Reference checking*\n",
    "\n",
    "A common and dangerous type of hallucination is the creation of fake scientific references.\n",
    "Run the following prompt and using [Google¬†Scholar](https://scholar.google.com/) verify if any of the papers found exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(model=\"gemma3\", prompt=\"Give me a list of recent impactful papers that explore the relation between bats migration and climate change.\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a6d98",
   "metadata": {},
   "source": [
    "This happens because LLMs operate on statistical probability.\n",
    "During training, the model learns the structural patterns of language; since scientific citations follow a highly predictable format, the model can \"simulate\" a reference by mixing common researcher names with relevant keywords.\n",
    "Because its internal knowledge is static (frozen at the moment its training ended) it isn't searching a real library.\n",
    "Instead, it is simply predicting what a plausible citation looks like based on the patterns it has seen.\n",
    "\n",
    "Nowadays, commercial models try to fix this by using web search to ground their answers in real-time data.\n",
    "While this helps significantly, it doesn't solve the problem entirely.\n",
    "Models can still misinterpret the search results or hallucinate details even when they have the correct source in front of them.\n",
    "**Always double check the outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209daa2",
   "metadata": {},
   "source": [
    "## Stochasticity\n",
    "\n",
    "LLMs generate text by \"sampling\" one word at a time.\n",
    "Instead of simply predicting a single next token, the model assigns a probability score to every possible token in its vocabulary, giving higher scores to those that are most likely to follow.\n",
    "It then samples a word at random based on these scores.\n",
    "This means the model's output can change from one run to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fe959",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Word predictability*\n",
    "\n",
    "Take the following sentence from Daan's paper:\n",
    "\n",
    "> The body of ecological literature, which informs much of our knowledge of the global loss of biodiversity, has been experiencing rapid growth in recent decades.\n",
    "\n",
    "Which of these words are most predictable based on the preceding text?\n",
    "Use the function below to test this on different segments, such as \"The body of...\" or \"The body of ecological...\".\n",
    "Prompt the model several times with the same segment to see if, and how often, the response changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sentence_chunk):\n",
    "    result = client.generate(model=\"gemma3\", prompt=f\"Complete the given sentence. One word only. {sentence_chunk}:\",)\n",
    "\n",
    "    return result.response\n",
    "\n",
    "# How predictable is ecological?\n",
    "print(predict_next_word(\"The body of\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b807491",
   "metadata": {},
   "source": [
    "This randomness can be a significant issue for the reproducibility of science.\n",
    "We can address this stochasticity by lowering the **\"temperature\"** of the model, which concentrates the probability on the most likely word, or by setting a **\"seed\"** to ensure the same random choices are made every time.\n",
    "While we will explore this later, note that these settings are often harder to control when using commercial models through their standard web interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2162af3",
   "metadata": {},
   "source": [
    "## Verbosity\n",
    "\n",
    "Model outputs tend to be very long and verbose.\n",
    "You can see this in the previous examples, but let's test another scenario: imagine you encountered the term \"LLM\" for the first time and wanted a quick definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a64d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A broad question often leads to a very long, conversational answer\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What is an LLM?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4dee3",
   "metadata": {},
   "source": [
    "Verbosity increases the likelihood of the output containing hallucinations or generally unhelpful content.\n",
    "It also makes it harder to find the specific information you need for your research.\n",
    "\n",
    "You can address this in two ways: specify a length limit or be more precise about what you are asking.\n",
    "For example, if you only need a brief summary, tell the model exactly how much to write:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa13aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a clear constraint helps\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What is an LLM? Reply in less than 100 words\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a142a1f",
   "metadata": {},
   "source": [
    "Often, a small change in the question results in a much more useful and direct answer.\n",
    "Instead of asking a broad \"what is\" question, try to pinpoint the exact information you require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc16566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more specific question leads to a more targeted response\n",
    "result = client.generate(model=\"gemma3\", prompt=\"What does the acronym LLM stand for?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d47497",
   "metadata": {},
   "source": [
    "## Logical errors\n",
    "\n",
    "Models do not reason in the way we understand it‚Äîby constructing solid, valid logical arguments step by step.\n",
    "While they can emulate reasoning, they often fail even in simple scenarios that require basic spatial or logical awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.generate(model=\"gemma3\", prompt=\"I get out on the top floor (third floor) at street level. How many stories is the building above the ground?\")\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d582b",
   "metadata": {},
   "source": [
    "One way researchers address this is by using models specifically trained to \"reason\" or \"think\".\n",
    "During the fine-tuning process (often using Reinforcement Learning), these models are rewarded for creating accurate reasoning chains, which significantly improves their performance on logical tasks.\n",
    "Another method is \"Chain-of-Thought\" (CoT) prompting, where you explicitly ask the model to explain its thinking process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2540b",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Reasoning models*\n",
    "\n",
    "`qwen3` is a reasoning model, it generates a long chain of text where it lays out its internal logic.\n",
    "Try the same \"third floor\" prompt with `qwen3`.\n",
    "Does it get it right?\n",
    "Why or why not?\n",
    "Give your interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed7533",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Chain of thought prompting*\n",
    "\n",
    "Now, return to `gemma3`.\n",
    "Try asking the same question again, but this time, specifically instruct it to \"provide a logical explanation step by step before giving the final answer\".\n",
    "Does the quality of the output improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47cc63",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Since model answers are based on the most statistically likely word, the output is heavily influenced by the training data.\n",
    "If certain words, concepts, or taxa appear together frequently in the text the model was trained on, it will naturally recreate those associations.\n",
    "This can bias the outputs towards common or well-documented examples, often at the expense of less-studied or minority cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe which taxa the model prioritises based on common literature trends\n",
    "prompt = (\n",
    "    \"Complete the sentence by filling in the blank with a single taxa, only provide the answer: \"\n",
    "    \"Recent years have seen increasing pressures from multiple fronts including \"\n",
    "    \"increased tourism and encroaching deforestation, in particular populations \"\n",
    "    \"of <blank> have been declining.\"\n",
    ")\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a84b1",
   "metadata": {},
   "source": [
    "Because of this, LLM outputs should rarely be used to answer questions that require a careful consideration of multiple viewpoints or diverse sources.\n",
    "While the model can be a helpful search tool, you must be aware of its biases and always contrast its output with other sources of information.\n",
    "Ultimately, answers to complex ecological questions should incorporate broad evidence and use rigorous statistical approaches to control for known biases in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26186a",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Spatial bias*\n",
    "\n",
    "Compare the model's ability to name common bird species in two different ecological contexts.\n",
    "Use the function below to list three common species in the UK, then try it again for a location you suspect is less represented in global ecological literature.\n",
    "Verify if the species provided are actually found in those locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_bird_species(location, n = 3):\n",
    "    result = client.generate(model=\"gemma3\", prompt=f\"List {n} common bird species in the {location}. Provide the list only, if possible using both common and scientific names.\")\n",
    "    return result.response\n",
    "\n",
    "print(get_common_bird_species(\"UK\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd12e51",
   "metadata": {},
   "source": [
    "## Writing style\n",
    "\n",
    "Due to the inherent bias in LLMs, it is generally a **bad idea** to ask them to write a research piece from scratch.\n",
    "Doing so often generates generic content that may not align with your specific intent or the nuances of your study.\n",
    "It is much safer to provide the model with a skeleton or a draft of your ideas and use the tool to help you polish the language or structure.\n",
    "While the outputs may look like high-quality academic writing, academic evaluation focuses on the strength of your ideas, narrative, and argumentation, areas where LLMs are not inherently strong.\n",
    "Ultimately, a model will either produce generic information or build upon what you provide.\n",
    "Therefore, it remains **your responsibility to provide the solid reasoning and verified facts** that form the core of the work.\n",
    "\n",
    "When using LLMs to refine or generate text, it is important to account for their default writing style.\n",
    "They are trained to be \"helpful\" and \"polite,\" which often results in a particular tone that may be too wordy or formal for your needs.\n",
    "You can steer the model by being specific about the desired tone and audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt often results in the model's 'default' academic style\n",
    "prompt = '''Refine my text:\n",
    "\n",
    "LLMs are being increasingly applied in ecology.\n",
    "They can be used to extract information from papers.\n",
    "This makes large automated sythesis collection feasible.\n",
    "It is very recent but already promising.\n",
    "'''\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e20ff6",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Guide the style*\n",
    "\n",
    "Assume you want to write a blog post for first-year undergraduate students about LLMs in ecology.\n",
    "Use the previous text as a starting point, but modify the prompt to steer the style so it communicates the ideas effectively for that specific audience.\n",
    "Write your improved prompt below and compare the output to the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7be018",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In general, follow these basic recommendations when working with LLMs:\n",
    "\n",
    "- **Always check the output**: Never assume the model is factually correct.\n",
    "- **Be precise**: Give clear instructions on exactly how you want your output to look.\n",
    "- **Use restrictions**: Add constraints to guide the model, such as \"use a maximum of 100 words\" or \"format as a bulleted list.\"\n",
    "- **Challenge bias**: Models are inherently biased toward their training data; do not rely on them as your sole source of information.\n",
    "- **Account for randomness**: Output may vary from one run to the next.\n",
    "- **Provide context**: The more information and background you provide, the more focused and relevant the outputs will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39b3ed",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Critique the prompt*\n",
    "\n",
    "Have a look at the following prompt.\n",
    "Based on what we have discussed regarding verbosity, bias, and precision, how could you make it better?\n",
    "\n",
    "> \"Tell me about climate change and birds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf94b0d",
   "metadata": {},
   "source": [
    "# General prompting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13930d0",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "\n",
    "In **zero-shot prompting**, you provide the model with a direct instruction or question without any prior examples or demonstrations.\n",
    "This is the \"usual\" approach to prompting.\n",
    "You are relying on the model's pre-trained knowledge to understand and execute the instruction.\n",
    "\n",
    "While simple, zero-shot prompting is highly effective for general tasks like basic summarisation or sentiment analysis.\n",
    "However, as we saw previously, it is likely to result in hallucinations or formatting errors.\n",
    "In a zero-shot context, the model performs significantly better when your prompt is very specific and rich in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c62400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical zero-shot prompt: just an instruction\n",
    "prompt = \"Classify the following research description as 'Field Study' or 'Lab Experiment': We measured the growth rates of 50 oak seedlings in a temperature-controlled greenhouse.\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6601634",
   "metadata": {},
   "source": [
    "## Role prompting\n",
    "\n",
    "A **role prompt** provides the model with a specific persona.\n",
    "By instructing the model to \"Act as a senior research ecologist,\" you steer it away from the generic, conversational tone of a standard \"chatbot\".\n",
    "While this doesn't give the model new knowledge it hasn't been trained on, it can significantly improve the depth and relevance of the information it chooses to present.\n",
    "You are pre-conditioning the output to a particular style or scientific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c15174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how the tone and detail change when a role is assigned\n",
    "task = \"Explain why biodiversity loss matters.\"\n",
    "\n",
    "# Standard instruction\n",
    "prompt_simple = f\"Answer this: {task}\"\n",
    "\n",
    "# Role-based instruction\n",
    "prompt_role = f\"You are a Professor of Conservation Biology. Provide a technically rigorous answer to this: {task}\"\n",
    "\n",
    "print(\"--- Simple Prompt ---\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_simple).response)\n",
    "\n",
    "print(\"--- Role Prompt ---\")\n",
    "print(client.generate(model=\"gemma3\", prompt=prompt_role).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ced86a",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è *Switching Personas*\n",
    "\n",
    "Try changing the role to something very different, like \"A science journalist writing for a local newspaper\" or \"A critical peer-reviewer\".\n",
    "Notice how the model adjusts its vocabulary and the types of evidence it emphasises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95462b",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "\n",
    "LLMs are great at recognising patterns and we can leverage that.\n",
    "Few-shot prompting involves giving the model 2 or 3 examples of how you want a task done before asking it to perform the task itself.\n",
    "Simplified example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide examples of extracting a 'focal species' from a sentence\n",
    "prompt = \"\"\"\n",
    "Extract the focal species from the text.\n",
    "\n",
    "Text: We monitored the nesting habits of Chelonia mydas in Costa Rica.\n",
    "Species: Chelonia mydas\n",
    "\n",
    "Text: Camera traps were used to detect the presence of Panthera onca.\n",
    "Species: Panthera onca\n",
    "\n",
    "Text: Our study focus was the movement of migrating Passer domesticus across urban gradients.\n",
    "Species: \n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92ba65",
   "metadata": {},
   "source": [
    "## Chain-of-thought prompting\n",
    "\n",
    "For complex tasks, it a helpful strategy is to ask it to decompose the tasks into several steps and provide reasoning for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Identify if a study is 'Experimental' or 'Observational'\n",
    "\n",
    "abstract = '''\n",
    "Chinese tallowtree, Triadica sebifera (L.) Small (Euphorbiaceae), is one of the worst invasive weeds of the southeastern USA impacting coastal wetlands, forests, and natural areas.\n",
    "Traditional mechanical and chemical controls have been unable to limit the spread, and this invasive species continues to expand its range.\n",
    "A proposed biological control candidate, the flea beetle Bikasha collaris (Baly) (Coleoptera: Chrysomelidae), shows high specificity for the target weed Chinese tallowtree.\n",
    "Results from a series of no-choice and choice feeding tests of B.¬†collaris adults and larvae indicated that this flea beetle was highly specific to Chinese tallowtree.\n",
    "The larvae of B.¬†collaris feed by tunneling in the roots, whereas the adults feed on the leaves of Chinese tallowtree.\n",
    "A total of 77 plant taxa, primarily from members of the tallow plant family Euphorbiaceae, were tested in numerous test designs.\n",
    "Larval no-choice tests indicated that larvae completed development only on two of the non-target taxa.\n",
    "Of 80 B.¬†collaris larvae fed roots of Hippomane mancinella L. and 50 larvae fed roots of Ricinus communis L., two and three larvae completed development, respectively.\n",
    "The emerging adults of these five larvae died within 3¬†days without reproducing.\n",
    "Larval choice tests also indicated little use of these non-target taxa.\n",
    "Adult no-choice tests indicated little leaf damage by B.¬†collaris on the non-targets except for Ditrysinia fruticosa (Bartram) Govaerts & Frodin and Gymnanthes lucida Sw. When given a choice, however, B.¬†collaris adults consumed much less of the non-targets D.¬†fruticosa (7.4%) and G.¬†lucida (6.1%) compared with the control leaves.\n",
    "Finally, no-choice oviposition tests indicated that no eggs were produced when adults were fed all non-target taxa, except those fed G.¬†lucida.\n",
    "These B.¬†collaris adults fed G.¬†lucida leaves produced an average of 4.6 eggs compared with 115.0 eggs per female when fed Chinese tallowtree.\n",
    "The eggs produced from adults fed G.¬†lucida were either inviable or the emerging larvae died within 1¬†day.\n",
    "These results indicate that the flea beetle B.¬†collaris was unable to complete its life cycle on any of the non-target taxa tested.\n",
    "If approved for field release, B.¬†collaris will be the first biological control agent deployed against Chinese tallow tree in the USA.\n",
    "This flea beetle may play an important role in suppressing Chinese tallowtree and contribute to the integrated control of this invasive weed.\n",
    "'''\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Analyze the following study abstract. \n",
    "First, identify the methodology. \n",
    "Second, explain if any variables were manipulated by the researchers. \n",
    "Third, conclude if the study is 'Experimental' or 'Observational'.\n",
    "\n",
    "Abstract: {abstract}\n",
    "\"\"\"\n",
    "\n",
    "result = client.generate(model=\"gemma3\", prompt=prompt)\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a53dc4",
   "metadata": {},
   "source": [
    "# Advanced control over the generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1c47b",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "To build a multi-turn conversation, we use the `chat` method.\n",
    "Unlike the generate method, chat allows the model to maintain context by keeping track of the history between the user and the LLM model.\n",
    "\n",
    "In a chat every message is assigned a specific role:\n",
    "- System: Sets the persona or \"rules\" for the AI (e.g., \"You are a helpful biologist\").\n",
    "- User: Your instructions or questions.\n",
    "- Assistant: The model's previous responses.\n",
    "\n",
    "The code below shows how to make a simple chat call, similar to the `generate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'What is ollama?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Notice the output is a bit different. It has more metadata, but we can now access the\n",
    "# answer in the `content` of the `message`.\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da6ada",
   "metadata": {},
   "source": [
    "A conversation is essentially a list of these message objects.\n",
    "To create a \"memory\" effect, you simply append new messages to the list and send the whole history back to the model.\n",
    "This allows the assistant (LLM) to refer back to things said earlier in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To maintain memory, we keep the history in a list\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a concise science communicator.'},\n",
    "    {'role': 'user', 'content': 'What is a trophic cascade?'},\n",
    "    {'role': 'assistant', 'content': 'A process where predators limit the density or behavior of their prey, benefiting the lower trophic levels.'},\n",
    "    {'role': 'user', 'content': 'Give me a classic example from North America.'}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model=\"gemma3\", messages=messages)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11e912",
   "metadata": {},
   "source": [
    "## Temperature & reproducibility\n",
    "\n",
    "As we discussed in the \"stochasticity\" section, LLMs are probabilistic.\n",
    "If you want the model to be more predictable, you can adjust the temperature and seed.\n",
    "\n",
    "The **temperature** controls the \"randomness\" of the sampling:\n",
    "- When `temperature = 0` the model always picks the most likely next word.\n",
    "  This is best for data extraction and factual tasks.\n",
    "- When `temperature > 1` the model is more likely to choose rare words and becomes more \"creative\" and diverse, which may be better for brainstorming or creative writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab165ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[{'role': 'user', 'content': 'Name a common bird in the UK.'}],\n",
    "    options={\n",
    "        \"temperature\": 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae8c81",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Up the temperature*\n",
    "\n",
    "Use the LLM to help you pick a title for a paper on the impact of microplastics on urban bumblebees.\n",
    "Experiment with the temperature 0, 0.7, 1.5, and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1fceb",
   "metadata": {},
   "source": [
    "A **seed** is a starting number for the random number generator.\n",
    "By setting a fixed seed the sampling process will make the same \"random\" choices every time.\n",
    "This is the way to achieve reproducibility in your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24efcb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Pick one random British bird and describe it in 5 words.'}\n",
    "    ],\n",
    "    options={\n",
    "        \"temperature\": 0.7, # We keep temperature up to allow for variety\n",
    "        \"seed\": 42,         # but the seed should keep the 'random' choice consistent.\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d02a3",
   "metadata": {},
   "source": [
    "üí° **Note**: While these tools help, true 100% determinism is difficult to achieve in AI due to how computer hardware (GPUs) handles floating-point math.\n",
    "However, for most cases, `temperature=0` or a fixed seed are sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e948678",
   "metadata": {},
   "source": [
    "## Structured outputs\n",
    "\n",
    "Often, you don't want a conversational response.\n",
    "In metadata extraction, you need specific data points (like species names, sample sizes, or coordinates) that can be directly writen into a spreadsheet or database.\n",
    "\n",
    "This is such a common requirement that modern LLM libraries now provide ways to specify exactly how the output should be structured.\n",
    "In Python, the gold standard for this is **Pydantic**.\n",
    "Pydantic is a library used to define **data containers** (models).\n",
    "Each data point in a Pydantic model has a specific \"type\" (e.g., integer, string, list) and can include a description to help the LLM understand what to look for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic models are defined via \"classes\" in Python. No need to understand that for now, focus on the rest\n",
    "class BirdObservation(BaseModel):\n",
    "    # Each field has a name and a type. The common syntax is field_name: data_type\n",
    "    species_name: str = Field(description=\"The scientific name of the bird\")\n",
    "\n",
    "    # Notice that fields can be integers to.\n",
    "    count: int = Field(description=\"The number of individuals observed\")\n",
    "\n",
    "    # You can provide the list with a description which will help the model parse the relevant information\n",
    "    location: str = Field(description=\"The specific site or park name\")\n",
    "\n",
    "    # Some fields can be optional too\n",
    "    is_migratory: bool | None = Field(default=None, description=\"Whether the species is known to be migratory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057db295",
   "metadata": {},
   "source": [
    "The model defines a schema, which is a textual description the structure of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ee207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're curious this is what the schema looks like. No need to understand it.\n",
    "BirdObservation.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba334a9",
   "metadata": {},
   "source": [
    "When using `ollama`, we can pass this schema directly into the `chat` method using the `format` parameter.\n",
    "This forces the model to ignore its usual conversational output and return only a JSON object that matches your Pydantic model.\n",
    "\n",
    "üí° *A Note on JSON*: JSON (JavaScript Object Notation) is a lightweight text format for storing and transporting data.\n",
    "[Learn¬†more¬†about¬†JSON](https://www.json.org/json-en.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e28d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"gemma3\",\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': 'I saw three Erithacus rubecula at Hyde Park yesterday morning.'\n",
    "        }\n",
    "    ],\n",
    "    format=BirdObservation.model_json_schema(),\n",
    "    options={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "# The output is now a clean JSON string\n",
    "print(response.message.content)\n",
    "\n",
    "# We can then turn that string back into a Python object for easy use\n",
    "data = BirdObservation.model_validate_json(response.message.content)\n",
    "print(f\"Extracted Species: {data.species_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39338a24",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Metadata from a scientific abstract*\n",
    "\n",
    "Consider the following text\n",
    "\n",
    "> We conducted a 2-year field study on the growth of 45 individual Quercus robur saplings.\n",
    "> We monitored natural variations in soil moisture and leaf area index.\n",
    "> No experimental treatments were applied.\n",
    "\n",
    "Extract the focal species, the sample size and duration of the study using the previous approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa515ce",
   "metadata": {},
   "source": [
    "## Tool usage\n",
    "\n",
    "The final way to expand and control LLM outputs is through **tool usage**.\n",
    "While relatively recent, this is one of the most effective ways to ground a model's response by injecting factual, real-time data.\n",
    "\n",
    "The core idea is simple: an LLM can be given the ability to invoke external functions to enhance its capabilities.\n",
    "Instead of relying solely on its static training data, the model can interact with the external world when it needs more information.\n",
    "\n",
    "A common example is conducting searches of the web via a search engine.\n",
    "During its reasoning, the LLM might think it is worth doing some extra research and so indicates that it wants to use a web search tool.\n",
    "The LLM itself provides all the needed info on how to run that tool, for example what to search for, and the query is then executed.\n",
    "The search engine is not an LLM and the outputs are fed back to the LLM so that it can continue its reasoning with the tool's results now injected into its context.\n",
    "Other tools could include access to a calculator, a specific database like eBird, or any programmatic function you define.\n",
    "\n",
    "An **agent** is simply a loop where the model can access tools until it has enough information to finish the task.\n",
    "Here is an annotated implementation of that logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35de377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_with_tools(prompt, tools, model=\"qwen3\"):\n",
    "    # Start the chat with the user prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Loop until we invoke the \"break\" keyword\n",
    "    while True:\n",
    "        # Given the current state of the chat generate an answer\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=list(tools.values()),  # Provide tools!\n",
    "            think=True,\n",
    "        )\n",
    "\n",
    "        # Append the new response to the chat history\n",
    "        messages.append(response.message)\n",
    "\n",
    "        if response.message.thinking:\n",
    "            print(\"Thinking: \", response.message.thinking)\n",
    "\n",
    "        if response.message.content:\n",
    "            print(\"Content: \", response.message.content)\n",
    "\n",
    "        # Do this in case the agent wants to use a tool\n",
    "        if response.message.tool_calls:\n",
    "            print(\"Tool calls: \", response.message.tool_calls)\n",
    "\n",
    "            # For each tool the agent wants to use\n",
    "            for tool_call in response.message.tool_calls:\n",
    "\n",
    "                # Get the tool by name\n",
    "                function_to_call = tools.get(tool_call.function.name)\n",
    "\n",
    "                if function_to_call:\n",
    "                    # Get the arguments specified by the LLM\n",
    "                    args = tool_call.function.arguments\n",
    "\n",
    "                    # And use the tool\n",
    "                    result = function_to_call(**args)\n",
    "                    print(\"Result: \", str(result)[:200] + \"...\")\n",
    "\n",
    "                    # Append to the chat history the results of the tool\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": str(result)[: 2000 * 4],\n",
    "                            \"tool_name\": tool_call.function.name,\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    # In case the tool wasn't found.\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": f\"Tool {tool_call.function.name} not found\",\n",
    "                            \"tool_name\": tool_call.function.name,\n",
    "                        }\n",
    "                    )\n",
    "        else:\n",
    "            # Break the agent loop if the model provides a final answer without more tool calls\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d269ca2",
   "metadata": {},
   "source": [
    "Here is an example using the built-in web_search and web_fetch tools.\n",
    "Note that we are using `qwen3` here; not all models are trained to use tools, and `gemma3` currently is not.\n",
    "\n",
    "You will also need an additional key to use the search functionality.\n",
    "Please ask the instructor for this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API_KEY = \"<ask instructor for the key>\"\n",
    "\n",
    "client = ollama.Client(host=\"http://localhost:11434\", headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'})\n",
    "\n",
    "available_tools = {\"web_search\": client.web_search, \"web_fetch\": client.web_fetch}\n",
    "\n",
    "run_agent_with_tools(\n",
    "    prompt=\"What is the main goal of the BIOS0032 module at UCL?\",\n",
    "    tools=available_tools,\n",
    "    model=\"qwen3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02863b3f",
   "metadata": {},
   "source": [
    "You can also create custom tools.\n",
    "For that, you write a standard Python function and then provide a precise description so the model understands when and how to use it.\n",
    "\n",
    "For an agent to use a tool effectively, it needs to know what the tool does and what arguments it expects.\n",
    "This is done by providing docstrings, the special strings defined with three quotes (\"\"\") immediately below the function definition.\n",
    "\n",
    "As with any prompting, the more precise you are in your description, the better the agent will perform.\n",
    "Following the [Google¬†docstring¬†style¬†guide](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) is highly recommended to make sure the model interprets your description correctly.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ebird_observations(region_code: str, days_back: int = 14):\n",
    "    \"\"\"\n",
    "    Fetches recent bird observations for a specific region from eBird.\n",
    "    Args:\n",
    "        region_code: The subnational code (e.g., 'US-NY' for New York).\n",
    "        days_back: Number of days to look back for records (1-30).\n",
    "    \"\"\"\n",
    "    # This is a mock tool; in a real scenario, you would query an API or database.\n",
    "    return f\"Found 12 observations in {region_code} over the last {days_back} days.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158cd57",
   "metadata": {},
   "source": [
    "Once the function is defined, you can include it in the tools dictionary when running your agent.\n",
    "The model will analyse your prompt, realise it needs bird observation data, and hopefully invoke `get_ebird_observations` with the correct `region_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ebc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent_with_tools(prompt=\"What birds were seen in US-NY lately?\", tools={\"get_ebird_observations\": get_ebird_observations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1e48d",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Get the current date and time*\n",
    "\n",
    "First, ask `qwen3` to give you today's date.\n",
    "Since LLMs have a \"knowledge cutoff\" and aren't naturally aware of the passing of time, it will likely struggle to get it right.\n",
    "If it doesn't get it right, implement a tool that gives the current date and time to ground the model in the present.\n",
    "Hint: You can use the `datetime.datetime.now()` function from the built-in `datetime` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791108ab",
   "metadata": {},
   "source": [
    "# Extracting info from abstracts\n",
    "\n",
    "In this section we will attempt to replicate sections of [Scheepens¬†et¬†al.¬†2024](https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210x.14341).\n",
    "\n",
    "> Scheepens, D., Millard, J., Farrell, M., & Newbold, T. (2024). Large language models help facilitate the automated synthesis of information on potential pest controllers. Methods in Ecology and Evolution, 15(7), 1261-1273.\n",
    "\n",
    "First let's have a look of a set of 27 abstracts taken from the test set of that paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abstracts = pd.read_csv(\"sample_abstracts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3354c5",
   "metadata": {},
   "source": [
    "These are a small subset of all papers that may be relevant to the research question Daan has.\n",
    "Here is the full Scopus query used.\n",
    "\n",
    "> TITLE-ABS-KEY(‚Äùpest control‚Äù OR ‚Äùbiological control‚Äù OR ‚Äùpest management‚Äù OR ‚Äùnatural enem*‚Äù) AND (LIMIT-TO(DOCTYPE, ‚Äùar‚Äù)) AND (LIMIT-TO(SUBJAREA, ‚ÄùAGRI‚Äù) OR LIMIT-TO(SUBJAREA, ‚ÄùENVI‚Äù)) AND (LIMIT-TO(LANGUAGE, ‚ÄùEnglish‚Äù))\n",
    "\n",
    "Let's take a look at a single random abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a52c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abstracts.sample(n=1).iloc[0][\"Abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ede95",
   "metadata": {},
   "source": [
    "For this section let's focus on getting the list of species mentioned and categorised as either hervibores or natural enemies.\n",
    "Here's the data schema Daan used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class SpeciesList(BaseModel):\n",
    "    herbivores: list[str] = Field(\n",
    "        description=(\n",
    "            \"List of all herbivorous (i.e., phytophagous species) mentioned in the text. \"\n",
    "            \"Only list herbivorous species that are mentioned at the genus or species level (latin binomials).\"\n",
    "        ),\n",
    "    )\n",
    "    natural_enemies: list[str] = Field(\n",
    "        description=(\n",
    "            \"List of all natural enemies (e.g., predators or parasitoids) of herbivores mentioned in the text. \"\n",
    "            \"Only list natural enemies that are mentioned at the genus or species level (latin binomials). \"\n",
    "            \"Ignore species that are solely mentioned as hyperparasitoids or solely as predators of other natural enemies.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248b1e3",
   "metadata": {},
   "source": [
    "Usually, you would select a subset of your data for \"training.\" In the context of LLMs, this doesn't mean retraining the model's weights, but rather fine-tuning your prompts and examples to get the best possible performance on that specific dataset.\n",
    "This is an iterative process that often feels like a mix of science and art.\n",
    "You might try different system personas, add few-shot examples, or adjust the Pydantic descriptions until the output matches your requirements.\n",
    "We are lucky Daan has already done that work, providing the optimised prompts and schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06026019",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Analyse the prompt*\n",
    "\n",
    "Have a look at Daan's prompts.\n",
    "Can you list all the prompting techniques he used?\n",
    "\n",
    "Here is the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert ecologist and taxonomist and you are tasked with extracting information on herbivorous species and their natural enemies in scientific publications.\n",
    "You are to carefully follow the instructions given to you and are to extract information only from the text provided to you.\n",
    "\n",
    "Reasoning: high\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b65e36",
   "metadata": {},
   "source": [
    "and here is the user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca7dfc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "user_prompt_simple = \"\"\"\n",
    "Based on this abstract, you are tasked with identifying any herbivorous species and their natural enemies. Let's think through the following tasks step by step.\n",
    "\n",
    "\n",
    "# Herbivorous species\n",
    "\n",
    "Your first task is to determine if there are any mentions of herbivorous (i.e., phytophagous) species in the text.\n",
    "- If there are mentions of pests, you are looking only for species that are pests due to their herbivory, so ignore livestock pests such as flesh flies, vectors of human diseases, or any pest that isn't related directly to herbivory, such as fungi, bacteria or viruses.\n",
    "- Ideally, the species is mentioned in association with a particular industry (e.g. field crops, greenhouse production, pasture, fruit plantations, forestry and timber production, stored grains, ornamental and horticultural plants, etc.) or affect/host crop or plant (e.g. corn, apple, pine trees, stored wheat, etc.).\n",
    "- Herbivory does not have to be stated explicitly: If the pest belongs to an order or family of known herbivores, then herbivory may be inferred.\n",
    "- You are only interested in herbivores that are mentioned at the genus or species level (latin binomials): Ignore herbivores that are only mentioned at the family or order level, or that are only mentioned with their common name (e.g. spiders, mice, aphids, etc.). \n",
    "\n",
    "If the text mentions any herbivorous species at the genus or species level, list these. Only list herbivores from the main abstract - don't list species that are only mentioned in the keywords.\n",
    "\n",
    "\n",
    "# Natural enemies\n",
    "\n",
    "Your second task is to determine if there are any natural enemies (e.g., predators and parasitoids) of herbivores.\n",
    "- Natural enemies may be biological control agents of a herbivorous pest, but any mention or implication of predation, parasitism, or otherwise preying on, attacking or regulating a herbivore suffices as evidence for being a natural enemy of this herbivore.\n",
    "- Look out for mentions of consumption: If a species is found to consume a herbivorous species, this is evidence for the species being a natural enemy of the herbivore. \n",
    "- Only list natural enemies of herbivorous species: Ignore species that are solely mentioned as hyperparasitoids or predators that predate on other predators. \n",
    "- You are only interested in natural enemies that are mentioned at the genus or species level (latin binomials or genus only): Ignore natural enemies that are only mentioned at the family or order level, or that are only mentioned with their common name (e.g. spiders, mice, aphids, etc.). \n",
    "\n",
    "If the text mentions any natural enemies at the genus or species level, list these. Only list natural enemies from the main abstract - don't list species that are only mentioned in the keywords. \n",
    "\n",
    "\n",
    "# Important\n",
    "\n",
    "- Ignore any species that are plants, bacteria, fungi or viruses.\n",
    "- Only list species that are mentioned at the genus or species level. \n",
    "- Only list species from the main abstract - don't list species that are only mentioned in the keywords.\n",
    "- If the text mentions multiple synonyms of a species, only list one (ideally the more common/newest one).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15cbc6",
   "metadata": {},
   "source": [
    "Now we can combine everything we've learned (role prompting, structured outputs, and reproducibility settings) into a single function.\n",
    "\n",
    "This function takes a row from our dataset, bundles the title and keywords for context, and asks the model to extract the species.\n",
    "Note how we use `temperature=0` and a `seed` to ensure the results are consistent every time we run the code.\n",
    "It uses the data schema, as well as the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_species_list(row):\n",
    "    # Gather all available information for the paper\n",
    "    title = row[\"Title\"]\n",
    "    abstract_text = row[\"Abstract\"]\n",
    "    author_keys = row[\"Author Keywords\"]\n",
    "    index_keys = row[\"Index Keywords\"]\n",
    "    paper_info = (\n",
    "        f\"{title}. Abstract: {abstract_text}\"\n",
    "        f\"\\nAuthor keywords: {author_keys}\"\n",
    "        f\"\\nIndex keywords: {index_keys}\"\n",
    "    )\n",
    "\n",
    "    # Assemble the final prompt\n",
    "    prompt = f\"\"\"\n",
    "    Here are a title, abstract and keywords: {paper_info}. \n",
    "    {user_prompt_simple}\n",
    "    Explain your reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the conversation with the system role\n",
    "    chat_history = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = client.chat(\n",
    "        model=\"gemma3\",\n",
    "        messages=chat_history,\n",
    "        format=SpeciesList.model_json_schema(), # use the schema!\n",
    "        options={\n",
    "            \"temperature\": 0,  # Notice temperature = 0\n",
    "            \"seed\": 42,  # and seed is set\n",
    "            \"think\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Convert the JSON string response back into a Python object\n",
    "    return SpeciesList.model_validate_json(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf08ab",
   "metadata": {},
   "source": [
    "You can test this on a single random abstract from your collection to see how well it performs before running it on the full set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760382de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single random row\n",
    "row = abstracts.sample(n=1).iloc[0]\n",
    "print(f\"Abstract: {row['Abstract'}\")\n",
    "\n",
    "# Run our extraction pipeline\n",
    "species_list = extract_species_list(row)\n",
    "print(species_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9762cd",
   "metadata": {},
   "source": [
    "#### üñåÔ∏èCompare against ground truth.\n",
    "\n",
    "Load the `test_set.xlsx` file.\n",
    "It contains all the human annotations of each of the abstracts of the test set.\n",
    "You can find all 27 of our subset here.\n",
    "You can identify them by their \"EID\" (electronic identifier).\n",
    "Extract the species on all of the sample set and compare to the ground truth.\n",
    "How many did we get right?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
