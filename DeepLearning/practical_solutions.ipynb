{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16257d1",
   "metadata": {},
   "source": [
    "# Week 3 Deep Learning\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "This week, we will:\n",
    "\n",
    "- Build and train deep learning models.\n",
    "- Diagnose and fix common training issues.\n",
    "- Implement transfer learning techniques.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4933c",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Building deep learning models is a complex task.\n",
    "It involves designing architectures with sometimes millions or billions of parameters and optimising them using gradient descent‚Äîthe process by which training loss is reduced and model parameters are adjusted.\n",
    "These operations must also be computationally efficient to minimise training and inference time.\n",
    "Fortunately, several libraries provide high-level tools to handle these complexities, making it easier to create and train models effectively.\n",
    "\n",
    "The most popular deep learning frameworks are `PyTorch`, `TensorFlow`, and `JAX`.\n",
    "While these are open-source, they are primarily developed by Meta (`PyTorch`) and Google (`TensorFlow` and `JAX`).\n",
    "Although there are specialised reasons to choose one over another, either is suitable for most problems; the choice usually depends on convenience and personal preference.\n",
    "`PyTorch` is often preferred in academic circles because it is intuitive and flexible, making it ideal for quick experimentation.\n",
    "\n",
    "![pytorch](https://pytorch.org/wp-content/uploads/2025/01/pytorch_seo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch library\n",
    "import torch\n",
    "\n",
    "# We can use it to check if we can use the GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265721f",
   "metadata": {},
   "source": [
    "In this notebook, however, we will use `Keras`.\n",
    "It is designed as a high-level wrapper, making it more user-friendly than those underlying frameworks.\n",
    "While `Keras` runs on top of these libraries, it hides their complexity from the user.\n",
    "If you eventually need to develop highly custom features or examine internal mechanics, you would likely work directly with a framework like `PyTorch` or `TensorFlow`, but for well-established workflows, `Keras` makes the process much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We need to specify that we want to use torch\n",
    "# as the \"backend\" for keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f77756",
   "metadata": {},
   "source": [
    "We will also use a few additional libraries to support our workflow.\n",
    "Since we will be working with images, we use `Pillow` (PIL) for image manipulation and torchvision to bridge the gap between `Pillow` and `PyTorch`.\n",
    "Finally, we will use `timm`, a package that provides access to a large set of pre-trained image models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9aebc1",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50283a",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "For this notebook, we will use an example dataset of moth images as a case study.\n",
    "\n",
    "![moths](https://storage.googleapis.com/kaggle-datasets-images/2439824/4128992/4527186ab6ea46da0adb9bbce34e8d81/dataset-cover.jpg?t=2022-08-27-19-01-29)\n",
    "\n",
    "This dataset was shared via [kaggle](https://www.kaggle.com/).\n",
    "Kaggle is a great resource for machine learning, serving as a platform that hosts datasets, models, and more.\n",
    "It also hosts competitions where participants attempt to train the best model for a specific task.\n",
    "\n",
    "This particular [dataset](https://www.kaggle.com/datasets/gpiosenka/moths-image-datasetclassification?select=MOTHS.csv) was assembled by Kaggle user Gerry through internet searches for various moth species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f0c2c",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "Let's download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ef3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle offers a library to interface with their platforms\n",
    "import kagglehub\n",
    "\n",
    "# We can download the dataset. The returned object is the path to where it was downloaded\n",
    "download_dir = kagglehub.dataset_download(\"gpiosenka/moths-image-datasetclassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3c219",
   "metadata": {},
   "source": [
    "The dataset includes a CSV file containing metadata for each image.\n",
    "This includes the location of the file within the download folder (`filepaths`), the species name (`labels`), and a pre-defined split for training, testing, and validation (`data set`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "path_to_table = os.path.join(download_dir, \"MOTHS.csv\")\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "df = pd.read_csv(path_to_table)\n",
    "\n",
    "# Check its first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee945bb8",
   "metadata": {
    "region_name": "md",
    "tags": [
     "note"
    ]
   },
   "source": [
    "üí°**Note:** If you are unfamiliar with `os.path.join`, here is a brief explanation.\n",
    "If you have a file named `my_file.txt` in a folder called `my_folder`, the path to that file on Windows is `my_folder\\my_file.txt`, whereas on macOS and Linux it is `my_folder/my_file.txt`.\n",
    "This difference (`/` vs `\\`) means code shared across different operating systems might break.\n",
    "The `os.path.join` function handles these differences automatically, making your code platform-independent.\n",
    "Using it is considered best practice for writing shareable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37b561",
   "metadata": {},
   "source": [
    "### Dataset summary\n",
    "\n",
    "We can now count how many images are available for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of images per species and sort by frequency\n",
    "counts = df.labels.value_counts().sort_values(ascending=False)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac260c",
   "metadata": {},
   "source": [
    "Plotting these counts makes the distribution easier to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "# Plot the number of counts\n",
    "ax.plot(range(len(counts)), counts)\n",
    "\n",
    "# Make the y axis range start from 0\n",
    "ax.set_ylim(0, counts.max() + 10)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel(\"Number of images\")\n",
    "ax.set_xlabel(\"Species rank\")\n",
    "ax.set_title(\"Counts of images per species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04de94c",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Full dataset summary*\n",
    "\n",
    "When publishing a model, it is good practice to provide a detailed breakdown of your dataset.\n",
    "This includes the number of examples per species across the training, validation, and test sets.\n",
    "Create a table with one row for each species and three columns showing the image counts for that species in each of the three splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105d9e1",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217e0d4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter the main dataframe into separate dataframes for each split\n",
    "train_df = df[df[\"data set\"] == \"train\"]\n",
    "validation_df = df[df[\"data set\"] == \"valid\"]\n",
    "test_df = df[df[\"data set\"] == \"test\"]\n",
    "\n",
    "# Count the number of images for each species within each split\n",
    "train_counts = train_df[\"labels\"].value_counts()\n",
    "validation_counts = validation_df[\"labels\"].value_counts()\n",
    "test_counts = test_df[\"labels\"].value_counts()\n",
    "\n",
    "# Combine these counts into a single summary dataframe\n",
    "# Each Series becomes a column named 'train', 'validation', or 'test'\n",
    "full_dataset_summary = pd.DataFrame({\n",
    "    \"train\": train_counts,\n",
    "    \"validation\": validation_counts,\n",
    "    \"test\": test_counts\n",
    "})\n",
    "\n",
    "# Check the result\n",
    "full_dataset_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8332017",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Alternative method (concise but less intuitive)\n",
    "\n",
    "# Create a summary by grouping the data by labels and dataset split\n",
    "# The size() function counts the occurrences in each group (combination of species + split),\n",
    "# and unstack() moves the 'data set' labels into columns.\n",
    "full_dataset_summary = df.groupby([\"labels\", \"data set\"]).size().unstack()\n",
    "\n",
    "full_dataset_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1640d8b",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "We will now use Pillow (`PIL`) to load and view some of the moth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# select a random example\n",
    "example = df.sample(n=1).iloc[0]\n",
    "\n",
    "# And display it\n",
    "im = Image.open(os.path.join(download_dir, example.filepaths))\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.title(example.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6efd7f",
   "metadata": {},
   "source": [
    "Each time you run this cell, a different image is selected at random from the dataset.\n",
    "Try running the cell several times to see the variety of species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df23d92",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Qualitative dataset description*\n",
    "\n",
    "Suppose you wanted to develop a moth identifier for biologists to use on mobile devices in the field.\n",
    "After looking at several images, list two reasons why training with this dataset might be unsuitable for this task, and two reasons why it might be suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043011c",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Reasons why it is unsuitable:\n",
    "\n",
    "1. Most images were taken under ideal conditions (simple backgrounds, consistent lighting, and centered subjects).\n",
    "   Field images are often messy, with complex backgrounds like foliage and poor lighting, which might cause the model to fail in real-world use.\n",
    "2. Some images contain watermarks or other artifacts.\n",
    "   A model might mistakenly learn to identify a species based on these artifacts rather than the moth's actual features, leading to errors when those artifacts are missing.\n",
    "\n",
    "Reasons why it is suitable.\n",
    "\n",
    "1. The dataset covers 50 different species.\n",
    "   This diversity helps the model learn to distinguish between many different patterns and shapes, making it more useful than a model trained on only a few types.\n",
    "2. With 100‚Äì200 images per species, there is enough data to create a reliable split for training and testing.\n",
    "   This ensures we can properly evaluate the model's accuracy for every species before deploying it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27e1e3",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "An essential step in deep learning is preparing your data for the model.\n",
    "This involves converting raw data into a numerical representation (e.g. turning an image into an array of pixel values) and applying transformations like cropping or scaling values to a 0‚Äì1 range.\n",
    "\n",
    "These steps are known as **preprocessing**.\n",
    "Getting this stage right is very important, as the quality of preprocessing can significantly impact a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cdd6e",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "The first step is converting images into a numerical format.\n",
    "\n",
    "In previous notebooks, we used `numpy` to store and process arrays of data.\n",
    "`torch` uses its own version of a numerical array called a **tensor**.\n",
    "In most situations, a tensor behaves similarly to a `numpy` array, making it easy to apply what you already know.\n",
    "However, they are not identical, and we will highlight the key differences as we progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08102aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the transforms provided by torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Create a transformation object that converts PIL images to an image tensor\n",
    "to_tensor = v2.ToImage()\n",
    "\n",
    "# Apply the transform to our example image\n",
    "im_tensor = to_tensor(im)\n",
    "\n",
    "im_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dacf8c9",
   "metadata": {},
   "source": [
    "We can also convert tensors back to images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transform that converts image tensors back to PIL images\n",
    "to_pil = v2.ToPILImage()\n",
    "\n",
    "# Apply the transform to the image tensor\n",
    "reconstructed_im = to_pil(im_tensor)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(reconstructed_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5059b",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "`torchvision` provides a variety of transforms for manipulating images.\n",
    "For example, we can use these tools to resize images and reduce their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation that will resize any image into an 32x32 image\n",
    "resize = v2.Resize([32, 32])\n",
    "\n",
    "# Apply the transform to the image *tensor*\n",
    "resized = resize(im_tensor)\n",
    "\n",
    "# Plot the original and resized images side-by-side for comparison\n",
    "_, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.imshow(im)\n",
    "ax2.imshow(to_pil(resized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd54c38",
   "metadata": {},
   "source": [
    "Multiple transformations can be combined into a single pipeline.\n",
    "For example, we can resize an image and convert it to grayscale in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5637e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a pipeline of multiple transformations\n",
    "compose_transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),  # convert to image tensor\n",
    "        v2.Resize([32, 32]),  # resize 32x32\n",
    "        v2.Grayscale(),  # make grayscale\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the combined transformations to the original PIL image\n",
    "compose_tensor = compose_transform(im)\n",
    "\n",
    "# Plot the original and processed results side-by-side\n",
    "_, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.imshow(im)\n",
    "ax2.imshow(to_pil(compose_tensor), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250d400",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Why rescaling?*\n",
    "\n",
    "The original images in this dataset are 224x224 pixels.\n",
    "Reducing the resolution makes each image smaller by reducing the total number of pixels, but it also results in a loss of detail.\n",
    "When developing a model, what are some considerations that might guide your decision on which image size to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7815866",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Three considerations:\n",
    "\n",
    "1. Some moth species look very similar and are only distinguished by fine-grained details, like small spots or the shape of their antennae.\n",
    "   Reducing the resolution can remove these features, making it impossible for the model to tell similar species apart.\n",
    "2. Lower-resolution images require significantly less memory and processing power.\n",
    "   This is an important consideration if you plan to deploy the model on low-powered devices or if you need to limit energy consumption.\n",
    "3. While high-resolution images provide more detail, they also introduce more uninformative \"noise\" (unique variations on a specific moth that aren't actually characteristic of the species as a whole).\n",
    "   A model using high-resolution images might need a much larger dataset to learn which details to ignore, otherwise it risks overfitting to those irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6e14b",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "For training and evaluation, we use a **dataset**‚Äîa collection of examples, which in our case are images of moths.\n",
    "We need a way to load these images and iterate through them during the training process.\n",
    "\n",
    "In this dataset, images are organised into `train`, `valid`, and `test` folders.\n",
    "Within each of these, there are subfolders named after each species containing the corresponding images.\n",
    "Like so:\n",
    "\n",
    "```\n",
    "download_dir\n",
    "‚îú‚îÄ‚îÄ test\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "‚îú‚îÄ‚îÄ train\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "‚îî‚îÄ‚îÄ valid\n",
    "    ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "    ‚îú‚îÄ‚îÄ ...\n",
    "    ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "\n",
    "```\n",
    "\n",
    "This is a standard structure for classification tasks.\n",
    "We can use the `ImageFolder` utility to load this data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab497aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define the path to the training directory\n",
    "train_dataset_path = os.path.join(download_dir, \"train\")\n",
    "\n",
    "# Create a transform for the dataset\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),  # convert to image tensor\n",
    "        v2.Resize([32, 32]),  # resize 32x32\n",
    "        v2.ToDtype(torch.float32, scale=True),  # convert to floating numbers\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a dataset from the folder structure\n",
    "# Subfolder names are automatically used as the labels for the images within them\n",
    "# Setting `transform=transform` ensures every image is pre-processed automatically when loaded.\n",
    "train_dataset = ImageFolder(train_dataset_path, transform=transform)\n",
    "\n",
    "# The length of the dataset represents the total number of examples\n",
    "print(f\"Num of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e05632",
   "metadata": {},
   "source": [
    "You can access individual items from the dataset as you would with a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first element in the dataset\n",
    "# Notice it contains both the image tensor and the label (species)\n",
    "im, label = train_dataset[0]\n",
    "\n",
    "print(f\"Image label: {label}\")\n",
    "\n",
    "plt.imshow(to_pil(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de73fc",
   "metadata": {},
   "source": [
    "You may have noticed that the image labels appear as numbers rather than species names.\n",
    "This is because models require numerical inputs to perform calculations.\n",
    "However, the dataset keeps track of the original class names and the mapping between these integers and the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of all species names detected in the folder structure\n",
    "print(train_dataset.classes)\n",
    "\n",
    "# We'll store the total number of classes for later use\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Retrieve the original species name from an integer label\n",
    "class_name = train_dataset.classes[label]\n",
    "print(f\"Label to species: {label} -> {class_name}\")\n",
    "\n",
    "# Find the integer label associated with a specific species name\n",
    "species = \"REGAL MOTH\"\n",
    "label = train_dataset.class_to_idx[species]\n",
    "print(f\"Species to label: {species} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaedde",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The final step is the actual process of loading the images.\n",
    "When iterating over a dataset for training or evaluation, we typically process data in **batches**.\n",
    "Loading files from a disk can be slow and is often the primary bottleneck in the training process.\n",
    "To address this, `torch` provides the `DataLoader` utility, which makes data retrieval more efficient by handling batching and loading in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader to handle batching and shuffling\n",
    "# 'batch_size' determines how many images are processed at once\n",
    "# 'shuffle=True' ensures batches are assembled randomly during training\n",
    "# 'num_workers=2' enables multi-processing, loading two images in parallel for efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "# A loader is an 'iterable', allowing you to loop through the entire dataset\n",
    "for im_batch, label_batch in train_loader:\n",
    "    # This is where you would typically pass the batches to your model\n",
    "\n",
    "    # Check the shape of the image batch\n",
    "    print(f\"Image batch shape: {im_batch.shape}\")\n",
    "\n",
    "    # We break the loop here\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdb667",
   "metadata": {},
   "source": [
    "To help verify that our data is loading correctly, we can visualise an entire batch of images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a609cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_image_batch(batch, ncols=8, figsize=None):\n",
    "    # Calculate the number of rows required based on the batch size and columns\n",
    "    batch_size = len(batch)\n",
    "    nrows = int(np.ceil(batch_size / ncols))  # np.ceil rounds up to nearest integer\n",
    "\n",
    "    # Create a figure with a grid of sub-axes\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "\n",
    "    # Iterate over images in batch and axes in figure\n",
    "    for im, ax in zip(batch, axes.flatten()):\n",
    "        ax.imshow(to_pil(im))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualise the batch we just loaded\n",
    "plot_image_batch(im_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea93cf8",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Practice batching*\n",
    "\n",
    "Create a new dataset using the test directory.\n",
    "This time, keep the images at their **original resolution** but convert them to **grayscale**.\n",
    "Once the dataset and loader are ready, draw a **random** batch of **16 images** and plot them using the function above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03591c0",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9223a2",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the path to the test directory\n",
    "test_dataset_path = os.path.join(download_dir, \"test\")\n",
    "\n",
    "# Define the transform: convert to tensor, then to grayscale, then scale values\n",
    "test_transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Grayscale(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a dataset using all the images in the test folder and the given transform\n",
    "test_dataset = ImageFolder(test_dataset_path, transform=test_transform)\n",
    "\n",
    "# Create a new data loader for the test dataset with random batches of 16 images.\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Taking a batch using a loop with a break.\n",
    "# This is a one way to grab the first batch and stop.\n",
    "for im_batch, label_batch in test_loader:\n",
    "    # stop after the first iteration\n",
    "    break\n",
    "\n",
    "# Option 2: Using the next() function.\n",
    "# This is a more concise way to pull a single element from an iterator.\n",
    "im_batch, label_batch = next(iter(test_loader))\n",
    "\n",
    "# Plot the resulting batch\n",
    "plot_image_batch(im_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451ca05",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# If your plot appears in shades of blue and green, it is because of the\n",
    "# default \"viridis\" colormap used by Matplotlib. You can change this globally to\n",
    "# grayscale with the following:\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set the default colormap to grayscale\n",
    "mpl.rc('image', cmap='gray')\n",
    "\n",
    "# Re-run the plotting function\n",
    "plot_image_batch(im_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe41fe2",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "With our data loading pipeline in place, we are ready to build our first deep learning model.\n",
    "We will use `keras`, which provides high-level building blocks to create complex architectures and manage the training process.\n",
    "\n",
    "As discussed in the lectures, the fundamental building block is the fully connected layer, known in Keras as a **Dense** layer.\n",
    "In this layer, every neuron is connected to every neuron in the preceding layer.\n",
    "We also apply an **activation** function, such as **ReLU** (Rectified Linear Unit), to introduce non-linearity, allowing the model to learn complex patterns.\n",
    "\n",
    "Here is an example of a neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "keras.config.set_image_data_format(\"channels_first\")\n",
    "\n",
    "resolution = 32\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc9a5b",
   "metadata": {},
   "source": [
    "When defining the model architecture, there are several key components to understand:\n",
    "\n",
    "First, we specify the input shape.\n",
    "Our data consists of RGB images defined by their height and width.\n",
    "In this example, we use 32√ó32 pixels to match our downscaled data, though the model can be configured for any resolution.\n",
    "\n",
    "Then we have the **Flatten** layer.\n",
    "This converts the 3D structure of the image into a 1D list of features.\n",
    "In this basic model, we do not account for the spatial relationship between pixels; instead, every pixel value across all three colour channels is treated as an individual input neuron.\n",
    "\n",
    "Then we have two intermediate layers each with 64 neurons.\n",
    "Both of them use **ReLU** as the activation function.\n",
    "\n",
    "The final layer is a dense layer where the number of neurons matches the total number of moth species in our dataset.\n",
    "This layer uses the softmax activation function, which is key for classification.\n",
    "Softmax takes the raw outputs from the network and scales them so that:\n",
    "\n",
    "- Every output value is between 0 and 1.\n",
    "- The sum of all output values equals exactly 1.\n",
    "\n",
    "This allow us to interpret the output of each neuron as a probability.\n",
    "For example, a value of 0.75 on a specific neuron indicates a 75% confidence that the image belongs to that particular species.\n",
    "\n",
    "We can use the `summary` method to view a detailed breakdown of the model.\n",
    "This includes the output shape of each layer and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc524db4",
   "metadata": {},
   "source": [
    "We can also generate a visual diagram to see how the data flows through the different layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f27e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641929b",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Dependance of model size on resolution*\n",
    "\n",
    "The model above is relatively simple, with only two hidden layers and a modest number of neurons, yet it already contains around 200,000 parameters.\n",
    "Try changing the input resolution using the values (8, 16, 32, 64, 128, 256) and record the number of trainable parameters for each case.\n",
    "Plot this relationship to see how the model size changes as the image resolution increases.\n",
    "What patterns do you observe in the plot?\n",
    "What does this suggest about the practical trade-offs involved when choosing an image resolution for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e34e3",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e70d9e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# List of candidate resolutions\n",
    "resolutions = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "# Empty list to store parameter counts\n",
    "num_parameters = []\n",
    "\n",
    "for resolution in resolutions:\n",
    "    # Build the model using the current resolution in the input shape\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Get the total number of parameters. \n",
    "    # You could either print the summary and manually capture each parameter count \n",
    "    # or use this built in `method` from `keras`\n",
    "    total_parameters = model.count_params()\n",
    "    num_parameters.append(total_parameters)\n",
    "\n",
    "# Print to see the final numbers\n",
    "print(num_parameters)\n",
    "\n",
    "# Plotting the results\n",
    "plt.plot(resolutions, num_parameters, marker='o')\n",
    "plt.xlabel(\"Image resolution (N x N)\")\n",
    "plt.ylabel(\"Number of trainable parameters\")\n",
    "plt.title(\"Model Size vs. Input Resolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44302bfd",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The plot shows that the number of parameters grows at an increasing rate as resolution rises.\n",
    "Increasing the resolution of a large image adds significantly more parameters than doing the same for a small image.\n",
    "At a resolution of 256, the model reaches over 12 million parameters.\n",
    "\n",
    "In practical terms this means that:\n",
    "1. As the resolution increases, the model quickly becomes too large to store in memory or too slow to train on standard hardware.\n",
    "2. Higher resolution leads to a disproportionate increase in \"model capacity\" (the number of parameters).\n",
    "   Without a significantly larger dataset to match this capacity, the model is much more likely to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b2a289",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We are now ready to train our model.\n",
    "As you observed in the previous exercise, using high-resolution images leads to a significantly larger model with more parameters.\n",
    "To allow for faster experimentation, we will start with a very low resolution of 8√ó8 pixels.\n",
    "At this size, it is difficult to identify specific moth features, but basic shapes and colours remain visible.\n",
    "\n",
    "First, we define the transformation and the model architecture for this 8√ó8 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a338b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for 8x8 resolution\n",
    "transform_0 = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize([8, 8]),\n",
    "        v2.ToDtype(torch.float32, scale=True), # scale=True ensures values are 0-1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset_0 = ImageFolder(train_dataset_path, transform=transform_0)\n",
    "\n",
    "# Build a small model for fast experimentation\n",
    "model_0 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 8, 8]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a847f04f",
   "metadata": {},
   "source": [
    "We use the `compile` method in Keras to configure the training process.\n",
    "This requires two final components: the **training loss** and the **optimisation algorithm**.\n",
    "\n",
    "For classification, the standard choice is **cross-entropy loss**.\n",
    "As discussed, our model outputs a probability score for each species.\n",
    "The cross-entropy loss measures the difference between these predicted probabilities and the ideal case, where the correct species has a score of 1 and all others are 0.\n",
    "\n",
    "For the optimisation, we will use the **Adam** optimiser.\n",
    "This is an special version of the **Stochastic Gradient Descent (SGD)** covered in the lectures.\n",
    "It calculates the loss for each batch, determines the direction of the steepest descent, and updates the model parameters accordingly.\n",
    "The size of these updates is controlled by the **learning rate**, which we have set to 1e-3 (0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73feba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate for the optimiser\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Configure the model for training\n",
    "# We also track 'accuracy' to monitor how many images the model classifies correctly\n",
    "model_0.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cde9c",
   "metadata": {},
   "source": [
    "We have also included `metrics` in the configuration.\n",
    "These are additional values, such as accuracy, calculated during training to help us monitor performance.\n",
    "Unlike the loss function, metrics do not influence the gradient descent process itself.\n",
    "\n",
    "Finally, we initiate the training process using the fit method.\n",
    "We first create a data loader to feed our 8√ó8 images into the model in batches.\n",
    "We will train the model for 20 **epochs**, which means the model will iterate through the entire dataset 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for our low-resolution dataset\n",
    "train_loader_0 = DataLoader(train_dataset_0, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "# Start the training process\n",
    "# The 'history' object will store the loss and accuracy values for each epoch\n",
    "history = model_0.fit(train_loader_0, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa788b1",
   "metadata": {},
   "source": [
    "We can now plot how the training loss and accuracy evolved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "ax1.plot(history.history[\"loss\"])\n",
    "ax1.set_title(\"Training loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "\n",
    "ax2.plot(history.history[\"acc\"])\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9f85d",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Hyperparameter choice*\n",
    "\n",
    "In this training run, we achieved a relatively low accuracy score on the training set.\n",
    "\n",
    "We used a `learning_rate=1e-3`, `batch_size=8`, and `epochs=20`.\n",
    "These are configurations of the model or the learning process that are not learned by the model itself, unlike its weights and biases.\n",
    "Because they are set before training begins, they are called **hyperparameters**.\n",
    "\n",
    "Choosing the right values can impact how well a model learns.\n",
    "Researchers often perform **hyperparameter tuning**, searching for the best combination through trial and error or structured methods like a Grid Search.\n",
    "\n",
    "Pick one of the hyperparameters mentioned above and change its value.\n",
    "Can you achieve a better accuracy score?\n",
    "Try at most three different configurations; hyperparameter tuning can be a time-consuming process and does not always guarantee a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ffc5aa",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution below.\n",
    "Note that there is no right answer in this question, as the idea is to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371c7d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# To make testing easier and avoid errors from copy-pasting, we can wrap the training process in a function. \n",
    "# This allows us to quickly swap values and compare results.\n",
    "\n",
    "# This function wraps the entire experiment‚Äîbuilding the model, creating the\n",
    "# data loader, and training. It accepts hyperparameter values\n",
    "# as arguments, which default to our baseline settings if not specified\n",
    "def test_hyperparameter_settings(\n",
    "    learning_rate = 1e-3,\n",
    "    batch_size = 8,\n",
    "    epochs = 20\n",
    "):\n",
    "    # Build the model\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=[3, 8, 8]),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the data loader with the chosen batch size\n",
    "    train_loader = DataLoader(train_dataset_0, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Configure the model with the chosen learning rate\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train and return the history for plotting\n",
    "    history = model.fit(train_loader, epochs=epochs)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b27dcb",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Baseline (learning_rate=1e-3, batch_size=8, epochs=20)\n",
    "history_baseline = test_hyperparameter_settings()\n",
    "\n",
    "# Notes: The model achieves roughly 38% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f34fa6",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Test 1: higher learning rate (1e-2)\n",
    "history_01 = test_hyperparameter_settings(learning_rate=1e-2)\n",
    "\n",
    "# Notes: Accuracy drops to around 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1863c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Trial 2: larger batch size (32)\n",
    "Let's try using a larger batch size\n",
    "history_02 = test_hyperparameter_settings(batch_size=32)\n",
    "\n",
    "# Notes: Performance remains similar to the baseline (37%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc75a4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Trial 3: more epochs (40)\n",
    "history_03 = test_hyperparameter_settings(epochs=40)\n",
    "\n",
    "# Notes: Accuracy increases to nearly 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c963d5",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "So far, we have only measured the training loss and accuracy.\n",
    "However, these metrics can be misleading when it comes to **generalisation**.\n",
    "Since the model has been explicitly trained to provide the correct answers for the training set, evaluating it with those same examples is a biased measure of its true ability.\n",
    "\n",
    "The moth dataset includes a **validation set** and a **test set** to help us assess performance more objectively.\n",
    "\n",
    "Recall from the lectures that the **validation set** is used during hyperparameter tuning.\n",
    "It is a subset of data that the model never sees during training, providing a better estimate of how it performs on new data.\n",
    "However, because we make decisions on how to improve the model based on this specific subset, using it for our final evaluation would still lead to overoptimistic results.\n",
    "\n",
    "For the final, unbiased assessment of our model, we use the **test set**.\n",
    "This data is kept completely separate until the very end of our development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab562666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create datasets pointing to the folders containing the validation and test images.\n",
    "# Note that we are using the same transform.\n",
    "val_dataset_0 = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform_0)\n",
    "\n",
    "test_dataset_0 = ImageFolder(os.path.join(download_dir, \"test\"), transform=transform_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67747b",
   "metadata": {},
   "source": [
    "We can use the `evaluate` method to calculate the loss and performance metrics across our separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc640c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specific loaders for the validation and test sets\n",
    "val_loader_0 = DataLoader(val_dataset_0, batch_size=8, shuffle=False)\n",
    "test_loader_0 = DataLoader(test_dataset_0, batch_size=8, shuffle=False)\n",
    "\n",
    "# Evaluate the model on both sets\n",
    "val_loss, val_accuracy = model_0.evaluate(val_loader_0)\n",
    "test_loss, test_accuracy = model_0.evaluate(test_loader_0)\n",
    "\n",
    "print(f\"Loss: validation={val_loss}  test={test_loss}\")\n",
    "print(f\"Accuracy: validation={val_accuracy}  test={test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4dbfec",
   "metadata": {},
   "source": [
    "You can also monitor the validation performance during the training process by passing the `validation_data` argument to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e74c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for another 20 epochs while monitoring validation performance\n",
    "history_1 = model_0.fit(train_loader_0, epochs=20, validation_data=val_loader_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850aa7a3",
   "metadata": {},
   "source": [
    "Visualising these curves helps us understand how the model is learning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to reuse throughout\n",
    "def plot_train_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "    ax1.plot(history_1.history[\"loss\"], label=\"train\")\n",
    "    ax1.plot(history_1.history[\"val_loss\"], label=\"val\")\n",
    "    ax1.set_title(\"Training loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history_1.history[\"acc\"], label=\"train\")\n",
    "    ax2.plot(history_1.history[\"val_acc\"], label=\"val\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_train_history(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe9ac6",
   "metadata": {},
   "source": [
    "Notice that we used the same model_0 as before.\n",
    "Unless you re-ran the cell where the model was initially defined, this second round of training resumed exactly where the previous one finished.\n",
    "\n",
    "Observe the gap between the two lines: the training loss continues to decrease as the model \"memorises\" the training images, but the validation loss may stay stagnant or even begin to rise.\n",
    "This is a clear sign of **overfitting**, where the model is no longer learning general features of moths, but rather specific details unique to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b20d02",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Start from scratch*\n",
    "\n",
    "Create a new model called `model_01` and train it from scratch for 40 epochs.\n",
    "Monitor both the validation loss and accuracy throughout the process.\n",
    "Once finished, plot the training curves to get a complete view of how the model evolved.\n",
    "\n",
    "*Optional*: Research about early stopping and how to implement it in keras.\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4389be",
   "metadata": {
    "lines_to_next_cell": 2,
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5931eb4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# We create a fresh model instance to ensure we start with random parameters, rather than continuing from a previous training run.\n",
    "model_2 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 8, 8]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure loss, optimiser, and metrics\n",
    "model_2.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train for 40 epochs and include validation data to monitor overfitting\n",
    "history_2 = model_2.fit(train_loader_0, epochs=40, validation_data=val_loader_0)\n",
    "\n",
    "# Plot the full training history\n",
    "plot_train_history(history_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ddd56",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Early stopping is a technique to halt training once a specific metric (like validation loss or accuracy) stops improving.\n",
    "Since validation data isn't used for training, it is our best indicator of how well the model generalises.\n",
    "If validation performance plateaus or worsens, continuing to train will likely lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85961f99",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Start from scratch again\n",
    "model_3 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 8, 8]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set the training loss/optimiser/metrics\n",
    "model_3.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create the early stopping callback\n",
    "# monitor='val_acc': watch the validation accuracy\n",
    "# patience=3: stop if there is no improvement for 3 consecutive epochs\n",
    "# mode=\"max\": we want the accuracy to increase\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_all', mode=\"max\", patience=3)\n",
    "\n",
    "# Train for 40 epochs with early stopping\n",
    "history_3 = model_3.fit(train_loader_0, epochs=40, validation_data=val_loader_0, callbacks=[early_stopping])\n",
    "\n",
    "# Plot the full training history\n",
    "plot_train_history(history_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec818c2",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ If your model trained for the full 40 epochs, it means your validation accuracy kept improving (or at least didn't stay flat long enough to trigger the stop).\n",
    "\n",
    "*Note:* Interestingly, some researchers have observed a phenomenon called *grokking*, where a model that appears to be overfitting suddenly changes phase and begins generalising again after much longer training.\n",
    "You can read more about it on [Wikipedia](https://en.wikipedia.org/wiki/Grokking_(machine_learning))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69e033",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Full evaluation*\n",
    "\n",
    "Accuracy provides a good overall summary, but it can hide specific weaknesses in a model.\n",
    "For example, a model might be very good at identifying common moth species but consistently confuse two similar-looking ones.\n",
    "To investigate this, we use a confusion matrix.\n",
    "\n",
    "While Keras has many built-in features, its native metrics for detailed error analysis are limited.\n",
    "We can bridge this gap by using `scikit-learn`.\n",
    "First, we need to extract the raw prediction scores (the confidence for each class) and the true labels from our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb79592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(model, dataset):\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "\n",
    "    for im_batch, label_batch in loader:\n",
    "        outputs = model(im_batch)\n",
    "\n",
    "        y_score.extend(outputs.detach().cpu().numpy())\n",
    "        y_true.extend(label_batch.detach().cpu().numpy())\n",
    "\n",
    "    return np.array(y_score), np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadbfdf",
   "metadata": {},
   "source": [
    "Use the `extract_scores` function with your preferred model and the test dataset to retrieve the raw confidence scores and true labels.\n",
    "Then use the argmax function to get the most confident score for each image `y_pred = y_score.argmax(axis=1)` Finally, use `scikit-learn` to generate the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea7598",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c67925",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Use the function to get raw scores and true labels from the test set\n",
    "# We use our trained model and the matching test dataset\n",
    "y_score, y_true = extract_scores(model_0, test_dataset_0)\n",
    "\n",
    "# Get the index of the highest score for each image (the predicted species)\n",
    "y_pred = y_score.argmax(axis=1)\n",
    "\n",
    "# Create and plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    display_labels=train_dataset_0.classes,\n",
    "    xticks_rotation='vertical',\n",
    "    ax=ax,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99559f",
   "metadata": {},
   "source": [
    "## Better model design\n",
    "\n",
    "One strategy to improve performance is to use architectures specifically suited to your data type.\n",
    "For images, we use **Convolutional Neural Networks** (CNNs).\n",
    "\n",
    "While we will cover the technical details later in the module, the main advantage of a CNN is its ability to recognize patterns (like edges or textures) regardless of where they appear in an image.\n",
    "This makes them far more efficient for visual tasks than the Dense models we have used so far.\n",
    "\n",
    "Below is a simple convolutional architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 64\n",
    "\n",
    "model_cnn = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41726430",
   "metadata": {},
   "source": [
    "If you look at the summary, you will notice that this model has fewer trainable parameters than a Dense model, even when handling larger images (`resolution = 64`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830e270",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*CNN sizes*\n",
    "\n",
    "Try out different resolution sizes (for example: 32, 64, 128).\n",
    "Does the number of parameters change in the same way as it did for the previous Dense model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973c1ac",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad261b8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for resolution in [32, 64, 128]:\n",
    "    model_cnn_0 = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.GlobalMaxPooling2D(),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(model_cnn_0.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc02d8",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Unlike the Dense model, the number of parameters in this CNN stays fixed (11794) regardless of the input resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44b461",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Train a CNN model*\n",
    "\n",
    "Train the model using the same settings as our earlier experiments: 8√ó8 images, cross-entropy loss, and the Adam optimiser.\n",
    "Ensure you monitor the validation loss throughout the process.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "- Do you observe a change in accuracy or loss?\n",
    "- What happens if you increase the resolution to 32√ó32 or even 64√ó64?\n",
    "- How does the training time change as you increase the resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989951af",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Since we want to compare different resolutions, we can again wrap the training process in a function. \n",
    "\n",
    "def train_cnn(resolution=8, epochs=20, learning_rate=1e-3, batch_size=8):\n",
    "    # Build a CNN model with the given input resolution\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            keras.layers.GlobalMaxPooling2D(),\n",
    "            keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a transform to resize images to the target resolution\n",
    "    transform = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.Resize([resolution, resolution]),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Configure the model with cross-entropy loss and the Adam optimiser\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Prepare datasets and loaders with the new transform\n",
    "    train_dataset = ImageFolder(train_dataset_path, transform=transform)\n",
    "    val_dataset = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Train the model and track validation metrics\n",
    "    history_cnn = model.fit(train_loader, epochs=epochs, validation_data=val_loader)\n",
    "\n",
    "    return history_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ceeb6b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Baseline: 8x8 resolution\n",
    "history_cnn_0 = train_cnn(resolution=8)\n",
    "\n",
    "plot_train_history(history_cnn_0)\n",
    "\n",
    "# Notes: Validation accuracy is around 47%.\n",
    "# This is a slight improvement over the Dense model (~43%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e3fd4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Higher resolution: 32x32\n",
    "history_cnn_1 = train_cnn(resolution=32)\n",
    "\n",
    "plot_train_history(history_cnn_1)\n",
    "\n",
    "# Notes: Validation accuracy jumps to ~54%.\n",
    "# Even though the image is 16x larger (pixels), training speed remains similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657ca13",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Even higher resolution: 64x64\n",
    "history_cnn_2 = train_cnn(resolution=64)\n",
    "\n",
    "plot_train_history(history_cnn_2)\n",
    "\n",
    "# Notes: Validation accuracy reaches ~59%.\n",
    "# The training time per step only increases slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723f5b0",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ The CNN model trained on 8x8 images performed marginally better than the Dense model.\n",
    "After 20 epochs, the CNN reached a validation accuracy of approximately 47%, compared to 42% for the Dense model.\n",
    "Since these metrics vary between training runs, multiple experiments would be needed for a more robust comparison.\n",
    "\n",
    "Model performance improved significantly as the image resolution increased, rising from 47% (8x8) to 54% (32x32) and reaching 59% at 64x64.\n",
    "The training speed did not change substantially across these resolutions.\n",
    "\n",
    "*note*: Actual training speed depends on your hardware.\n",
    "Larger images may increase training time depending on GPU memory, while CPU and disk drive speeds can affect how quickly data is loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13166ee1",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "So far, our models have struggled with overfitting.\n",
    "You may have noticed the training accuracy climbing while the validation accuracy remains low‚Äîa clear sign that the model is simply \"memorising\" the specific images in our training set rather than learning general features of moths.\n",
    "\n",
    "To improve generalisation, we need more data.\n",
    "However, in ecology, collecting thousands of additional samples is often not feasible.\n",
    "This is where **data augmentation** becomes a great strategy.\n",
    "\n",
    "Augmentation artificially expands our dataset by creating slightly modified versions of our existing images.\n",
    "By applying random transformations (e.g. cropping, flipping, or adjusting colours) we force the model to focus on the essential features of the moth (its shape and wing patterns) rather than irrelevant details like the exact position in the frame or the specific lighting conditions of the photo.\n",
    "\n",
    "We can use the `torchvision` library to define a range of random transformations.\n",
    "These will be applied \"on the fly\" during training, meaning the model sees a slightly different version of the image in every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of random augmentations\n",
    "augmentations = v2.Compose([\n",
    "    # Randomly crop a portion of the image and resize it back to 128x128\n",
    "    v2.RandomResizedCrop([128, 128]),\n",
    "\n",
    "    # Randomly flip the image horizontally\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # Randomly convert the image to grayscale 10% of the time\n",
    "    v2.RandomGrayscale(p=0.1),\n",
    "\n",
    "    # Randomly adjust brightness, contrast, and saturation\n",
    "    v2.ColorJitter(),\n",
    "])\n",
    "\n",
    "# Visualise the effect of these augmentations on a single image\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for ax in axes.flatten():\n",
    "    # Applying the same augmentation pipeline generates a unique result each time\n",
    "    im_aug = augmentations(im_tensor)\n",
    "    ax.imshow(to_pil(im_aug))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130255a",
   "metadata": {},
   "source": [
    "Now we can integrate these augmentations into our training pipeline.\n",
    "It is important to note that we only apply augmentations to the training set.\n",
    "The validation and test sets should remain unmodified (except for basic resizing) so they provide a reliable, \"real-world\" measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_128 = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize([128, 128]),\n",
    "        v2.ToDtype(torch.float32, scale=True), # scale=True ensures values are 0-1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We can also compose two complex transformations\n",
    "transform_aug = v2.Compose([\n",
    "    transform_128,\n",
    "    augmentations,\n",
    "])\n",
    "\n",
    "# Use the new augmentation transform\n",
    "train_dataset_aug = ImageFolder(train_dataset_path, transform=transform_aug)\n",
    "\n",
    "# Create val dataset. Note we are not using augmentations here\n",
    "val_dataset_aug = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform_128)\n",
    "\n",
    "model_cnn_128 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 128, 128]),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn_128.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487fc2a",
   "metadata": {},
   "source": [
    "Finally, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957dba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=8, num_workers=2, shuffle=True)\n",
    "val_loader_aug = DataLoader(val_dataset_aug, batch_size=8, num_workers=2, shuffle=False)\n",
    "history_aug = model_cnn_128.fit(train_loader_aug, epochs=20, validation_data=val_loader_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d2117",
   "metadata": {},
   "source": [
    "With augmentations enabled, the training loss decreases more slowly, as the task has become harder.\n",
    "However, the gap between training and validation performance should ideally narrow, indicating better generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(history_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a465d",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Improve the model performance*\n",
    "\n",
    "Experiment with the settings we have covered to achieve the best possible performance on the test set.\n",
    "You can try adjusting: architecture, hyperparameters, transforms.\n",
    "As an additional strategy to prevent overfitting, research Dropout.\n",
    "Make sure you leave at least 20 minutes for the next section of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccd6a1",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ See solution below.\n",
    "Again there is no right answer, but more of a change to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172c41a",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# We increase resolution and epochs to give the model more detail and time to learn.\n",
    "resolution = 128\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 60\n",
    "\n",
    "# Model definition\n",
    "# We add an extra Convolutional and Pooling layer to capture more complex features.\n",
    "model_final = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_final.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Data Pre-processing and Augmentation\n",
    "# Basic processing to resize and scale images.\n",
    "transform_final = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize([resolution, resolution]),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Augmentations help prevent overfitting\n",
    "augmentations = v2.Compose([\n",
    "    v2.RandomResizedCrop([resolution, resolution]),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomGrayscale(p=0.1),\n",
    "    v2.ColorJitter(),\n",
    "])\n",
    "\n",
    "# Combine basic transforms with augmentations for the training set.\n",
    "transform_final_aug = v2.Compose([\n",
    "    transform_final,\n",
    "    augmentations,\n",
    "])\n",
    "\n",
    "# Prepare Datasets and Loaders\n",
    "# Note: Augmentation is only applied to the training set. \n",
    "# Validation data should stay representative of real-world images.\n",
    "train_dataset_final = ImageFolder(train_dataset_path, transform=transform_final_aug)\n",
    "val_dataset_final = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform_final)\n",
    "\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "val_loader_final = DataLoader(val_dataset_final, batch_size=8)\n",
    "\n",
    "# Train the model\n",
    "history_final = model_final.fit(train_loader_final, epochs=epochs, validation_data=val_loader_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e853112",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "The final strategy we will explore is **transfer learning**.\n",
    "As discussed in the lectures, this involves reusing a model that has already been trained on a massive, general dataset.\n",
    "The idea is that the model has already learned to recognise fundamental visual features‚Äîsuch as edges, textures, and shapes‚Äîwhich are \"transferable\" to our specific task of identifying moth species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defeed4a",
   "metadata": {},
   "source": [
    "### Model loading\n",
    "\n",
    "We will use the `timm` library (PyTorch Image Models), which provides a wide range of pre-trained architectures.\n",
    "We have selected `efficientnet_b0`, a model known for being highly efficient and fast while maintaining strong performance.\n",
    "This model was originally trained on ImageNet, a famous dataset containing millions of images across a thousand different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7dde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models(pretrained=True)\n",
    "\n",
    "efficientnet = timm.create_model(\"efficientnet_b0\", pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b7717",
   "metadata": {},
   "source": [
    "Every pre-trained model has specific requirements for its input images (such as specific normalisation values).\n",
    "It is essential to use the same preprocessing pipeline that the model was originally trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = timm.data.create_transform(\n",
    "    **timm.data.resolve_data_config(efficientnet.pretrained_cfg)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550f05c",
   "metadata": {},
   "source": [
    "### Feature embeddings\n",
    "\n",
    "Instead of training a deep network from scratch, we will use EfficientNet as a \"feature extractor\".\n",
    "We pass our moth images through the model and stop just before the final classification layer.\n",
    "The outputs at this stage are called **feature embeddings**.\n",
    "\n",
    "Embeddings are high-dimensional numerical representations of an image.\n",
    "Because the model was trained on millions of images, these embeddings are quite rich and descriptive.\n",
    "\n",
    "We can then use these embeddings as inputs for a much simpler model, such as a Logistic Regression classifier.\n",
    "This is exactly the same approach we used in the machine learning session.\n",
    "\n",
    "First, we define a function to extract these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_efficientnet_features(dataset, batch_size=8):\n",
    "    loader = DataLoader(dataset, batch_size=8, num_workers=2)\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for im_batch, label_batch in tqdm(loader):\n",
    "        # Extract features from the model\n",
    "        feats = efficientnet.forward_features(im_batch)\n",
    "\n",
    "        # Global average pooling to convert spatial features into a 1D vector\n",
    "        feats = feats.mean(axis=(2, 3))\n",
    "\n",
    "        # Convert tensors to numpy arrays\n",
    "        features.extend(feats.detach().numpy())\n",
    "        targets.extend(label_batch.detach().numpy())\n",
    "\n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf453b",
   "metadata": {},
   "source": [
    "Now, we apply this to our moth datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for the training set\n",
    "train_dataset = ImageFolder(os.path.join(download_dir, \"train\"), transform=transform)\n",
    "X_train, y_train = extract_efficientnet_features(train_dataset)\n",
    "\n",
    "# Extract features for the test set\n",
    "test_dataset = ImageFolder(os.path.join(download_dir, \"test\"), transform=transform)\n",
    "X_test, y_test = extract_efficientnet_features(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920041ff",
   "metadata": {},
   "source": [
    "With our features ready, we can train a Logistic Regression model in seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aba229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the classifier on the extracted embeddings\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d09a1",
   "metadata": {},
   "source": [
    "And evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"Transfer Learning Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb7279",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Few-shot transfer learning*\n",
    "\n",
    "Transfer learning is particularly interesting when you have very little data.\n",
    "In ecology, obtaining thousands of labelled images is often impossible.\n",
    "This is the idea of **few-shot learning**, training a model with only a handful of examples.\n",
    "\n",
    "How well can we perform with only 1, 5, or 10 examples per species?\n",
    "Since we have already extracted the embeddings for the entire dataset, we can quickly test this by selecting small subsets.\n",
    "\n",
    "Use the function below to run an experiment.\n",
    "For each scenario (1, 5, and 10 examples per species), train the model 5 times (using different random seeds) and plot the average accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(X, y, n=5, seed=None):\n",
    "    series = pd.DataFrame({\"label\": y})\n",
    "    selection = series.groupby(\"label\").sample(n=n, random_state=seed)\n",
    "    X_subset = X[selection.index]\n",
    "    y_subset = y[selection.index]\n",
    "    return X_subset, y_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef548792",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ Solution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24df2c7",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# # Define different random seeds for repeated trials\n",
    "seeds = [66702218, 90612235, 39795356, 11573231, 34588301]\n",
    "scenario = [1, 5, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "for num in scenario:\n",
    "    for seed in seeds:\n",
    "        print(f\"training and evaluating {num}-shot scenario, seed: {seed}\")\n",
    "\n",
    "        # Select a small subset of the training data\n",
    "        X_subset, y_subset = select_subset(X_train, y_train, n=num, seed=seed)\n",
    "\n",
    "        # Train a simple Logistic Regression on the embeddings\n",
    "        lr_model = LogisticRegression()\n",
    "        lr_model.fit(X_subset, y_subset)\n",
    "\n",
    "        # Evaluate accuracy on the full test set\n",
    "        y_pred = lr_model.predict(X_test)\n",
    "        score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "        # Store the results for analysis\n",
    "        results.append({\n",
    "            \"seed\": seed,\n",
    "            \"examples\": num,\n",
    "            \"accuracy\": score,\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for analysis and plotting\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "results\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualise the spread of accuracy scores using a boxplot\n",
    "sns.boxplot(data=results, x=\"examples\", y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c66c60",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate the average and standard deviation for each scenario\n",
    "results.groupby(\"examples\")[\"accuracy\"].mean()\n",
    "results.groupby(\"examples\")[\"accuracy\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461791ca",
   "metadata": {
    "region_name": "md",
    "tags": [
     "solution"
    ]
   },
   "source": [
    "‚úÖ As expected there is a strong dependence of performance on size of trianing data.\n",
    "When trained with 1 example the model reaches 52% accuracy, however already with 5 examples per species we get 84% accuracy and with 10 we get 91%."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
