{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a427b92",
   "metadata": {},
   "source": [
    "# Week 3 Deep Learning\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "This week, we will:\n",
    "\n",
    "- Build and train deep learning models.\n",
    "- Diagnose and fix common training issues.\n",
    "- Implement transfer learning techniques.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd68ea2f",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Building deep learning models is a complex task.\n",
    "It involves designing architectures with sometimes millions or billions of parameters and optimising them using gradient descent‚Äîthe process by which training loss is reduced and model parameters are adjusted.\n",
    "These operations must also be computationally efficient to minimise training and inference time.\n",
    "Fortunately, several libraries provide high-level tools to handle these complexities, making it easier to create and train models effectively.\n",
    "\n",
    "The most popular deep learning frameworks are `PyTorch`, `TensorFlow`, and `JAX`.\n",
    "While these are open-source, they are primarily developed by Meta (`PyTorch`) and Google (`TensorFlow` and `JAX`).\n",
    "Although there are specialised reasons to choose one over another, either is suitable for most problems; the choice usually depends on convenience and personal preference.\n",
    "`PyTorch` is often preferred in academic circles because it is intuitive and flexible, making it ideal for quick experimentation.\n",
    "\n",
    "![pytorch](https://pytorch.org/wp-content/uploads/2025/01/pytorch_seo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch library\n",
    "from astropy.visualization import hist\n",
    "import torch\n",
    "\n",
    "# We can use it to check if we can use the GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1153e1",
   "metadata": {},
   "source": [
    "In this notebook, however, we will use `Keras`.\n",
    "It is designed as a high-level wrapper, making it more user-friendly than those underlying frameworks.\n",
    "While `Keras` runs on top of these libraries, it hides their complexity from the user.\n",
    "If you eventually need to develop highly custom features or examine internal mechanics, you would likely work directly with a framework like `PyTorch` or `TensorFlow`, but for well-established workflows, `Keras` makes the process much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We need to specify that we want to use torch\n",
    "# as the \"backend\" for keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8e36e",
   "metadata": {},
   "source": [
    "We will also use a few additional libraries to support our workflow.\n",
    "Since we will be working with images, we use `Pillow` (PIL) for image manipulation and torchvision to bridge the gap between `Pillow` and `PyTorch`.\n",
    "Finally, we will use `timm`, a package that provides access to a large set of pre-trained image models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97318f78",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c2337",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "For this notebook, we will use an example dataset of moth images as a case study.\n",
    "\n",
    "![moths](https://storage.googleapis.com/kaggle-datasets-images/2439824/4128992/4527186ab6ea46da0adb9bbce34e8d81/dataset-cover.jpg?t=2022-08-27-19-01-29)\n",
    "\n",
    "This dataset was shared via [kaggle](https://www.kaggle.com/).\n",
    "Kaggle is a great resource for machine learning, serving as a platform that hosts datasets, models, and more.\n",
    "It also hosts competitions where participants attempt to train the best model for a specific task.\n",
    "\n",
    "This particular [dataset](https://www.kaggle.com/datasets/gpiosenka/moths-image-datasetclassification?select=MOTHS.csv) was assembled by Kaggle user Gerry through internet searches for various moth species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57909f86",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "Let's download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f19361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle offers a library to interface with their platforms\n",
    "import kagglehub\n",
    "\n",
    "# We can download the dataset. The returned object is the path to where it was downloaded\n",
    "download_dir = kagglehub.dataset_download(\"gpiosenka/moths-image-datasetclassification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57a5b7",
   "metadata": {},
   "source": [
    "The dataset includes a CSV file containing metadata for each image.\n",
    "This includes the location of the file within the download folder (`filepaths`), the species name (`labels`), and a pre-defined split for training, testing, and validation (`data set`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5331aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "path_to_table = os.path.join(download_dir, \"MOTHS.csv\")\n",
    "\n",
    "# Load the file into a DataFrame\n",
    "df = pd.read_csv(path_to_table)\n",
    "\n",
    "# Check its first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72aece",
   "metadata": {
    "region_name": "md",
    "tags": [
     "note"
    ]
   },
   "source": [
    "üí°**Note:** If you are unfamiliar with `os.path.join`, here is a brief explanation.\n",
    "If you have a file named `my_file.txt` in a folder called `my_folder`, the path to that file on Windows is `my_folder\\my_file.txt`, whereas on macOS and Linux it is `my_folder/my_file.txt`.\n",
    "This difference (`/` vs `\\`) means code shared across different operating systems might break.\n",
    "The `os.path.join` function handles these differences automatically, making your code platform-independent.\n",
    "Using it is considered best practice for writing shareable code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139abec",
   "metadata": {},
   "source": [
    "### Dataset summary\n",
    "\n",
    "We can now count how many images are available for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c73bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of images per species and sort by frequency\n",
    "counts = df.labels.value_counts().sort_values(ascending=False)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd41ad",
   "metadata": {},
   "source": [
    "Plotting these counts makes the distribution easier to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4602a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "# Plot the number of counts\n",
    "ax.plot(range(len(counts)), counts)\n",
    "\n",
    "# Make the y axis range start from 0\n",
    "ax.set_ylim(0, counts.max() + 10)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel(\"Number of images\")\n",
    "ax.set_xlabel(\"Species rank\")\n",
    "ax.set_title(\"Counts of images per species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67416408",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Full dataset summary*\n",
    "\n",
    "When publishing a model, it is good practice to provide a detailed breakdown of your dataset.\n",
    "This includes the number of examples per species across the training, validation, and test sets.\n",
    "Create a table with one row for each species and three columns showing the image counts for that species in each of the three splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf9356",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "We will now use Pillow (`PIL`) to load and view some of the moth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# select a random example\n",
    "example = df.sample(n=1).iloc[0]\n",
    "\n",
    "# And display it\n",
    "im = Image.open(os.path.join(download_dir, example.filepaths))\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.title(example.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e985d",
   "metadata": {},
   "source": [
    "Each time you run this cell, a different image is selected at random from the dataset.\n",
    "Try running the cell several times to see the variety of species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27db45",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Qualitative dataset description*\n",
    "\n",
    "Suppose you wanted to develop a moth identifier for biologists to use on mobile devices in the field.\n",
    "After looking at several images, list two reasons why training with this dataset might be unsuitable for this task, and two reasons why it might be suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405fa793",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "An essential step in deep learning is preparing your data for the model.\n",
    "This involves converting raw data into a numerical representation (e.g. turning an image into an array of pixel values) and applying transformations like cropping or scaling values to a 0‚Äì1 range.\n",
    "\n",
    "These steps are known as **preprocessing**.\n",
    "Getting this stage right is very important, as the quality of preprocessing can significantly impact a model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c38cf",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "The first step is converting images into a numerical format.\n",
    "\n",
    "In previous notebooks, we used `numpy` to store and process arrays of data.\n",
    "`torch` uses its own version of a numerical array called a **tensor**.\n",
    "In most situations, a tensor behaves similarly to a `numpy` array, making it easy to apply what you already know.\n",
    "However, they are not identical, and we will highlight the key differences as we progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the transforms provided by torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Create a transformation object that converts PIL images to an image tensor\n",
    "to_tensor = v2.ToImage()\n",
    "\n",
    "# Apply the transform to our example image\n",
    "im_tensor = to_tensor(im)\n",
    "\n",
    "im_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22bc15",
   "metadata": {},
   "source": [
    "We can also convert tensors back to images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transform that converts image tensors back to PIL images\n",
    "to_pil = v2.ToPILImage()\n",
    "\n",
    "# Apply the transform to the image tensor\n",
    "reconstructed_im = to_pil(im_tensor)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(reconstructed_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372eddd4",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "`torchvision` provides a variety of transforms for manipulating images.\n",
    "For example, we can use these tools to resize images and reduce their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adfc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation that will resize any image into an 32x32 image\n",
    "resize = v2.Resize([32, 32])\n",
    "\n",
    "# Apply the transform to the image *tensor*\n",
    "resized = resize(im_tensor)\n",
    "\n",
    "# Plot the original and resized images side-by-side for comparison\n",
    "_, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.imshow(im)\n",
    "ax2.imshow(to_pil(resized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675559f2",
   "metadata": {},
   "source": [
    "Multiple transformations can be combined into a single pipeline.\n",
    "For example, we can resize an image and convert it to grayscale in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb203fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define a pipeline of multiple transformations\n",
    "compose_transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),  # convert to image tensor\n",
    "        v2.Resize([32, 32]),  # resize 32x32\n",
    "        v2.Grayscale(),  # make grayscale\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the combined transformations to the original PIL image\n",
    "compose_tensor = compose_transform(im)\n",
    "\n",
    "# Plot the original and processed results side-by-side\n",
    "_, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.imshow(im)\n",
    "ax2.imshow(to_pil(compose_tensor), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8971eb6",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Why rescaling?*\n",
    "\n",
    "The original images in this dataset are 224x224 pixels.\n",
    "Reducing the resolution makes each image smaller by reducing the total number of pixels, but it also results in a loss of detail.\n",
    "When developing a model, what are some considerations that might guide your decision on which image size to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f48b1c",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "For training and evaluation, we use a **dataset**‚Äîa collection of examples, which in our case are images of moths.\n",
    "We need a way to load these images and iterate through them during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda2262",
   "metadata": {},
   "source": [
    "In this dataset, images are organised into `train`, `valid`, and `test` folders.\n",
    "Within each of these, there are subfolders named after each species containing the corresponding images.\n",
    "Like so:\n",
    "\n",
    "```\n",
    "download_dir\n",
    "‚îú‚îÄ‚îÄ test\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "‚îú‚îÄ‚îÄ train\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n",
    "‚îÇ¬†¬† ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "‚îî‚îÄ‚îÄ valid\n",
    "    ‚îú‚îÄ‚îÄ ARCIGERA FLOWER MOTH\n",
    "    ‚îú‚îÄ‚îÄ ...\n",
    "    ‚îî‚îÄ‚îÄ WHITE SPOTTED SABLE MOTH\n",
    "\n",
    "```\n",
    "\n",
    "This is a standard structure for classification tasks.\n",
    "We can use the `ImageFolder` utility to load this data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc9d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Define the path to the training directory\n",
    "train_dataset_path = os.path.join(download_dir, \"train\")\n",
    "\n",
    "# Create a transform for the dataset\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),  # convert to image tensor\n",
    "        v2.Resize([32, 32]),  # resize 32x32\n",
    "        v2.ToDtype(torch.float32, scale=True),  # convert to floating numbers\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a dataset from the folder structure\n",
    "# Subfolder names are automatically used as the labels for the images within them\n",
    "# Setting `transform=transform` ensures every image is pre-processed automatically when loaded.\n",
    "train_dataset = ImageFolder(train_dataset_path, transform=transform)\n",
    "\n",
    "# The length of the dataset represents the total number of examples\n",
    "print(f\"Num of examples: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087f74b",
   "metadata": {},
   "source": [
    "You can access individual items from the dataset as you would with a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first element in the dataset\n",
    "# Notice it contains both the image tensor and the label (species)\n",
    "im, label = train_dataset[0]\n",
    "\n",
    "print(f\"Image label: {label}\")\n",
    "\n",
    "plt.imshow(to_pil(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ca625",
   "metadata": {},
   "source": [
    "You may have noticed that the image labels appear as numbers rather than species names.\n",
    "This is because models require numerical inputs to perform calculations.\n",
    "However, the dataset keeps track of the original class names and the mapping between these integers and the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a247446",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Print the list of all species names detected in the folder structure\n",
    "print(train_dataset.classes)\n",
    "\n",
    "# We'll store the total number of classes for later use\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Retrieve the original species name from an integer label\n",
    "class_name = train_dataset.classes[label]\n",
    "print(f\"Label to species: {label} -> {class_name}\")\n",
    "\n",
    "# Find the integer label associated with a specific species name\n",
    "species = \"REGAL MOTH\"\n",
    "label = train_dataset.class_to_idx[species]\n",
    "print(f\"Species to label: {species} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1861519",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The final step is the actual process of loading the images.\n",
    "When iterating over a dataset for training or evaluation, we typically process data in **batches**.\n",
    "Loading files from a disk can be slow and is often the primary bottleneck in the training process.\n",
    "To address this, `torch` provides the `DataLoader` utility, which makes data retrieval more efficient by handling batching and loading in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b901deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader to handle batching and shuffling\n",
    "# 'batch_size' determines how many images are processed at once\n",
    "# 'shuffle=True' ensures batches are assembled randomly during training\n",
    "# 'num_workers=2' enables multi-processing, loading two images in parallel for efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "# A loader is an 'iterable', allowing you to loop through the entire dataset\n",
    "for im_batch, label_batch in train_loader:\n",
    "    # This is where you would typically pass the batches to your model\n",
    "\n",
    "    # Check the shape of the image batch\n",
    "    print(f\"Image batch shape: {im_batch.shape}\")\n",
    "\n",
    "    # We break the loop here\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc9f65",
   "metadata": {},
   "source": [
    "To help verify that our data is loading correctly, we can visualise an entire batch of images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc46466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_image_batch(batch, ncols=8, figsize=None):\n",
    "    # Calculate the number of rows required based on the batch size and columns\n",
    "    batch_size = len(batch)\n",
    "    nrows = int(np.ceil(batch_size / ncols))  # np.ceil rounds up to nearest integer\n",
    "\n",
    "    # Create a figure with a grid of sub-axes\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "\n",
    "    # Iterate over images in batch and axes in figure\n",
    "    for im, ax in zip(batch, axes.flatten()):\n",
    "        ax.imshow(to_pil(im))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Visualise the batch we just loaded\n",
    "plot_image_batch(im_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831bfe1",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Practice batching*\n",
    "\n",
    "Create a new dataset using the test directory.\n",
    "This time, keep the images at their **original resolution** but convert them to **grayscale**.\n",
    "Once the dataset and loader are ready, draw a **random** batch of **16 images** and plot them using the function above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0fec3",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "With our data loading pipeline in place, we are ready to build our first deep learning model.\n",
    "We will use `keras`, which provides high-level building blocks to create complex architectures and manage the training process.\n",
    "\n",
    "As discussed in the lectures, the fundamental building block is the fully connected layer, known in Keras as a **Dense** layer.\n",
    "In this layer, every neuron is connected to every neuron in the preceding layer.\n",
    "We also apply an **activation** function, such as **ReLU** (Rectified Linear Unit), to introduce non-linearity, allowing the model to learn complex patterns.\n",
    "\n",
    "Here is an example of a neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "keras.config.set_image_data_format(\"channels_first\")\n",
    "\n",
    "resolution = 32\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cbaa4",
   "metadata": {},
   "source": [
    "When defining the model architecture, there are several key components to understand:\n",
    "\n",
    "First, we specify the input shape.\n",
    "Our data consists of RGB images defined by their height and width.\n",
    "In this example, we use 32√ó32 pixels to match our downscaled data, though the model can be configured for any resolution.\n",
    "\n",
    "Then we have the **Flatten** layer.\n",
    "This converts the 3D structure of the image into a 1D list of features.\n",
    "In this basic model, we do not account for the spatial relationship between pixels; instead, every pixel value across all three colour channels is treated as an individual input neuron.\n",
    "\n",
    "Then we have two intermediate layers each with 64 neurons.\n",
    "Both of them use **ReLU** as the activation function.\n",
    "\n",
    "The final layer is a dense layer where the number of neurons matches the total number of moth species in our dataset.\n",
    "This layer uses the softmax activation function, which is key for classification.\n",
    "Softmax takes the raw outputs from the network and scales them so that:\n",
    "\n",
    "- Every output value is between 0 and 1.\n",
    "- The sum of all output values equals exactly 1.\n",
    "\n",
    "This allow us to interpret the output of each neuron as a probability.\n",
    "For example, a value of 0.75 on a specific neuron indicates a 75% confidence that the image belongs to that particular species.\n",
    "\n",
    "We can use the `summary` method to view a detailed breakdown of the model.\n",
    "This includes the output shape of each layer and the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a921c",
   "metadata": {},
   "source": [
    "We can also generate a visual diagram to see how the data flows through the different layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fd65b",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Dependance of model size on resolution*\n",
    "\n",
    "The model above is relatively simple, with only two hidden layers and a modest number of neurons, yet it already contains around 200,000 parameters.\n",
    "Try changing the input resolution using the values (8, 16, 32, 64, 128, 256) and record the number of trainable parameters for each case.\n",
    "Plot this relationship to see how the model size changes as the image resolution increases.\n",
    "What patterns do you observe in the plot?\n",
    "What does this suggest about the practical trade-offs involved when choosing an image resolution for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4224b",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We are now ready to train our model.\n",
    "As you observed in the previous exercise, using high-resolution images leads to a significantly larger model with more parameters.\n",
    "To allow for faster experimentation, we will start with a very low resolution of 8√ó8 pixels.\n",
    "At this size, it is difficult to identify specific moth features, but basic shapes and colours remain visible.\n",
    "\n",
    "First, we define the transformation and the model architecture for this 8√ó8 resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3469008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for 8x8 resolution\n",
    "transform_0 = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize([8, 8]),\n",
    "        v2.ToDtype(torch.float32, scale=True), # scale=True ensures values are 0-1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset_0 = ImageFolder(train_dataset_path, transform=transform_0)\n",
    "\n",
    "# Build a small model for fast experimentation\n",
    "model_0 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 8, 8]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0128f",
   "metadata": {},
   "source": [
    "We use the `compile` method in Keras to configure the training process.\n",
    "This requires two final components: the **training loss** and the **optimisation algorithm**.\n",
    "\n",
    "For classification, the standard choice is **cross-entropy loss**.\n",
    "As discussed, our model outputs a probability score for each species.\n",
    "The cross-entropy loss measures the difference between these predicted probabilities and the ideal case, where the correct species has a score of 1 and all others are 0.\n",
    "\n",
    "For the optimisation, we will use the **Adam** optimiser.\n",
    "This is an special version of the **Stochastic Gradient Descent (SGD)** covered in the lectures.\n",
    "It calculates the loss for each batch, determines the direction of the steepest descent, and updates the model parameters accordingly.\n",
    "The size of these updates is controlled by the **learning rate**, which we have set to 1e-3 (0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate for the optimiser\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Configure the model for training\n",
    "# We also track 'accuracy' to monitor how many images the model classifies correctly\n",
    "model_0.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478f28c",
   "metadata": {},
   "source": [
    "We have also included `metrics` in the configuration.\n",
    "These are additional values, such as accuracy, calculated during training to help us monitor performance.\n",
    "Unlike the loss function, metrics do not influence the gradient descent process itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869bffb",
   "metadata": {},
   "source": [
    "Finally, we initiate the training process using the fit method.\n",
    "We first create a data loader to feed our 8√ó8 images into the model in batches.\n",
    "We will train the model for 20 **epochs**, which means the model will iterate through the entire dataset 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25214ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for our low-resolution dataset\n",
    "train_loader_0 = DataLoader(train_dataset_0, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "# Start the training process\n",
    "# The 'history' object will store the loss and accuracy values for each epoch\n",
    "history = model_0.fit(train_loader_0, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b276c7",
   "metadata": {},
   "source": [
    "We can now plot how the training loss and accuracy evolved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "\n",
    "ax1.plot(history.history[\"loss\"])\n",
    "ax1.set_title(\"Training loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "\n",
    "ax2.plot(history.history[\"acc\"])\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c385200",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Hyperparameter choice*\n",
    "\n",
    "In this training run, we achieved a relatively low accuracy score on the training set.\n",
    "\n",
    "We used a `learning_rate=1e-3`, `batch_size=8`, and `epochs=20`.\n",
    "These are configurations of the model or the learning process that are not learned by the model itself, unlike its weights and biases.\n",
    "Because they are set before training begins, they are called **hyperparameters**.\n",
    "\n",
    "Choosing the right values can impact how well a model learns.\n",
    "Researchers often perform **hyperparameter tuning**, searching for the best combination through trial and error or structured methods like a Grid Search.\n",
    "\n",
    "Pick one of the hyperparameters mentioned above and change its value.\n",
    "Can you achieve a better accuracy score?\n",
    "Try at most three different configurations; hyperparameter tuning can be a time-consuming process and does not always guarantee a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e34fb3",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "So far, we have only measured the training loss and accuracy.\n",
    "However, these metrics can be misleading when it comes to **generalisation**.\n",
    "Since the model has been explicitly trained to provide the correct answers for the training set, evaluating it with those same examples is a biased measure of its true ability.\n",
    "\n",
    "The moth dataset includes a **validation set** and a **test set** to help us assess performance more objectively.\n",
    "\n",
    "Recall from the lectures that the **validation set** is used during hyperparameter tuning.\n",
    "It is a subset of data that the model never sees during training, providing a better estimate of how it performs on new data.\n",
    "However, because we make decisions on how to improve the model based on this specific subset, using it for our final evaluation would still lead to overoptimistic results.\n",
    "\n",
    "For the final, unbiased assessment of our model, we use the **test set**.\n",
    "This data is kept completely separate until the very end of our development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create datasets pointing to the folders containing the validation and test images.\n",
    "# Note that we are using the same transform.\n",
    "val_dataset_0 = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform_0)\n",
    "\n",
    "test_dataset_0 = ImageFolder(os.path.join(download_dir, \"test\"), transform=transform_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a4d02",
   "metadata": {},
   "source": [
    "We can use the `evaluate` method to calculate the loss and performance metrics across our separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c770c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specific loaders for the validation and test sets\n",
    "val_loader_0 = DataLoader(val_dataset_0, batch_size=8, shuffle=False)\n",
    "test_loader_0 = DataLoader(test_dataset_0, batch_size=8, shuffle=False)\n",
    "\n",
    "# Evaluate the model on both sets\n",
    "val_loss, val_accuracy = model_0.evaluate(val_loader_0)\n",
    "test_loss, test_accuracy = model_0.evaluate(test_loader_0)\n",
    "\n",
    "print(f\"Loss: validation={val_loss}  test={test_loss}\")\n",
    "print(f\"Accuracy: validation={val_accuracy}  test={test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3a736b",
   "metadata": {},
   "source": [
    "You can also monitor the validation performance during the training process by passing the `validation_data` argument to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed47ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for another 20 epochs while monitoring validation performance\n",
    "history_1 = model_0.fit(train_loader_0, epochs=20, validation_data=val_loader_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7708dfa",
   "metadata": {},
   "source": [
    "Visualising these curves helps us understand how the model is learning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1abc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to reuse throughout\n",
    "def plot_train_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "\n",
    "    ax1.plot(history_1.history[\"loss\"], label=\"train\")\n",
    "    ax1.plot(history_1.history[\"val_loss\"], label=\"val\")\n",
    "    ax1.set_title(\"Training loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history_1.history[\"acc\"], label=\"train\")\n",
    "    ax2.plot(history_1.history[\"val_acc\"], label=\"val\")\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_train_history(history_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d588d3",
   "metadata": {},
   "source": [
    "Notice that we used the same model_0 as before.\n",
    "Unless you re-ran the cell where the model was initially defined, this second round of training resumed exactly where the previous one finished.\n",
    "\n",
    "Observe the gap between the two lines: the training loss continues to decrease as the model \"memorises\" the training images, but the validation loss may stay stagnant or even begin to rise.\n",
    "This is a clear sign of **overfitting**, where the model is no longer learning general features of moths, but rather specific details unique to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ac9fa",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Start from scratch*\n",
    "\n",
    "Create a new model called `model_01` and train it from scratch for 40 epochs.\n",
    "Monitor both the validation loss and accuracy throughout the process.\n",
    "Once finished, plot the training curves to get a complete view of how the model evolved.\n",
    "\n",
    "*Optional*: Research about early stopping and how to implement it in keras.\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97d00f",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Full evaluation*\n",
    "\n",
    "Accuracy provides a good overall summary, but it can hide specific weaknesses in a model.\n",
    "For example, a model might be very good at identifying common moth species but consistently confuse two similar-looking ones.\n",
    "To investigate this, we use a confusion matrix.\n",
    "\n",
    "While Keras has many built-in features, its native metrics for detailed error analysis are limited.\n",
    "We can bridge this gap by using scikit-learn.\n",
    "First, we need to extract the raw prediction scores (the confidence for each class) and the true labels from our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd735a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(model, dataset):\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "\n",
    "    for im_batch, label_batch in loader:\n",
    "        outputs = model(im_batch)\n",
    "\n",
    "        y_score.extend(outputs.detach().numpy())\n",
    "        y_true.extend(label_batch.detach().numpy())\n",
    "\n",
    "    return np.array(y_score), np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416ff2c",
   "metadata": {},
   "source": [
    "Use the extract_scores function with your preferred model and the test dataset to retrieve the raw confidence scores and true labels.\n",
    "Then use the argmax function to get the most confident score for each image `y_pred = y_score.argmax(axis=1)` Finally, use `scikit-learn` to generate the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a3082",
   "metadata": {},
   "source": [
    "## Better model design\n",
    "\n",
    "One strategy to improve performance is to use architectures specifically suited to your data type.\n",
    "For images, we use **Convolutional Neural Networks** (CNNs).\n",
    "\n",
    "While we will cover the technical details later in the module, the main advantage of a CNN is its ability to recognize patterns (like edges or textures) regardless of where they appear in an image.\n",
    "This makes them far more efficient for visual tasks than the Dense models we have used so far.\n",
    "\n",
    "Below is a simple convolutional architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1088761",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 64\n",
    "\n",
    "model_cnn = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, resolution, resolution]),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36613347",
   "metadata": {},
   "source": [
    "If you look at the summary, you will notice that this model has fewer trainable parameters than a Dense model, even when handling larger images (`resolution = 64`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4baccc",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*CNN sizes*\n",
    "\n",
    "Try out different resolution sizes (for example: 32, 64, 128).\n",
    "Does the number of parameters change in the same way as it did for the previous Dense model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccafc4",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Train a CNN model*\n",
    "\n",
    "Train the model using the same settings as our earlier experiments: 8√ó8 images, cross-entropy loss, and the Adam optimiser.\n",
    "Ensure you monitor the validation loss throughout the process.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "- Do you observe a change in accuracy or loss?\n",
    "- What happens if you increase the resolution to 32√ó32 or even 64√ó64?\n",
    "- How does the training time change as you increase the resolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa62e24",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "So far, our models have struggled with overfitting.\n",
    "You may have noticed the training accuracy climbing while the validation accuracy remains low‚Äîa clear sign that the model is simply \"memorising\" the specific images in our training set rather than learning general features of moths.\n",
    "\n",
    "To improve generalisation, we need more data.\n",
    "However, in ecology, collecting thousands of additional samples is often not feasible.\n",
    "This is where **data augmentation** becomes a great strategy.\n",
    "\n",
    "Augmentation artificially expands our dataset by creating slightly modified versions of our existing images.\n",
    "By applying random transformations (e.g. cropping, flipping, or adjusting colours) we force the model to focus on the essential features of the moth (its shape and wing patterns) rather than irrelevant details like the exact position in the frame or the specific lighting conditions of the photo.\n",
    "\n",
    "We can use the `torchvision` library to define a range of random transformations.\n",
    "These will be applied \"on the fly\" during training, meaning the model sees a slightly different version of the image in every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of random augmentations\n",
    "augmentations = v2.Compose([\n",
    "    # Randomly crop a portion of the image and resize it back to 128x128\n",
    "    v2.RandomResizedCrop([128, 128]),\n",
    "\n",
    "    # Randomly flip the image horizontally\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "    # Randomly convert the image to grayscale 10% of the time\n",
    "    v2.RandomGrayscale(p=0.1),\n",
    "\n",
    "    # Randomly adjust brightness, contrast, and saturation\n",
    "    v2.ColorJitter(),\n",
    "])\n",
    "\n",
    "# Visualise the effect of these augmentations on a single image\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4)\n",
    "for ax in axes.flatten():\n",
    "    # Applying the same augmentation pipeline generates a unique result each time\n",
    "    im_aug = augmentations(im_tensor)\n",
    "    ax.imshow(to_pil(im_aug))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991debb5",
   "metadata": {},
   "source": [
    "Now we can integrate these augmentations into our training pipeline.\n",
    "It is important to note that we only apply augmentations to the training set.\n",
    "The validation and test sets should remain unmodified (except for basic resizing) so they provide a reliable, \"real-world\" measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_128 = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize([128, 128]),\n",
    "        v2.ToDtype(torch.float32, scale=True), # scale=True ensures values are 0-1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# We can also compose two complex transformations\n",
    "transform_aug = v2.Compose([\n",
    "    transform_128,\n",
    "    augmentations,\n",
    "])\n",
    "\n",
    "# Use the new augmentation transform\n",
    "train_dataset_aug = ImageFolder(train_dataset_path, transform=transform_aug)\n",
    "\n",
    "# Create val dataset. Note we are not using augmentations here\n",
    "val_dataset_aug = ImageFolder(os.path.join(download_dir, \"valid\"), transform=transform_128)\n",
    "\n",
    "model_cnn_128 = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[3, 128, 128]),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.GlobalMaxPooling2D(),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn_128.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012eb85",
   "metadata": {},
   "source": [
    "Finally, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=8, num_workers=2, shuffle=True)\n",
    "val_loader_aug = DataLoader(val_dataset_aug, batch_size=8, num_workers=2, shuffle=False)\n",
    "history_aug = model_cnn_128.fit(train_loader_aug, epochs=20, validation_data=val_loader_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e029415",
   "metadata": {},
   "source": [
    "With augmentations enabled, the training loss decreases more slowly, as the task has become harder.\n",
    "However, the gap between training and validation performance should ideally narrow, indicating better generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ff902",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(history_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e308328",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Improve the model performance*\n",
    "\n",
    "Play around with the architecture, hyperparamers, transform etc. Try to get the best performance you can.\n",
    "As another way to avoid overfitting research dropout.\n",
    "Make sure you leave at least 20 min for the next section though:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c3c2e",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "The final strategy we will explore is **transfer learning**.\n",
    "As discussed in the lectures, this involves reusing a model that has already been trained on a massive, general dataset.\n",
    "The idea is that the model has already learned to recognise fundamental visual features‚Äîsuch as edges, textures, and shapes‚Äîwhich are \"transferable\" to our specific task of identifying moth species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3095e",
   "metadata": {},
   "source": [
    "### Model loading\n",
    "\n",
    "We will use the `timm` library (PyTorch Image Models), which provides a wide range of pre-trained architectures.\n",
    "We have selected `efficientnet_b0`, a model known for being highly efficient and fast while maintaining strong performance.\n",
    "This model was originally trained on ImageNet, a famous dataset containing millions of images across a thousand different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdf3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models(pretrained=True)\n",
    "\n",
    "efficientnet = timm.create_model(\"efficientnet_b0\", pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fa955",
   "metadata": {},
   "source": [
    "Every pre-trained model has specific requirements for its input images (such as specific normalisation values).\n",
    "It is essential to use the same preprocessing pipeline that the model was originally trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08788f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = timm.data.create_transform(\n",
    "    **timm.data.resolve_data_config(efficientnet.pretrained_cfg)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc7ebf",
   "metadata": {},
   "source": [
    "### Feature embeddings\n",
    "\n",
    "Instead of training a deep network from scratch, we will use EfficientNet as a \"feature extractor\".\n",
    "We pass our moth images through the model and stop just before the final classification layer.\n",
    "The outputs at this stage are called **feature embeddings**.\n",
    "\n",
    "Embeddings are high-dimensional numerical representations of an image.\n",
    "Because the model was trained on millions of images, these embeddings are quite rich and descriptive.\n",
    "\n",
    "We can then use these embeddings as inputs for a much simpler model, such as a Logistic Regression classifier.\n",
    "This is exactly the same approach we used in the machine learning session.\n",
    "\n",
    "First, we define a function to extract these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96240b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_efficientnet_features(dataset, batch_size=8):\n",
    "    loader = DataLoader(dataset, batch_size=8)\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for im_batch, label_batch in tqdm(loader):\n",
    "        # Extract features from the model\n",
    "        feats = efficientnet.forward_features(im_batch)\n",
    "\n",
    "        # Global average pooling to convert spatial features into a 1D vector\n",
    "        feats = feats.mean(axis=(2, 3))\n",
    "\n",
    "        # Convert tensors to numpy arrays\n",
    "        features.extend(feats.detach().numpy())\n",
    "        targets.extend(label_batch.detach().numpy())\n",
    "\n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750532c",
   "metadata": {},
   "source": [
    "Now, we apply this to our moth datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for the training set\n",
    "train_dataset = ImageFolder(os.path.join(download_dir, \"train\"), transform=transform)\n",
    "X_train, y_train = extract_efficientnet_features(train_dataset)\n",
    "\n",
    "# Extract features for the test set\n",
    "test_dataset = ImageFolder(os.path.join(download_dir, \"test\"), transform=transform)\n",
    "X_test, y_test = extract_efficientnet_features(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa3c36",
   "metadata": {},
   "source": [
    "With our features ready, we can train a Logistic Regression model in seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070055c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit the classifier on the extracted embeddings\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbfa5a",
   "metadata": {},
   "source": [
    "And evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a495c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f\"Transfer Learning Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67025a4c",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Few-shot transfer learning*\n",
    "\n",
    "Transfer learning is particularly interesting when you have very little data.\n",
    "In ecology, obtaining thousands of labelled images is often impossible.\n",
    "This is the idea of **few-shot learning**, training a model with only a handful of examples.\n",
    "\n",
    "How well can we perform with only 1, 5, or 10 examples per species?\n",
    "Since we have already extracted the embeddings for the entire dataset, we can quickly test this by selecting small subsets.\n",
    "\n",
    "Use the function below to run an experiment.\n",
    "For each scenario (1, 5, and 10 examples per species), train the model 5 times (using different random seeds) and plot the average accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79187f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(X, y, n=5, seed=None):\n",
    "    series = pd.DataFrame({\"label\": y})\n",
    "    selection = series.groupby(\"label\").sample(n=n, random_state=seed)\n",
    "    X_subset = X[selection.index]\n",
    "    y_subset = y[selection.index]\n",
    "    return X_subset, y_subset"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
