{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8fa11f",
   "metadata": {},
   "source": [
    "# Week 2 Introduction to Machine Learning\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "In this weeks colab we will introduce the Machine Learning library `scikit-learn` and practice some basic concepts of Machine Learning (ML), including:\n",
    "\n",
    "1. A brief recap of what ML is\n",
    "2. How to do classification with `scikit-learn`\n",
    "3. How to evaluate model performance\n",
    "4. How to do clustering with `scikit-learn`\n",
    "5. How to do dimensionality reduction with `scikit-learn`\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- If a line starts with the fountain pen symbol (üñåÔ∏è), it asks you to implement a code part or answer a question.\n",
    "- Lines starting with the light bulb symbol (üí°) provide important information or tips and tricks.\n",
    "- Lines starting with the checkmark symbol (‚úÖ) reveal the solutions to specific exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be39d7",
   "metadata": {},
   "source": [
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873baf12",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "This notebook relies on helper functions defined in a separate Python file.\n",
    "You can download it by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04166c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/MScEcologyAndDataScienceUCL/BIOS0032_AI4Environment/raw/refs/heads/main/IntroductionToML/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536ef9e",
   "metadata": {},
   "source": [
    "You don't need to worry about what is inside the file to follow this tutorial, though you are welcome to take a peek if you're curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0afb5",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "**What is Machine Learning?**\n",
    "\n",
    "In ecology, we often deal with complex systems where it's hard to write manual rules for every scenario.\n",
    "**Machine Learning (ML)** solves this by letting the computer discover patterns in data (Experience) to improve its accuracy (Performance) on a specific goal (Task).\n",
    "\n",
    "Or formally:\n",
    "\n",
    "> A computer program is said to learn from *experience (E)* with respect to some classes of *tasks (T)* and *performance measure (P)* if its performance can improve with E on T measured by P.\n",
    ">\n",
    "> M. T. Mitchell. 1997. Machine Learning\n",
    "\n",
    "**Example**\n",
    "\n",
    "Identifying species in the field is time-consuming and requires expert knowledge.\n",
    "Machine Learning can help automate this process, allowing researchers to scale up studies.\n",
    "Imagine you are building a tool to identify British butterflies from field photos:\n",
    "\n",
    "- **Task (T)**: Classifying a new photo into the correct butterfly species.\n",
    "- **Performance (P)**: The percentage of images correctly identified by the model.\n",
    "- **Experience (E)**: A large database of labeled butterfly images (e.g., from museum collections or iNaturalist).\n",
    "\n",
    "**How does a computer program learn?**\n",
    "\n",
    "There are many ways to learn from data, and we call each specific approach a **model**.\n",
    "Commonly models are **parameterised**, meaning they have internal \"dials\" (called **parameters**) that control their behavior.\n",
    "During a process called **fitting**, the computer uses an algorithm to automatically tune these dials so the model's output matches the training data as closely as possible.\n",
    "Once the parameters are optimised, the model is ready to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cb2e5",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "The go-to tool for general machine learning in Python is **Scikit-Learn** (`sklearn`).\n",
    "\n",
    "![scikit-learn](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/260px-Scikit_learn_logo_small.svg.png?20180808062052)\n",
    "\n",
    "It provides an extensive library of ready-to-use algorithms and tools for building machine learning pipelines.\n",
    "It aslso uses a consistent \"template\" for every model, which makes it easy to experiment with different approaches once you learn the basics.\n",
    "You can learn more about `scikit-learn` in its official [documentation](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c558029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn  # notice scikit-learn is imported as sklearn.\n",
    "\n",
    "# scikit-learn will be preinstalled in colab environments\n",
    "\n",
    "# print the currently installed scikit-learn version\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb76557",
   "metadata": {},
   "source": [
    "## 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a6e2",
   "metadata": {},
   "source": [
    "### Case study: species identification of flowers\n",
    "\n",
    "Now let's work with a toy example to see machine learning in action.\n",
    "Suppose you need to automate the identification of Iris flower species for a large-scale study.\n",
    "You collect flowers from the field and bring them to the lab, where a device scans each flower and measures several properties.\n",
    "\n",
    "How would **you** describe this Iris flower?\n",
    "\n",
    "<img alt=\"Iris versicolor\" src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\" width=\"600\" />\n",
    "\n",
    "* Colour?\n",
    "* Number of stripes?\n",
    "* Size?\n",
    "* Weight?\n",
    "* Environment?\n",
    "\n",
    "In machine learning, these descriptors are called features.\n",
    "\n",
    "**What are features?**\n",
    "\n",
    "They are numerical or categorical descriptors, attributes or traits of the object of study.\n",
    "\n",
    "> A **feature** is an individual measurable property or characteristic of a phenomenon\n",
    ">\n",
    "> *Bishop, Christopher (2006).\n",
    "  Pattern recognition and machine learning*\n",
    "\n",
    "For Iris flowers, many features could be used, but for now let's assume that our scanning device only measures the length and width of the sepals and petals.\n",
    "\n",
    "![iris sepal/petal length/width](https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg)\n",
    "\n",
    "**Where to get Iris flower data?**\n",
    "\n",
    "A dataset of iris measurements is publicly available.\n",
    "Find a description [here](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "Lets load a dataset of Iris flower measurements using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn offers toy datasets including the iris dataset.\n",
    "# load the dataset module from scikit-learn.\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fe35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# extract the features table\n",
    "iris_data = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11605239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first rows\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850c045",
   "metadata": {},
   "source": [
    "**Dataset summary**\n",
    "\n",
    "We now have two of the three ingredients for our machine learning task:\n",
    "\n",
    "1. **Task (T)**: Identify the species of the Iris flower from the measurements taken by the scanner.\n",
    "2. **Experience (E)**: A dataset containing measurements of flowers from three different species.\n",
    "\n",
    "We will cover performance evaluation in the next section.\n",
    "Before that, let's explore what a classification task actually is and look at a couple of models commonly used to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb34f4",
   "metadata": {},
   "source": [
    "### What is a classification task?\n",
    "\n",
    "In **supervised learning**, we aim to predict a **target** variable using a set of input **features**.\n",
    "When that target variable is categorical (representing discrete groups or labels), the task is called **classification**.\n",
    "\n",
    "For the Iris dataset, the question is: given our feature vector **x** (the measurements), can we predict the correct species (the class label) **y**?\n",
    "\n",
    "**Inspect the targets**\n",
    "\n",
    "Let's examine the **targets** of the Iris dataset.\n",
    "These represent the specific species ID assigned to each individual flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3800dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the classification targets\n",
    "iris_target = iris.target\n",
    "\n",
    "# print first values of iris_target\n",
    "iris_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196166d",
   "metadata": {},
   "source": [
    "Notice that the target classes are encoded as integers.\n",
    "For readability, we will map these values to their corresponding species names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the class names\n",
    "species_names = iris.target_names\n",
    "\n",
    "# map the target integer values to species names\n",
    "y_iris = iris_target.apply(lambda index: species_names[index])\n",
    "\n",
    "y_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2c4f0",
   "metadata": {},
   "source": [
    "Finally, let's count how many flowers of each species we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows per target class\n",
    "iris_target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ef96f",
   "metadata": {},
   "source": [
    "**Visualsing the dataset**\n",
    "\n",
    "To simplify visualisation, we will start by selecting just two flower features: petal length and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1597b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = \"petal length (cm)\"\n",
    "feature_2 = \"petal width (cm)\"\n",
    "\n",
    "# This will extract a dataframe with only the two selected columns\n",
    "X_iris = iris_data[[feature_1, feature_2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdf3475",
   "metadata": {},
   "source": [
    "Each datapoint has some features and a class label (the species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(\n",
    "    iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645795f",
   "metadata": {},
   "source": [
    "The fundamental challenge of classification is: Given a **new datapoint** with unknown identity, how can we determine its correct class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scatter_plot_with_test_point\n",
    "\n",
    "# create new test point (change if you like)\n",
    "test_point = [3.8, 1.6]  # petal length cm, petal width cm\n",
    "\n",
    "scatter_plot_with_test_point(\n",
    "    iris_data,\n",
    "    feature_1=feature_1,\n",
    "    feature_2=feature_2,\n",
    "    target=y_iris,\n",
    "    test_point=test_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64866f",
   "metadata": {},
   "source": [
    "### Classification models\n",
    "\n",
    "In this section, we will briefly explore three types of models that use very different strategies for learning.\n",
    "\n",
    "**Nearest neighbour**\n",
    "\n",
    "The idea behind the **Nearest Neighbor** model is straightforward: for any new test point, we search for the observation in our dataset that is **most similar** to it and assume they share the same class.\n",
    "Here, similarity is determined by the features, specifically by calculating the distance between feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scatter_plot_with_closest_point_to_test_point\n",
    "\n",
    "scatter_plot_with_closest_point_to_test_point(\n",
    "    iris_data,\n",
    "    feature_1=feature_1,\n",
    "    feature_2=feature_2,\n",
    "    target=y_iris,\n",
    "    test_point=test_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa3ba7",
   "metadata": {},
   "source": [
    "Implementing these models is quite easy using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf517ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the KNeighborsClassifier from scikit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "# create a model instance\n",
    "knn_model_1 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit model with the iris dataset\n",
    "knn_model_1.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a518747",
   "metadata": {},
   "source": [
    "A great way to visualise what the model is learning is to plot the decision regions and boundary.\n",
    "These indicate all how the model splits the **feature space** into regions of the same class.\n",
    "Any new point is classified depending on which region it falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0483b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_light = ListedColormap([\"lightblue\", \"peachpuff\", \"palegreen\"])\n",
    "\n",
    "# plot the decision regions\n",
    "ax = plot_decision_boundary(knn_model_1, X_iris, cmap=cmap_light)\n",
    "\n",
    "# overlay data points from iris dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330eb9d",
   "metadata": {},
   "source": [
    "You can modify the behavior of the model in several ways; these settings are known as **hyperparameters**.\n",
    "For instance, rather than relying on just a single neighbor, you can consider the `k`-nearest neighbors and assign the most common class among them as the predicted label.\n",
    "\n",
    "Too see all available hyperparameters, you can check the [official¬†documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c8407",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Exploring hyper-parameters*\n",
    "\n",
    "Train a NearestNeighbor classifier using `k=5` and plot the decision boundaries.\n",
    "What qualitative changes do you observe in the decision boundary plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f0748",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "\n",
    "Despite its name, Logistic Regression is a model used for classification, not regression.\n",
    "\n",
    "It is a linear model because it makes predictions based on a weighted sum of the input features.\n",
    "For every possible class (e.g., each flower species), the model learns a specific set of weights ($w$).\n",
    "It calculates a \"score\" for each class by multiplying each feature by its corresponding weight:\n",
    "\n",
    "$$ score_i = w_{i, 1} \\cdot \\text{feat}_1 + ...\n",
    "+ w_{i, k} \\cdot \\text{feat}_k $$\n",
    "\n",
    "The class that achieves the highest weighted sum is the one the model selects as its prediction.\n",
    "Training the model means finding the optimal values for these weights ($w$) so that the highest scores align with the correct labels in our dataset.\n",
    "\n",
    "Here's how you train one with `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a model instance. You could also provide\n",
    "# some hyperparameters here, but by not providing\n",
    "# any we are using the default\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the dataset\n",
    "lr_model.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6780f5",
   "metadata": {},
   "source": [
    "Because these scores are calculated using a linear equation, the \"boundaries\" where the model switches from predicting one class to another will appear as straight lines when visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision regions\n",
    "ax = plot_decision_boundary(lr_model, X_iris, cmap=cmap_light)\n",
    "\n",
    "# overlay data points from iris dataset\n",
    "ax = sns.scatterplot(\n",
    "    data=iris_data,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f2f2a",
   "metadata": {},
   "source": [
    "**Decision Trees and Random Forest**\n",
    "\n",
    "Decision Trees use a series of binary choices to discriminate between data points.\n",
    "By following a sequence of logical splits, the model \"bins\" a test point into its predicted species.\n",
    "\n",
    "Example binary decisions include checking if **petal length ‚â§ 5.1 cm** or if **petal width ‚â§ 1.75 cm**.\n",
    "These rules can be nested to create specific classifications, such as: *\"If petal length > 5.2 cm AND petal width < 1.3 cm, then predict Setosa.\"*\n",
    "\n",
    "Each individual decision splits the feature space into two distinct regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataset points\n",
    "ax = sns.scatterplot(\n",
    "    data=X_iris,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")\n",
    "\n",
    "# draw horizontal line at y = 1.75\n",
    "ax.axhline(1.75, color=\"gray\", linewidth=3, alpha=0.2)\n",
    "\n",
    "# draw vertical line at x = 4.95\n",
    "ax.axvline(4.95, color=\"blue\", linewidth=3, alpha=0.2, ymax=0.65)\n",
    "\n",
    "# add text to label regions\n",
    "ax.text(4.2, 2.15, f\"width >= 1.75\")\n",
    "ax.text(2.9, 1.5, f\"width < 1.75\\n& length < 4.95\")\n",
    "ax.text(5.5, 1.1, f\"width < 1.75\\n& length >= 4.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a63f2b",
   "metadata": {},
   "source": [
    "Implementing a decision tree with scikit-learn is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Decision Tree Classifier model from scikit-learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create a Decision Tree classifier model\n",
    "decision_tree = DecisionTreeClassifier(max_depth=2, min_impurity_decrease=0.01)\n",
    "# check the scikit-learn documentation to see possible configurations\n",
    "\n",
    "# fit to dataset\n",
    "decision_tree.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3340",
   "metadata": {},
   "source": [
    "One of the primary advantages of Decision Trees is that they are highly interpretable, allowing us to follow the exact logic behind any specific prediction.\n",
    "We can visualise the entire hierarchy of these decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plot_tree function from tree tools in scikit-learn\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a new figure\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# visualize the trained decision trees\n",
    "plot_tree(\n",
    "    decision_tree,\n",
    "    feature_names=[feature_1, feature_2],\n",
    "    class_names=decision_tree.classes_,\n",
    "    impurity=False,\n",
    "    label=\"root\",\n",
    "    rounded=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Add labels to decisions\n",
    "ax.text(0.43, 0.66, \"yes\")\n",
    "ax.text(0.73, 0.66, \"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c6272",
   "metadata": {},
   "source": [
    "A **Random Forest** buils on this concept by using a large collection of decision trees.\n",
    "Each tree is trained on a random subset of the data and features.\n",
    "To reach a final classification, the forest takes a \"vote\" among all its trees, using the majority result as the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2d6c9",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Train a random forest model*\n",
    "\n",
    "Search in the scikit-learn documentation for `RandomForestClassifier`.\n",
    "Train a random forest models with different number of trees (`n_estimators=1, 10, 100`) and plot their decision boundary.\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be904646",
   "metadata": {},
   "source": [
    "### How to choose a model?\n",
    "\n",
    "The most reliable way to select a model is to test multiple options and evaluate their performance (which we will cover in the next section).\n",
    "However, because it is impossible to test every available algorithm, we use specific criteria to narrow down an initial list of candidates.\n",
    "\n",
    "Here we will discuss a few:\n",
    "\n",
    "1. Model flexibility or capacity\n",
    "2. Speed and computational demands\n",
    "3. Probabilistic vs categorical outputs.\n",
    "\n",
    "**Model capacity**\n",
    "\n",
    "Model capacity describes an algorithm's ability to fit complex patterns.\n",
    "It essentially determines how intricate the decision regions can be.\n",
    "\n",
    "For instance, a **Logistic Regression** model is limited to linear patterns, resulting in relatively low capacity.\n",
    "In contrast, a Nearest Neighbor model (`k=1`) has very high capacity; it can practically adapt to fit any pattern present in a dataset.\n",
    "\n",
    "While high-capacity models can capture complex relationships, they are prone to **overfitting**.\n",
    "They often memorize noise in the training data rather than the underlying true signal which leads to poor **generalizability** on new data.\n",
    "Conversely, low-capacity models are \"simpler\" and less prone to overfitting, though they may struggle to capture subtle nuances.\n",
    "\n",
    "Complex models (high capacity) can fit complex patterns however, because of this, they can easily overfit to the training data, leading to poor generalisability.\n",
    "Simple models (low capacity) cannot fit complex patterd yet they are much less prone to overfit.\n",
    "\n",
    "One way to see this in action is to train a model on different random subsets of the same data and observe how the decision boundaries shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7630967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random subsets of the dataset using different seeds for replication\n",
    "seeds = [23984, 19823, 39848]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for seed, ax in zip(seeds, axes):\n",
    "    # We'll take 30% of the dataset\n",
    "    X_subset = X_iris.sample(frac=0.3)\n",
    "    y_subset = y_iris.loc[X_subset.index]\n",
    "\n",
    "    # Create and fit a new model instance\n",
    "    knn_model_subset = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn_model_subset.fit(X_subset, y_subset)\n",
    "\n",
    "    # Plot decision boundaries\n",
    "    plot_decision_boundary(knn_model_subset, X_iris, cmap=cmap_light, ax=ax)\n",
    "\n",
    "    # Overlay the full dataset to see how the model generalizes\n",
    "    sns.scatterplot(\n",
    "        data=iris_data,\n",
    "        x=feature_1,\n",
    "        y=feature_2,\n",
    "        hue=y_iris,\n",
    "        style=y_iris,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Highlight the specific subset used for training with black borders\n",
    "    ax = sns.scatterplot(\n",
    "        data=X_subset,\n",
    "        x=feature_1,\n",
    "        y=feature_2,\n",
    "        style=y_iris,\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=\"none\",\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220aeb44",
   "metadata": {},
   "source": [
    "Notice how the decision regions change between plots!\n",
    "This behavior is known as **variance**: the tendency of a model to change significantly based on the specific training data it receives.\n",
    "Try switching the model to Logistic Regression, you will find that the regions barely change at all.\n",
    "\n",
    "This tension is known as the **bias-variance trade-off**.\n",
    "Here, bias refers to how wrong the model is on average.\n",
    "It is essentially another way to describe the balance between **underfitting** (high bias) and **overfitting** (high variance).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Bias_and_variance_contributing_to_total_error.png\" alt=\"variace-bias trade-off\" width=\"600\" />\n",
    "\n",
    "One must choose a balance between enough capacity to learn the underlying pattern and enough simplicity to ignore the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24687b3",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Choosing the right capacity*\n",
    "\n",
    "You have collected a small dataset of plant traits and you wish to automate the identification of species.\n",
    "You suspect the relationship between traits and species is complex.\n",
    "You only have data from 10% of the possible sites, and there is significant natural variation between plants.\n",
    "Which model would you choose, high capacity or low capacity, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d881c7f8",
   "metadata": {},
   "source": [
    "üí° *Noise sensitivity*\n",
    "\n",
    "Data is rarely perfect.\n",
    "Measurements and annotations are often imprecise or, at worst, completely incorrect.\n",
    "It is common for a dataset to contain a certain amount of noise, i.e. incorrectly labeled examples or outlier values.\n",
    "A high-capacity model is particularly sensitive to these errors because it may attempt to \"learn\" the noise as if it were a true pattern, which ultimately hurts its performance on new, clean data.\n",
    "In contrast, simpler models tend to ignore these individual errors in favor of the overall trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a98463",
   "metadata": {},
   "source": [
    "**Speed and computational demands**\n",
    "\n",
    "Another important consideration is a model's operational efficiency: how quickly a model trains and performs **inference** (prediction), and the hardware it requires\n",
    "\n",
    "In practice, computational resources are often limited.\n",
    "Prioritising \"speedy and simple\" models saves time and reduces the carbon footprint of large-scale computing.\n",
    "Generally, model capacity is tied to resource consumption, as a model's ability to fit complex patterns grows, so do its memory and processing demands.\n",
    "\n",
    "As an example, let's measure how training and inference times scale for a Nearest Neighbor model as we increase the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e99890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a performance timer from the standard library\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "test_point_4 = np.array([4, 4])\n",
    "\n",
    "\n",
    "def measure_nearest_neighbor_speed(n_samples, test_point=test_point_4):\n",
    "    # generate toy dataset with scikit-learn dataset functions.\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=2,  # Two features\n",
    "        n_informative=2,\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        n_classes=2,  # Two classes\n",
    "    )\n",
    "\n",
    "    # create a Nearest Neighbor model\n",
    "    model = KNeighborsClassifier(n_neighbors=1, algorithm=\"brute\", n_jobs=1)\n",
    "\n",
    "    # measure fit time\n",
    "    start_fit = perf_counter()\n",
    "    model.fit(X, y)\n",
    "    fit_time = perf_counter() - start_fit\n",
    "\n",
    "    # measure prediction time\n",
    "    start_predict = perf_counter()\n",
    "    model.predict(test_point.reshape(1, -1))\n",
    "    predict_time = perf_counter() - start_predict\n",
    "\n",
    "    return fit_time, predict_time\n",
    "\n",
    "# select different sizes of dataset.\n",
    "# logspace will return exponentially separated points.\n",
    "# 10, 100, 1000, ..., 10E8\n",
    "dataset_sizes = np.logspace(start=1, stop=7, num=7, dtype=np.int32)\n",
    "\n",
    "# measure time to fit and predict for each dataset size\n",
    "fit_times, predict_times = zip(\n",
    "    *[measure_nearest_neighbor_speed(n_samples) for n_samples in dataset_sizes]\n",
    ")\n",
    "\n",
    "# plot times\n",
    "plt.plot(dataset_sizes, fit_times, label=\"fit\")\n",
    "plt.plot(dataset_sizes, predict_times, label=\"predict\")\n",
    "\n",
    "# make axis logarithmic\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# add axis labels\n",
    "plt.ylabel(\"duration (s)\")\n",
    "plt.xlabel(\"dataset size (# samples)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf9e4b",
   "metadata": {},
   "source": [
    "Notice that as the dataset grows, both inference and training times increase linearly.\n",
    "Even for a single test point, the model begins to take a significant amount of time to predict.\n",
    "Imagine processing an large dataset!\n",
    "\n",
    "Do the same for LogisticRegression and observe the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac448cc9",
   "metadata": {},
   "source": [
    "**Probabilistic vs categorical outputs**\n",
    "\n",
    "Different models provide different types of answers.\n",
    "The Nearest Neighbor algorithm gives a definite answer for each prediction: it assigns the data to one class or another with no in-between.\n",
    "In contrast, Logistic Regression provides a score for each possible class.\n",
    "Usually, we take the class with the highest score as the final prediction.\n",
    "\n",
    "These are often referred to as **confidence scores**.\n",
    "They give the researcher another \"knob\" with which to control the quality of the outputs.\n",
    "By only trusting predictions with very high scores‚Äîa process called **thresholding**‚Äîyou choose results that are more likely to be correct, though you might also ignore some valid predictions in the process.\n",
    "\n",
    "Having a model that outputs these scores is highly valuable, as it allows different users to tune the thresholds to their specific ecological needs.\n",
    "We will explore this further in the evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array([0.4, 2.5])\n",
    "\n",
    "# With the logistic regresion we can extract the confidence score by using the\n",
    "# predict_proba method.\n",
    "confidence_scores = lr_model.predict_proba([test_point])\n",
    "\n",
    "# The confidence score is a list of 3 confidence scores, one for each class\n",
    "confidence_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad5c56",
   "metadata": {},
   "source": [
    "Having a model that can output these scores is great, because then different users can tune the thresholds to their needs.\n",
    "\n",
    "**Other considerations**\n",
    "\n",
    "When choosing a model, it is important to consider how it handles different types of data.\n",
    "For instance, some models, like Random Forests, can handle missing data points, whereas others, like Linear Regression, require a complete dataset to function.\n",
    "\n",
    "Additionally, many models are sensitive to the units or the scale of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e1d75",
   "metadata": {},
   "source": [
    "#### üñåÔ∏èChanging units\n",
    "\n",
    "So far we have been using **cm** as units for length.\n",
    "What happens if we change cm to meters for a single feature?\n",
    "How does this choice affect predictions?\n",
    "\n",
    "Use the code below to modify the dataset and test whether the model gives the same answer for the same flower when the units change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the feature array so that the first feature is in meters\n",
    "X_iris_2 = X_iris.copy()\n",
    "X_iris_2[\"petal length (m)\"] = X_iris[\"petal length (cm)\"] / 100\n",
    "X_iris_2 = X_iris_2.reindex(columns=[\"petal length (m)\", \"petal width (cm)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06169598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wish to classify a new flower. These are the measurements of the flower with\n",
    "# two different units of measurements\n",
    "test_point_5_cm = np.array([2.7, 0.7])  # (petal length cm, petal width cm)\n",
    "test_point_5_m = np.array([0.027, 0.7])  # (petal length m, petal width cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae450353",
   "metadata": {},
   "source": [
    "## 3. Model evaluation\n",
    "\n",
    "You have seen multiple models for Iris flower classification.\n",
    "Which model is the best fit?\n",
    "How can we be confident about the predictions of a model, or evaluate its performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba32fb",
   "metadata": {},
   "source": [
    "### Training and test split\n",
    "\n",
    "We could use the training data to count the number of correct and erroneous predictions.\n",
    "However this is a bad choice, as the Nearest Neighbor will always have 0 errors (can you see why?).\n",
    "Using the training data will not provide a clear picture of prediction accuracy for new points.\n",
    "\n",
    "**Solution**: Split the dataset into two parts: one for **training** another for evaluation or **testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the train_test_split function from scikit-learn module for model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split dataset and labels into test and train\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris,\n",
    "    y_iris,\n",
    "    test_size=0.3,  # test dataset is 30% of all data\n",
    ")\n",
    "\n",
    "# draw the dataset\n",
    "sns.scatterplot(data=iris_data, x=feature_1, y=feature_2, hue=y_iris)\n",
    "\n",
    "# with circles around the training set\n",
    "sns.scatterplot(\n",
    "    data=X_iris_train,\n",
    "    x=feature_1,\n",
    "    y=feature_2,\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"none\",\n",
    "    label=\"train set\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef35b0c",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Imbalanced datasets*\n",
    "\n",
    "Datasets are rarely **balanced**, they often contain varying numbers of examples for each class (species).\n",
    "Even with a balanced original dataset, a random selection can produce imbalanced subsets.\n",
    "Training on imbalanced data can degrade performance, as models often become biased toward the majority class, \"ignoring\" the minority species.\n",
    "\n",
    "1. Count how many examples per species you currently have in your training set.\n",
    "2. Research **stratified sampling**.\n",
    "3. Check the `scikit-learn` documentation for the `train_test_split` function.\n",
    "   Identify which parameter controls stratification and use it to create a representative split for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b633aa4",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "There are many measures of performance.\n",
    "Accuracy, which is percentage of correct predictions, is commonly used for classification.\n",
    "Other metrics will provide different information on the model's performace.\n",
    "See the list of [classification¬†metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) available in `scikit-learn`.\n",
    "\n",
    "Let's start with accuracy which is the number of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8304d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create new Nearest Neighbor model\n",
    "knn_model_3 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit with train split\n",
    "knn_model_3.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "# predict on the test data\n",
    "iris_test_predictions = knn_model_3.predict(X_iris_test)\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_iris_test, iris_test_predictions)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cca58",
   "metadata": {},
   "source": [
    "A more comprehensive report with per-species info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_iris_test, iris_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf648a",
   "metadata": {},
   "source": [
    "Another helpful tool to understand and visualize model performance is the **Confusion Matrix**.\n",
    "This table counts how many times each species was correctly identified versus how often it was misclassified as another species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_true=y_iris_test,\n",
    "    y_pred=iris_test_predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84117c98",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Evaluating performance of multiple models*\n",
    "\n",
    "Compute the accuracy score, performance report and confusion matrices of all previous classification models on the iris dataset.\n",
    "Which one is better?\n",
    "Justify your answer.\n",
    "Does the answer change if you use a different dataset split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9c2e4",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction\n",
    "\n",
    "Often, the data we collect is high-dimensional, meaning each observation is represented by a large number of features.\n",
    "This poses a challenge because it is nearly impossible to visualise anything beyond three dimensions.\n",
    "\n",
    "Ideally, we can find a way to summarise all that data into fewer features that still capture the most relevant information.\n",
    "This is the goal of dimensionality reduction: aggregating and simplifying many features into a smaller set without significant loss of information.\n",
    "\n",
    "This is particularly useful for visualisation.\n",
    "By reducing a complex dataset down to just two or three dimensions, we can plot the data and visually identify patterns, clusters, or outliers that were previously hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f146bf",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "One of the simplest approach is to do a linear projection.\n",
    "\n",
    "**PCA** is a linear projection that aligns with the directions of highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full iris dataset has 4 features\n",
    "iris_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2-dimensional PCA projection\n",
    "pca_model = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a70ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the 4-dimensional iris dataset into 2-d points\n",
    "projected_iris = pca_model.fit_transform(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot projected points\n",
    "# use species to color points\n",
    "ax = sns.scatterplot(\n",
    "    x=projected_iris[:, 0],\n",
    "    y=projected_iris[:, 1],\n",
    "    hue=y_iris,\n",
    "    style=y_iris,\n",
    ")\n",
    "\n",
    "# add labels to axis\n",
    "ax.set_xlabel(\"PCA component 1\")\n",
    "ax.set_ylabel(\"PCA component 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b3f26",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "As a final example let explore a dataset of digits.\n",
    "\n",
    "Each data point is an grayscale image of a handwritten digit.\n",
    "\n",
    "The images are 8x8 pixels, so in total each point has 64 features.\n",
    "\n",
    "In this case a feature is the grayscale value of a single pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f688d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_digits function from scikit-learn datasets module\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# extract data and target values\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# select a single data point\n",
    "# reshape to original 8x8 array\n",
    "digit = X_digits[0].reshape(8, 8)\n",
    "\n",
    "# use matplotlib to show image\n",
    "plt.imshow(digit, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d01509",
   "metadata": {},
   "source": [
    "**How to visualise the whole dataset?**\n",
    "\n",
    "Use dimensionality reduction\n",
    "\n",
    "Lets try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05807953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to project to 2 dimensions\n",
    "pca_digits = PCA(n_components=2).fit_transform(X_digits)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=pca_digits[:, 0],\n",
    "    y=pca_digits[:, 1],\n",
    "    hue=y_digits,\n",
    "    palette=\"tab20\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a20f5",
   "metadata": {},
   "source": [
    "Some digits seem to cluster.\n",
    "\n",
    "Still, there is a lot of overlap.\n",
    "\n",
    "Lets try a different projection method.\n",
    "\n",
    "Now we will use a non-linear projection called **t-SNE**.\n",
    "\n",
    "Checkout the [paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) where t-SNE was introduced, or this amazing [blog](https://distill.pub/2016/misread-tsne/) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import t-SNE mapping from scikit-learn\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6bd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TSNE to project to 2 dimensions\n",
    "tsne_digits = TSNE(\n",
    "    n_components=2,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    ").fit_transform(X_digits)\n",
    "\n",
    "# do a scatterplot, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=tsne_digits[:, 0],\n",
    "    y=tsne_digits[:, 1],\n",
    "    hue=y_digits,\n",
    "    palette=\"tab20\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ef39d",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Forest cover type dataset*\n",
    "\n",
    "Checkout the [fores¬†cover¬†type¬†dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype).\n",
    "\n",
    "Load the forest cover type dataset with scikit-learn.\n",
    "Use PCA and t-SNE projections on the data.\n",
    "Do a scatterplot with the results and use the cover type class to colour points.\n",
    "Are the points clearly separated?\n",
    "Install the [umap-learn](https://umap-learn.readthedocs.io/en/latest/index.html) library and test the UMAP (Uniform Manifold Approximation and Projection) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94493c16",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*Scaling variables*\n",
    "\n",
    "Most dimensionallity reduction methods are sensitive to scaling.\n",
    "Projection results are often dominated by features with larger values.\n",
    "Therefore it is common to standardise the variables before using dimensionallity reduction methods.\n",
    "A standardised variable has mean 0 and standard deviation of 1.\n",
    "Here is the formula to standardise a variable:\n",
    "\n",
    "$$\\hat{x} = \\frac{x - mean(x)}{std(x)}$$\n",
    "\n",
    "Scikit-learn provides standarisation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db265e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the StandardScaler from scikit-learn preprocessing module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardise the iris features\n",
    "X_iris_std = StandardScaler().fit_transform(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58516196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the standardized features with PCA\n",
    "pca_iris_std = PCA(\n",
    "    n_components=2,\n",
    ").fit_transform(X_iris_std)\n",
    "\n",
    "# do a scatterplot of PCA projection, color points by digit\n",
    "sns.scatterplot(\n",
    "    x=pca_iris_std[:, 0],\n",
    "    y=pca_iris_std[:, 1],\n",
    "    hue=y_iris,\n",
    "    palette=\"tab20\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8a2d3",
   "metadata": {},
   "source": [
    "Try the clustering techniques on the `covtype` dataset after standarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d390654",
   "metadata": {},
   "source": [
    "## 5. Clustering [20 min]\n",
    "\n",
    "**What if we don't have any labels?**\n",
    "\n",
    "Often our data contains some **structure**.\n",
    "\n",
    "* Features from different classes might be separated (**separability**)\n",
    "\n",
    "* Similar objects might have similar features (**smoothness**)\n",
    "\n",
    "Often we wish to find groupings or patterns in our data.\n",
    "This is called **clustering**.\n",
    "\n",
    "Datapoints in the same **cluster** are deemed to be similar under some measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07cb08",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "There are many algorithms for clustering.\n",
    "Here you will use k-means clustering.\n",
    "\n",
    "Scikit-learn has a [collection¬†of¬†clustering¬†algorithms](https://scikit-learn.org/stable/modules/clustering.html#clustering), including k-means.\n",
    "\n",
    "If interested, checkout an [explanation](https://www.youtube.com/watch?v=4b5d3muPQmA) of the k-means clustering algorithm or an [interactive¬†simulation](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make_blobs function in scikit-learn datasets module\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate synthetic dataset made up of 5 blobs\n",
    "X_blobs, y_blobs = make_blobs(n_features=2, n_samples=4000, centers=5)\n",
    "\n",
    "# plot synthetic dataset\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1])\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means clustering model from scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create a new K-means clustering model.\n",
    "# specify 5 wanted clusters\n",
    "kmeans_model_1 = KMeans(n_clusters=5)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_1.fit(X_blobs)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_blobs_pred = kmeans_model_1.predict(X_blobs)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1], hue=y_blobs_pred)\n",
    "\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4498d51",
   "metadata": {},
   "source": [
    "Clustering performance will depend on clustering parameters, choice of algorithm and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with 3 clusters\n",
    "kmeans_model_2 = KMeans(n_clusters=3)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_2.fit(X_blobs)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_blobs_pred = kmeans_model_2.predict(X_blobs)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_blobs[:, 0], y=X_blobs[:, 1], hue=y_blobs_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# repeat with other dataset\n",
    "X_circles, y_circles = make_circles(factor=0.2, n_samples=4000, noise=0.1)\n",
    "\n",
    "# create K means with 2 clusters\n",
    "kmeans_model_3 = KMeans(n_clusters=2)\n",
    "\n",
    "# fit to dataset\n",
    "kmeans_model_3.fit(X_circles)\n",
    "\n",
    "# get predicted clusters for the dataset\n",
    "y_pred = kmeans_model_3.predict(X_circles)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_circles[:, 0], y=X_circles[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat with DBscan algorithm\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# create DBSCAN model\n",
    "dbscan_model = DBSCAN(eps=0.15)\n",
    "\n",
    "# fit to dataset\n",
    "y_pred = dbscan_model.fit_predict(X_circles)\n",
    "\n",
    "# plot predictions\n",
    "ax = sns.scatterplot(x=X_circles[:, 0], y=X_circles[:, 1], hue=y_pred)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd7022",
   "metadata": {},
   "source": [
    "#### üñåÔ∏è*k-means clustering of the iris dataset*\n",
    "\n",
    "Use K-means clustering on the iris dataset.\n",
    "Can you recover the species separation?\n",
    "Research Affinity Propagation clustering and compare to K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80e05c",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d2635",
   "metadata": {},
   "source": [
    "**Which algorithm to choose?**\n",
    "\n",
    "Short answer: It depends!\n",
    "\n",
    "> The ‚ÄúNo Free Lunch‚Äù Theorem argues that, without having substantive information about the modeling problem, there is no single model that will always do better than any other model. Because of this, a strong case can be made to try a wide variety of techniques, then determine which model to focus on.\n",
    ">\n",
    "> ‚Äî Pages 25-26, Applied Predictive Modeling, 2013.\n",
    "\n",
    "No silver bullet, but often for classification it is sensible to first try a Support Vector Machine or Random Forest.\n",
    "\n",
    "This will give you an idea of how separable your data is. The next step is to try different features, and perhaps even collect more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef04718",
   "metadata": {},
   "source": [
    "**How much data do I need?**\n",
    "\n",
    "Short answer: It depends!\n",
    "\n",
    "It depends on how easy it is for your classifier to separate your data. \n",
    "\n",
    "Some problems are relatively easy and don‚Äôt require lots of data, others such as species identification in images can require 10,000s."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
